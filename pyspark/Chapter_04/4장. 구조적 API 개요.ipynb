{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec22541",
   "metadata": {},
   "source": [
    "# PART Ⅱ\n",
    "# 구조적 API : DataFrame, SQL, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bdc550",
   "metadata": {},
   "source": [
    "# 4장. 구조적 API 개요\n",
    "- 구조적 API : 비정형 로그 파일부터 반정형 CSV 파일, 매우 정형적인 파케이 파일까지 다양한 유형의 데이터를 처리할 수 있음\n",
    "\n",
    "- 구조적 API의 세 가지 분산 컬렉션\n",
    "  - Dataset\n",
    "  - DataFrame\n",
    "  - SQL 테이블과 뷰\n",
    "- 구조적 API 사용할 수 있는 고 : 배치, 스트리밍\n",
    "  - 구조적 API 활용 시, 배치 작업을 스트리밍 작업으로 손쉽게 변환(반대의 경우도 마찬가지)\n",
    "  \n",
    "- 구조적 API : 데이터 흐름을 정의하는 기본 추상화 개념\n",
    "\n",
    "- 반드시 이해해야 하는 세 가지 기본 개념 설명\n",
    "  - 타입형(typed)/비타입형(untyped) API의 개념과 차이점\n",
    "  - 핵심 용어\n",
    "  - 스파크가 구조적 API의 데이터 흐름을 해석하고 클러스터에서 실행하는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec5840",
   "metadata": {},
   "source": [
    "## 4.1 DataFrame과 Dataset\n",
    "- 스파크는 DataFrame과 Dataset이라는 두 가지 구조화된 컬렉션 개념을 가지고 있음\n",
    "- DataFrame과 Dataset이 무엇을 나타내는지 먼저 정의\n",
    "- DataFrame과 Dataset은 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션\n",
    "  - 각 컬럼은 다른 컬럼과 동일한 수의 로우를 가져야 함('값 없음'은 null로 표시함)\n",
    "  - 컬렉션의 모든 로우는 같은 데이터 타입 정보를 가지고 있어야 함\n",
    "  - 결과를 생성하기 위해 어떤 데이터에 어떤 연산을 적용해야 하느지 정의하는 지연 연산의 실행 계획이며 불변성을 가짐\n",
    "- DataFrame에 액션을 호출하면 스파크는 트랜스포메이션을 실제로 실행하고 결과를 반환함\n",
    "  - 이 과정은 사용자가 원하는 결과를 얻기 위해 로우와 컬럼을 처리하는 방법에 대한 계획을 나타냄\n",
    "\n",
    "- **NOTE**\n",
    "  - 기본적으로 테이블과 뷰는 DataFrame과 같음\n",
    "  - 대신 테이블은 DataFrame 코드 대신 SQL을 사용함\n",
    "- DataFrame과 Dataset을 조금 더 구체적으로 정의하려면 스키마를 알아야 함\n",
    "  - 스키마는 분산 컬렉션에 저장할 데이터 타입을 정의하는 방법\n",
    "\n",
    "\n",
    "## 4.2 스키마\n",
    "- 스키마\n",
    "  - DataFrame의 컬럼명과 데이터 타입을 정의함\n",
    "  - 데이터 소스에서 얻거나 직접 정의할 수 있음\n",
    "  - 여러 데이터 타입으로 구성되므로 어떤 데이터 타입이 어느 위치에 있는지 정의하는 방법이 필요함\n",
    "  \n",
    "## 4.3 스파크의 구조적 데이터 타입 개요\n",
    "- 스파크 : 사실상 프로그래밍 언어\n",
    "  - 카탈리스트 엔진 : 실행 계획 수리보가 처리에 사용하는 자체 데이터 타입 정보를 가지고 있는 엔진을 사용함\n",
    "    - 카탈리스트 엔진의 특징 : 다양한 실행 최적화 기능을 제공\n",
    "  - 자체 테이터 타입을 지원하는 여러 언어 API와 직접 매핑되며 각 언어에 대한 매핑 테이블을 가지고 있음\n",
    "    - 파이썬이나 R을 이용해 스파크의 구조적 API를 사용하더라도 대부분의 연산은 파이썬과 R의 데이터 타입이 아닌 스파크의 데이터 타입을 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac1a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"sample\").master(\"local[*]\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef2d9405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>(number + 10)</th></tr>\n",
       "<tr><td>10</td></tr>\n",
       "<tr><td>11</td></tr>\n",
       "<tr><td>12</td></tr>\n",
       "<tr><td>13</td></tr>\n",
       "<tr><td>14</td></tr>\n",
       "<tr><td>15</td></tr>\n",
       "<tr><td>16</td></tr>\n",
       "<tr><td>17</td></tr>\n",
       "<tr><td>18</td></tr>\n",
       "<tr><td>19</td></tr>\n",
       "<tr><td>20</td></tr>\n",
       "<tr><td>21</td></tr>\n",
       "<tr><td>22</td></tr>\n",
       "<tr><td>23</td></tr>\n",
       "<tr><td>24</td></tr>\n",
       "<tr><td>25</td></tr>\n",
       "<tr><td>26</td></tr>\n",
       "<tr><td>27</td></tr>\n",
       "<tr><td>28</td></tr>\n",
       "<tr><td>29</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-------------+\n",
       "|(number + 10)|\n",
       "+-------------+\n",
       "|           10|\n",
       "|           11|\n",
       "|           12|\n",
       "|           13|\n",
       "|           14|\n",
       "|           15|\n",
       "|           16|\n",
       "|           17|\n",
       "|           18|\n",
       "|           19|\n",
       "|           20|\n",
       "|           21|\n",
       "|           22|\n",
       "|           23|\n",
       "|           24|\n",
       "|           25|\n",
       "|           26|\n",
       "|           27|\n",
       "|           28|\n",
       "|           29|\n",
       "+-------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#스칼라나 파이썬이 아닌 스파크의 덧셈 연산을 수행함\n",
    "df=spark.range(500).toDF(\"number\")\n",
    "df.select(df[\"number\"]+10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cffb85",
   "metadata": {},
   "source": [
    "- 스파크에서 덧셈 연산이 수행되는 이유\n",
    "  - 스파크가 지원하는 언어를 이용해 작성된 표현식을 카탈리스트 엔진에서 스파크의 데이터 타입으로 변환해 명령을 처리하기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32663681",
   "metadata": {},
   "source": [
    "### 4.3.1 DataFrame과 Dataset 비교\n",
    "- 구조적 API에는 '비타입형'인 DataFrame과 '타입형'인 Dataset이 있음\n",
    "  - DataFrame을 '비타입형'으로 보는 것이 다솨 부정확할 수도 있음\n",
    "  - DataFrame에도 데이터 타입이 있음\n",
    "  - 하지만 스키마에 명시된 데이터 타입의 일치 여부를 런타임이 되어서야 확인함\n",
    "- Dataset은 스키마에 명시된 데이터 타입의 일치 여부를 컴파일 타임에 확인함\n",
    "  - Dataset은 JVM 기반의 언어인 스칼라와 자바에서만 지원함\n",
    "  - Dataset의 데이터 타입을 정의하려면 스칼라의 케이스 클래스나 자바 빈을 사용해야 함\n",
    "\n",
    "- 이 책의 예제는 대부분 DataFrame을 사용함\n",
    "- 스파크의 DataFrame은 Row 타입으로 구성된 Dataset임\n",
    "  - Row 타입은 스파크가 사용하는 '연산에 최적화된 인메모리 포맷'의 내부적인 표현 방식임\n",
    "  - Row 타입을 사용하면 가비지 컬렉션과 객체 초기화 부하가 있는 JVM 데이터 타입을 사용하는 대신 자체 데이터 포맷을 사용하기 때문에 매우 효율적인 연산이 가능함\n",
    "  - 파이썬이나 R에서는 스파크의 Dataset을 사용할 수 없음\n",
    "    - 하지만 최적화된 포맷인 Dataframe으로 처리할 수 있음\n",
    "    \n",
    "- DataFrame, 스파크의 데이터 타입 그리고 스키마를 완전히 이해하려면 시간이 필요함\n",
    "  - 지금 기억해야 할 것은 DataFrame을 사용하면 스파크의 최적화된 내부 포맷을 사용할 수 있음\n",
    "  - 스파크의 최적화된 내부 포맷을 사용하면 스파크가 지원하는 어떤 언어 API를 사용하더라도 동일한 효과 효율성을 얻을 수 있음\n",
    "  \n",
    "  \n",
    "### 4.3.2 컬럼\n",
    "- 컬럼\n",
    "  - 단순 데이터 타입 : 정수형이나 문자열\n",
    "  - 복합 데이터 타입 : 배열이나 맵\n",
    "  - null 값 표현\n",
    "- 스파크는 데이터 타입의 모든 정보를 추적하며 다양한 컬럼 변환 방법을 제공함\n",
    "\n",
    "### 4.3.3 로우\n",
    "- 로우 : 데이터 레코드\n",
    "  - DataFrame의 레코드는 Row 타입으로 구성됨\n",
    "  - 로우는 SQL, RDD, 데이터소스에서 얻거나 직접 만들 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c302536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#range 메서드를 사용해 DataFrame을 생성하는 예제\n",
    "spark.range(2).collect()\n",
    "#Row 객체로 이루어진 배열을 반환함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a5149",
   "metadata": {},
   "source": [
    "### 4.3.4 스파크 데이터 타입\n",
    "- 스파크는 여러 가지 내부 데이터 타입을 가지고 있음\n",
    "  - [표4-1-파이썬],[표4-2-스칼라],[표4-3-자바]은 다양한 프로그래밍 언어의 데이터 타입이 스파크의 어떤 데이터 타입과 매핑되는지를 나타냄\n",
    "  - 표4-1. 파이썬 데이터 타입 매핑\n",
    "  \n",
    "  |스파크 데이터 타입|파이썬 데이터 타입|데이터 타입 생성/접근용 API|\n",
    "  |------------------|------------------|---------------------------|\n",
    "  |ByteType|int.long.참고: 숫자는 런타임에 1바이트 크리의 부호형 정수(integer)로 변환됨. -128과 127 사이의 값을 가짐|ByteType()|\n",
    "  |ShortType|int.long.참고: 숫자는 런타임에 2바이트 크기의 부호형 정수로 변환됨. -32768과 32767 사이의 값을 가짐|ShortType()|\n",
    "  |IntergerType|int.long.참고 : 파이썬은 '정수형' 데이터 타입의 숫자를 관대하게 정의함. 매우 큰 숫자값을 IntegerType()에서 사용하면 스파크 SQL에서 거부할 수 있음. 숫자값이 너무 크면 LongType을 사용해야 함|IntegerType()|\n",
    "  |LongType|||\n",
    "  ||||\n",
    "  ||||\n",
    "  ||||\n",
    "  ||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f9540b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
