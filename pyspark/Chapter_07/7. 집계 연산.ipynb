{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5c2d99",
   "metadata": {},
   "source": [
    "# 집계 연산\n",
    "- 집계 : 무언가를 함께 모으는 행위이며 빅데이터 분석의 초석\n",
    "- 집계 함수 : 집계를 수행하려면 키나 그룹을 지정하고 하나 이상의 컬럼을 변환하는 방법을 지정할 수 있음\n",
    "  - 여러 입력값이 주어지면 그룹별로 결과를 생성함\n",
    "- 스파크의 집계 능력은 다양한 활용 사례와 가능성으로 비추어보았을 때 매우 정교하며 충분히 발달해 있음\n",
    "  - 일반적으로 특정 그룹의 평균값을 구하는 것과 같은 수치형 데이터 요약에 집계를 사용할 수 있음\n",
    "  - 해당 연산은 합산, 곱셈 또는 카운팅 등의 작업이 될 수 있음\n",
    "  - 배열, 리스트 또는 맵 같은 복합 데이터 타입을 사용해 집계를 수행할 수도 있음\n",
    "  \n",
    "- 스파크는 모든 데이터 타입을 다루는 것 외에도 다음과 같은 그룹화 데이터 타입을 생성할 수 있음\n",
    "  - 가장 간단한 형태의 그룹화 : select 구문에서 집계를 수행해 DataFrame의 전체 데이터를 요약하는 것\n",
    "  - 'group by' : 하나 이상의 키를 지정할 수 있으며 값을 가진 컬럼을 변환하기 위해 다른 집계 함수를 사용할 수 있음\n",
    "  - '윈도우(window)' : 하나 이상의 키를 지정할 수 있으며 값을 가진 컬럼을 변환하기 위해 다른 집계 함수를 사용할 수 있음.\n",
    "    - 하지만 함수의 입력으로 사용할 로우는 현재 로우와 어느 정도 연관성이 있어야 함\n",
    "  - 그룹화 셋(grouping set) : 서로 다른 레벨의 값을 집계할 때 사용함\n",
    "    - SQL, DataFrame의 롤업, 큐브를 사용할 수 있음\n",
    "  - 롤업(rollup) : 하나 이상의 키를 지정할 수 있음\n",
    "    - 컬럼을 변환하는 데 다른 집계 함수를 사용하여 계층적으로 요약된 값을 구할 수 있음\n",
    "  - 큐브(cube) : 하나 이상의 키를 지정할 수 있으며 값을 가진 컬럼을 변환하기 위해 다른 집계 함수를 사용할 수 있음\n",
    "    - 모든 컬럼 조합에 대한 요약 값을 계산함\n",
    "- 지정된 집계 함수에 따라 그룹화된 결과 : RelationalGroupedDataset을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf4b0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"sample\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d215186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구매 이력 데이터를 사용해 파티션을 훨씬 적은 수로 분할할 수 있도록 리파티셔닝하고 빠르게 접근할 수 있도록 캐싱하겠음\n",
    "# 파티션 수를 줄이는 이유 : 적은 양의 데이터를 가진 수많은 파일이 존재하기 때문\n",
    "df=spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .load(\"./Spark-The-Definitive-Guide-master/data/retail-data/all/online-retail-dataset.csv\")\\\n",
    "    .coalesce(5)\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b56c4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataFrame을 사용해 기본 집계를 수행해보겠음\n",
    "#다음은 count 메서드를 사용한 간단한 예제임\n",
    "df.count()==541909"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c966a163",
   "metadata": {},
   "source": [
    "- count 메서드가 트랜스포메이셔이 아닌 액션잉라는 사실을 알고 있을 것임(앞에서 배움)\n",
    "  - 그러므로 결과를 즉시 반환함\n",
    "  - count 메서드는 데이터셋의 전체 크기를 알아보는 용도로 사용하지만 메모리에 DataFrame 캐싱 작업을 수행하는 용도로 사용되기도 함\n",
    "  - count 메서드가 약간 이질적으로 보일 수 있음\n",
    "    - why : 함수가 아니라 메서드 형태로 존재하고 트랜스포메이션처럼 지연 연산 방식이 아닌 즉시 연산을 수행하기 때문임\n",
    "    - 다음 절에서는 지연 연산 방식으로 count 메서드를 사용하는 방법을 알아보겠음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1f49c",
   "metadata": {},
   "source": [
    "## 7.1 집계 함수\n",
    "- 모든 집계는 DataFrame의 .stat 속성을 이용하는 특별한 경우를 제외한다면 함수를 사용함\n",
    "\n",
    "### 7.1.1 count\n",
    "- count 함수 동작 방식\n",
    "  - 액션 x\n",
    "  - 트랜스포메이션\n",
    "- count 함수 사용 방법(2가지)\n",
    "  - 하나, count 함수에 특정 컬럼을 지정하는 방식\n",
    "  - 둘, count(\\*)나 count(\\1)을 사용하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb259106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#count 함수를 사용해 전체 로우 수를 카운트할 수 있음\n",
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"StockCode\")).show()\n",
    "# -> sql\n",
    "#SELECT COUNT(*) FROM dfTable\n",
    "#-> count(*) 구문을 사용하면 null 값을 가진 로우를 포함해 카운트함\n",
    "#count 함수에 특정 컬럼을 지정하면 null 값을 카운트하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e2bcab",
   "metadata": {},
   "source": [
    "### 7.1.2 countDistinct\n",
    "- countDistrinct 함수 : 전체 레코드 수가 아닌 고유 레코드 수를 구할 수 있음\n",
    "  - 개별 컬럼을 처리하는 데 더 적합함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17500bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e5dfd",
   "metadata": {},
   "source": [
    "### 7.1.3 approx_count_distinct\n",
    "- approx_count_distinct\n",
    "  - 어느 정도 수준의 정확도를 가지는 근사치만으로도 유의미할 때 사용\n",
    "  - 근사치 계산 가능\n",
    "  - 최대 추정 오류율이라는 한 가지 파라미터를 더 사용함\n",
    "  - countDistinct 함수보다 더 빠르게 결과를 반환함\n",
    "  - 대규모 데이터셋을 사용할 때 훨씬 더 좋아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67fdb74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#큰 오류율 설정했기 때문에 크게 벗어나는 결과를 얻게 됨\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\",0.1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53f061",
   "metadata": {},
   "source": [
    "### 7.1.4 first와 last\n",
    "- first : DataFrame의 첫 번째 값\n",
    "- last : DataFrame의 마지막 값\n",
    "  - 위의 함수들은 값이 아닌 로우를 기반으로 동작함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a129fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          22138|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "df.select(first(\"StockCode\"),last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678acd65",
   "metadata": {},
   "source": [
    "### 7.1.5 min과 max\n",
    "- min : 최솟값\n",
    "- max : 최댓값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c98c2555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min,max\n",
    "df.select(min(\"Quantity\"),max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b6c63",
   "metadata": {},
   "source": [
    "### 7.1.6 sum\n",
    "- sum\n",
    "  - DataFrame에서 특정 컬럼의 모든 값 합산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf1eae6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946dd115",
   "metadata": {},
   "source": [
    "### 7.1.7 sumDistinct\n",
    "- sumDistinct\n",
    "  - 특정 컬럼의 모든 값을 합산하는 방법 외에도 고윳값을 합산할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e84b8a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab6301",
   "metadata": {},
   "source": [
    "### 7.1.8 avg\n",
    "- avg\n",
    "  - sum 함수의 결과를 count 함수의 결과로 나누어 평균값을 구할 수 있음\n",
    "  - 스파크의 avg 함수나 mean 함수를 사용하면 평균값을 더 쉽게 구할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbf31294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#집계된 컬렁믈 재활용하기 위해 alias 메서드를 사용함\n",
    "from pyspark.sql.functions import sum,count,avg,expr\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
    "    .selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3929a2",
   "metadata": {},
   "source": [
    "### 7.1.9 분산과 표준편차\n",
    "- 분산과 표준편차: 평균 주변에 데이터가 분포된 정도를 측정하는 방법\n",
    "  - 분산 : 평균과의 차이를 제곱한 결과의 평균\n",
    "  - 표준편차 : 분산의 제곱근\n",
    "- 스파크에서 분산과 표준편차 계산 가능\n",
    "  - 표본표준편차뿐만 아니라 모표준편차 방식도 지원하기 때문에 주의가 필요함\n",
    "    - variance : 표본표준분산\n",
    "    - stddev : 표본표준편차\n",
    "    - var_pop : 모표준분산\n",
    "    - stddev_pop : 모표준편차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65715bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+---------------------+\n",
      "| var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "|47559.303646609056|47559.391409298754|  218.08095663447796|   218.08115785023418|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "df.select(var_pop(\"Quantity\"),var_samp(\"Quantity\"),\n",
    "         stddev_pop(\"Quantity\"),stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1e44bb",
   "metadata": {},
   "source": [
    "### 7.1.10 비대칭도와 첨도\n",
    "- 비대칭도와 첨도 : 모두 데이터의 변곡점을 측정하는 방법\n",
    "  - 확률변수와 화귤분포로 데이터를 모델링할 때 특히 중요함\n",
    "- 비대칭도 : 데이터 평균의 비대칭 정도를 측정\n",
    "- 첨도 : 데이터 끝 부분을 측정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42bd384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "| skewness(Quantity)|kurtosis(Quantity)|\n",
      "+-------------------+------------------+\n",
      "|-0.2640755761052562|119768.05495536952|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness,kurtosis\n",
    "df.select(skewness(\"Quantity\"),kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd801a",
   "metadata": {},
   "source": [
    "### 7.1.11 공분산과 상관관계\n",
    "- 두 컬럼값 사이의 영향도를 비교하는 함수\n",
    "- cov : 공분산\n",
    "  - 데이터 입력값에 따라 다른 범위를 가짐\n",
    "- corr : 상관관계\n",
    "  - 피어슨 상관계수를 측정함\n",
    "  - -1과 1 사이의 값을 가짐\n",
    "  - 모집단이나 표본에 대한 계산 개념이 없음\n",
    "- var 함수처럼 표본공분산 방식이나 모공분산 방식으로 공분산을 계산할 수도 있음\n",
    "  - 사용하고자 하는 방식을 명확하게 지정하는 것이 좋음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e99cef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085635685E-4|             1052.7280543902734|            1052.7260778741693|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr,covar_pop,covar_samp\n",
    "\n",
    "df.select(corr(\"InvoiceNo\",\"Quantity\"),covar_samp(\"InvoiceNo\",\"Quantity\"),\n",
    "         covar_pop(\"InvoiceNo\",\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f17c08f",
   "metadata": {},
   "source": [
    "### 7.1.12 복합 데이터 타입의 집계\n",
    "- 스파크는 수식을 이용한 집계뿐만 아니라 복합 데이터 타입을 사용해 집계를 수행할 수 있음\n",
    "  - ex) 특정 컬럼의 값을 리스트로 수집하거나 셋 데이터 타입으로 고윳값만 수집할 수 있음\n",
    "- 수집된 데이터는 처리 파이프라인에서 다양한 프로그래밍 방식으로 다루거나 사용자 정의 함수를 사용해 전체 데이터에 접근할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67e1d53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set,collect_list\n",
    "df.agg(collect_set(\"Country\"),collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc41e9d5",
   "metadata": {},
   "source": [
    "## 7.2 그룹화\n",
    "- 데이터 그룹 기반의 집계를 수행하는 경우\n",
    "  - 단일 컬럼의 데이터를 그룹화하고 해당 그룹의 다른 여러 컬럼을 사용해서 계산하기 위해 카테고리형 데이터를 사용함\n",
    "- 데이터 그룹 기반의 집계를 설명하는 데 가장 좋은 방법 : 그룹화\n",
    "  - 이전에 했던 것처럼 카운트를 가장 먼저 수행함\n",
    "    - 해당 연산은 또 다른 DataFrame을 반환하여 지연 처리 방식으로 수행됨\n",
    "- 그룹화 작업 두 단계로 이루어짐\n",
    "  - 1. 하나 이상의 컬럼을 그룹화 : RelationalGroupedDataset이 반환\n",
    "  - 2. 집계 연산을 수행 : DataFrame이 반환됨\n",
    "  \n",
    "- 그룹의 기준이 되는 컬럼을 여러 개 지정할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0f7be11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "|   538800|     16458|   10|\n",
      "|   538942|     17346|   12|\n",
      "|  C539947|     13854|    1|\n",
      "|   540096|     13253|   16|\n",
      "|   540530|     14755|   27|\n",
      "|   541225|     14099|   19|\n",
      "|   541978|     13551|    4|\n",
      "|   542093|     17677|   16|\n",
      "|   536596|      null|    6|\n",
      "|   537252|      null|    1|\n",
      "|   538041|      null|    1|\n",
      "|   543188|     12567|   63|\n",
      "|   543590|     17377|   19|\n",
      "|  C543757|     13115|    1|\n",
      "|  C544318|     12989|    1|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\",\"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceccf01b",
   "metadata": {},
   "source": [
    "### 7.2.1 표현식을 이용한 그룹화\n",
    "- 카운팅\n",
    "  - 메서드로 사용할 수 있으므로 조금 특별함\n",
    "  - 메서드 대신 count 함수를 사용할 것을 추천함\n",
    "  - count 함수를 select 구문에 표현식으로 지정하는 것보다 agg 메서드를 사용하는 것이 좋음\n",
    "- agg 메서드\n",
    "  - 여러 집계 처리를 한 번에 지정할 수 있으며 집계에 표현식을 사용할 수 있음\n",
    "  - 트랜스포메이션이 완료된 컬럼에 alias 메서드를 사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd4381df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536596|   6|              6|\n",
      "|   536938|  14|             14|\n",
      "+---------+----+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    count(\"Quantity\").alias(\"quan\"),\n",
    "    expr(\"count(Quantity)\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e855a7a2",
   "metadata": {},
   "source": [
    "### 7.2.2 맵을 이용한 그룹화\n",
    "- 컬럼을 키로, 수행할 집계 함수의 문자열을 값으로 하는 맵 타입을 사용해 트랜스포메이션을 정의할 수 있음\n",
    "- 수행할 집계 함수를 한 줄로 작성하면 여러 컬럼명을 재사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "981c5547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536596|               1.5|  1.1180339887498947|\n",
      "|   536938|33.142857142857146|  20.698023172885524|\n",
      "|   537252|              31.0|                 0.0|\n",
      "|   537691|              8.15|   5.597097462078001|\n",
      "|   538041|              30.0|                 0.0|\n",
      "|   538184|12.076923076923077|   8.142590198943392|\n",
      "|   538517|3.0377358490566038|  2.3946659604837897|\n",
      "|   538879|21.157894736842106|  11.811070444356483|\n",
      "|   539275|              26.0|  12.806248474865697|\n",
      "|   539630|20.333333333333332|  10.225241100118645|\n",
      "|   540499|              3.75|  2.6653642652865788|\n",
      "|   540540|2.1363636363636362|  1.0572457590557278|\n",
      "|  C540850|              -1.0|                 0.0|\n",
      "|   540976|10.520833333333334|   6.496760677872902|\n",
      "|   541432|             12.25|  10.825317547305483|\n",
      "|   541518| 23.10891089108911|  20.550782784878713|\n",
      "|   541783|11.314285714285715|   8.467657556242811|\n",
      "|   542026| 7.666666666666667|   4.853406592853679|\n",
      "|   542375|               8.0|  3.4641016151377544|\n",
      "|  C542604|              -8.0|  15.173990905493518|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4768f251",
   "metadata": {},
   "source": [
    "## 7.3 윈도우 함수\n",
    "- 윈도우 함수\n",
    "  - 집계를 사용할 수도 있음\n",
    "  - 데이터의 특정 윈도우를 대상으로 고유의 집계 연산을 수행함\n",
    "  - 데이터의 '윈도우' : 현재 데이터에 대한 참조를 사용해 정의함\n",
    "  - 윈도우 명세 : 함수에 전달될 로우를 결정함\n",
    "  - group-by 함수와 유사해보일 수도 있으나 둘의 차이점이 존재\n",
    "\n",
    "- group-by 함수를 사용하면 모든 로우 레코드가 단일 그룹으로만 이동함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfadf2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
