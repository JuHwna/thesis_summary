{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2122a3c",
   "metadata": {},
   "source": [
    "# 5장. 구조적 API 기본 연산\n",
    "- 구조적 API의 아키텍처 개념에서 벗어나 DataFrame과 DataFrame의 데이터를 다루는 기능을 소개\n",
    "  - DataFrame의 기본 기능을 중점적으로 다룸\n",
    "- DataFrame은 Row 타입의 레코드(테이블의 로우 같은)와 각 레코드에 수행할 연산 표현식을 나타내는 여러 컬럼(스프레드시트의 컬럼 같은)으로 구성됨\n",
    "  - 스키마 : 각 컬럼명과 데이터 타입을 정의\n",
    "  - DataFrame의 파티셔닝 : DataFrame이나 Dataset이 클러스터에서 물리적으로 배치되는 형태를 정의\n",
    "    - 파티셔닝 스키마 : 파티션을 배치하는 방법을 정의함\n",
    "    - 파티셔닝의 분할 기준 : 특정 컬럼이나 비결정론적 값(매번 변하는 값)을 기반으로 설정할 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a6db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"sample\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c612f283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame을 생성함\n",
    "df=spark.read.format(\"json\")\\\n",
    ".load(\"./Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c496c4",
   "metadata": {},
   "source": [
    "- DataFrame은 컬럼을 가지며 스키마로 컬럼을 정의함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab1c593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcde5b3",
   "metadata": {},
   "source": [
    "- 스키마는 관련된 모든 것을 하나로 묶는 역할을 함\n",
    "\n",
    "## 5.1. 스키마\n",
    "- 스키마\n",
    "  - DataFrame의 컬럼명과 데이터 타입을 정의함\n",
    "  - 데이터소스에서 스키마를 얻거나 직접 정의할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3095e61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataFrame을 생성함\n",
    "spark.read.format(\"json\")\\\n",
    "    .load(\"./Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")\\\n",
    "    .schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9381807",
   "metadata": {},
   "source": [
    "- 스키마\n",
    "  - 여러 개의 StructField 타입 필드로 구성된 StructType 객체\n",
    "- StructField\n",
    "  - 이름, 데이터 타입, 컬럼이 값이 없거나 null일 수 있는지 지정하는 불리언값을 가짐\n",
    "  - 필요한 경우, 컬럼과 관련된 메타데이터를 지정할 수도 있음\n",
    "- 메타데이터\n",
    "  - 해당 컬럼과 관련된 정보이며 스파크의 머신러닝 라이브러리에서 사용함\n",
    "- 스키마는 복합 데이터 타입인 StructType을 가질 수 있음\n",
    "  - 스파크는 런타임에 데이터 타입이 스키마의 데이터 타입과 일치하지 않으면 오류를 발생시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b53e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame에 스키마를 만들고 적용하는 예제\n",
    "from pyspark.sql.types import StructField,StructType,StringType,LongType\n",
    "myManualSchema=StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\",StringType(),True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\",StringType(),True),\n",
    "    StructField(\"count\",LongType(),False,metadata={\"hello\":\"world\"})\n",
    "])\n",
    "df=spark.read.format(\"json\").schema(myManualSchema)\\\n",
    "    .load(\"./Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53fac8c",
   "metadata": {},
   "source": [
    "- 스파크는 자체 데이터 타입 정보를 사용함\n",
    "  - 프로그래밍 언어의 데이터 타입을 스파크의 데이터 타입으로 설정할 수 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c3d115",
   "metadata": {},
   "source": [
    "## 5.2. 컬럼과 표현식\n",
    "- 스파크의 컬럼 \n",
    "  - 스프레드시트, R의 dataframe, Pandas의 DataFrame 컬럼과 유사함\n",
    "  - 사용자는 표현식으로 DataFrame의 컬럼을 선택, 조작, 제거할 수 있음\n",
    "  - 표현식을 사용해 레코드 단위로 계산한 값을 단순하게 나타내는 논리적 구조\n",
    "  - 컬럼의 실젯값을 얻으려면 로우가 필요, 로우를 얻으러면 DataFrame이 필요함\n",
    "    - DataFrame을 통하지 않으면 외부에서 컬럼에 접근할 수 없음\n",
    "    - 컬럼 내용을 수정하려면 반드시 DataFrame의 스파크 트랜스포메이션을 사용해야 함\n",
    "\n",
    "\n",
    "### 5.2.1. 컬럼\n",
    "- col함수나 column 함수 : 컬럼을 생성하고 참조할 수 있는 간단한 방법\n",
    "  - 이들 함수는 컬럼명을 인수로 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "022d5712",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e67c14c07b08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"someColumnName\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcolumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"someColumnName\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\functions.py\u001b[0m in \u001b[0;36mcolumn\u001b[1;34m(col)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mbased\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0;31m'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \"\"\"\n\u001b[1;32m--> 114\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,column\n",
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a3813e",
   "metadata": {},
   "source": [
    "- 컬럼이 DataFrame에 있을지 없을지는 알 수 없음\n",
    "  - 컬럼은 컬럼명을 카탈로그에 저장된 정보와 비교하기 전까지 미확인 상태로 남음\n",
    "  - 분석기가 동작하는 단계에서 컬럼과 테이블을 분석함\n",
    "  \n",
    "\n",
    "\n",
    "#### 명시적 컬럼 참조\n",
    "- DataFrame의 컬럼 : col 메서드로 참조함\n",
    "  - col 메서드 : 조인 시 유용함\n",
    "    - ex) DataFrame의 컬럼을 다른 DataFrame의 조인 대상 컬럼에서 참조하기 위해 col 메서드를 사용함\n",
    "    - 이 메서드를 사용해 명시적으로 컬럼을 정의하면 스파크는 분석기 실행 단계에서 컬럼 확인 절차를 생략함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a42b824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'count'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.col(\"count\") -> 이건 scala나 java에서 사용\n",
    "df[\"count\"] # -> 파이썬에서는 이렇게 써야 한다고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7e567f",
   "metadata": {},
   "source": [
    "### 5.2.2. 표현식\n",
    "- 컬럼은 표현식\n",
    "- 표현식\n",
    "  - DataFrame 레코드의 여러 값에 대한 트랜스포메이션 집합을 의미\n",
    "  - 여러 컬럼명을 입력으로 받아 식별하고 '단일 값'을 만들기 위해 다양한 표현식을 각 레코드에 적용하는 함수라고 생각할 수 있음\n",
    "    - '단일 값' : Map이나 Array같은 복합 데이터 타입일 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e4908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
