{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63391a61",
   "metadata": {},
   "source": [
    "### 설치 방법\n",
    "- https://sparkdia.tistory.com/65\n",
    "- https://spidyweb.tistory.com/199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae07586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46caf718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"sample\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1157432",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "377ddf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame 생성 및 한 개의 컬럼과 1000개의 로우로 구성\n",
    "# 해당 숫자들은 분산 컬렉션을 나타냄\n",
    "myRange=spark.range(1000).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a890dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>number</th></tr>\n",
       "<tr><td>0</td></tr>\n",
       "<tr><td>1</td></tr>\n",
       "<tr><td>2</td></tr>\n",
       "<tr><td>3</td></tr>\n",
       "<tr><td>4</td></tr>\n",
       "<tr><td>5</td></tr>\n",
       "<tr><td>6</td></tr>\n",
       "<tr><td>7</td></tr>\n",
       "<tr><td>8</td></tr>\n",
       "<tr><td>9</td></tr>\n",
       "<tr><td>10</td></tr>\n",
       "<tr><td>11</td></tr>\n",
       "<tr><td>12</td></tr>\n",
       "<tr><td>13</td></tr>\n",
       "<tr><td>14</td></tr>\n",
       "<tr><td>15</td></tr>\n",
       "<tr><td>16</td></tr>\n",
       "<tr><td>17</td></tr>\n",
       "<tr><td>18</td></tr>\n",
       "<tr><td>19</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+------+\n",
       "|number|\n",
       "+------+\n",
       "|     0|\n",
       "|     1|\n",
       "|     2|\n",
       "|     3|\n",
       "|     4|\n",
       "|     5|\n",
       "|     6|\n",
       "|     7|\n",
       "|     8|\n",
       "|     9|\n",
       "|    10|\n",
       "|    11|\n",
       "|    12|\n",
       "|    13|\n",
       "|    14|\n",
       "|    15|\n",
       "|    16|\n",
       "|    17|\n",
       "|    18|\n",
       "|    19|\n",
       "+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6a4841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#스키마 : 컬럼과 컬럼의 타입을 정의한 목록"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5865fd29",
   "metadata": {},
   "source": [
    "## 스마크의 핵심 데이터 구조\n",
    "- 불편성 : 한번 생성하면 변경할 수 없음\n",
    "- 트랜스포메이션 : DataFrame을 변경하기 위해 사용하는 명령"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f9f49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2=myRange.where(\"number%2=0\")\n",
    "#코드를 실행해도 결과는 출력되지 않음\n",
    "#추상적인 트랜스포메이션만 지정한 상태여서 액션을 호출하지 않으면 \n",
    "#실제 트랜스포메이션을 수행하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad85632b",
   "metadata": {},
   "source": [
    "### 트랜스포메이션의 두 가지 유형\n",
    "- 좁은 의존성 : 각 입력 파티션이 하나의 출력 파티션에만 영향을 미침\n",
    "  - 하나의 파티션이 하나의 출력 파티션에만 영향을 미침\n",
    "  - 스파크에서 파이프라이닝을 자동으로 수행함\n",
    "    - 파이프라이닝이란? : 명령어를 순차적으로 실행하는 프로세서에 적용되는 기술로, 한 번에 하나의 명령어만 실행하는 것이 아니라 하나의 명령어가 실행되는 도중에 다른 명령어를 실행을 시작하는 식으로 동시에 여러 개의 명령어를 실행하는 기법\n",
    "    - DataFrame에 여러 필터를 지정하는 경우, 모든 작업이 메모리에서 일어남\n",
    "  - where 구문: 좁은 의존성을 가짐\n",
    "- 넓은 의존성 : 하나의 입력 파티션이 여러 출력 파티션에 영향을 미침\n",
    "  - 셔플 : 스파크가 클러스터에서 파티션을 교환하는 것\n",
    "  - 스파크의 경우, 셔플의 결과를 디스크에 저장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625a8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2=myRange.where(\"number%2=0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69137691",
   "metadata": {},
   "source": [
    "### 지연 연산\n",
    "- 지연 연산 : 스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식을 의미\n",
    "  - 스파크는 특정 연산 명령이 내려진 즉시 데이터를 수정하지 않고 원시 데이터에 적용할 트랜스포메이션의 실행 계획을 생성함\n",
    "  - 코드를 실행하는 마지막 순간까지 대기하다가 원형 DataFrame 트랜스포메이션을 간결한 물리적 실행 계획을 컴파일함\n",
    "  - 이 과정을 거치며 전체 데이터 흐름을 최적화함\n",
    "  - ex) DataFrame의 조건절 푸시다운 : 필터를 데이터소스로 위임하는 최적화 작업 수행\n",
    "  \n",
    "### 액션\n",
    "- 트랜스포메이션을 사용해 논리적 실행 계획을 세울 수 있음\n",
    "- 하지만 실제 연산을 수행하려면 액션 명령을 내려야함\n",
    "- 액션 : 일련의 트랜스포메이션으로부터 결과를 계산하도록 지시하는 명령\n",
    "  - count : DataFrame의 전체 레코드 수를 반환함\n",
    "  - 세 가지 유형의 액션\n",
    "    - 콘솔에서 데이터를 보는 액션\n",
    "    - 각 언어로 된 네이티브 객체에 데이터를 모으는 액션\n",
    "    - 출력 데이터소스에 저장하는 액션\n",
    "  - 액션을 지정하면 스파크 잡이 시작됨\n",
    "- 스파크 잡(job) : 필터(좁은 트랜스포메이션)를 수행한 후 파티션별로 레코드 수를 카운트(넓은 트랜스포메이션)함\n",
    "  - 각 언어에 적합한 네이티브 객체에 결과를 모음\n",
    "  - 스파크가 제공하는 스파크 UI로 클러스터에서 실행 중인 스파크 잡을 모니터링할 수 있음\n",
    "  \n",
    "  \n",
    "### 스파크 UI\n",
    "- 스파크 UI\n",
    "  - 스파크 잡의 진행 상황을 모니터링할 때 사용함\n",
    "  - 드라이버 노드 4040 포트로 접속할 수 있음\n",
    "  - 로컬 모드 시 주소 : http://localhost:4040\n",
    "  - 스파크 잡의 상태, 환경설정, 클러스터 상태 등의 정보를 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "048f597f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisBy2.count()\n",
    "#count 메서드 : DataFrame의 전체 레코드 수를 반환함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271e3e2",
   "metadata": {},
   "source": [
    "## 종합 예제\n",
    "- 미국 교통통계국의 항공운항 데이터 중 일부\n",
    "\n",
    "\n",
    "- 다양한 데이터 소스 지원\n",
    "  - SparkSession의 DataFrameReader 클래스를 사용해서 읽음\n",
    "  - 특정 파일 포맷과 몇 가지 옵션을 함께 설정함\n",
    "  - 스키마 추론 기능과 파일의 첫 로우를 헤더로 지정하는 옵션도 함께 설정함\n",
    "- 스키마 정보를 얻기 위해 데이터를 조금 읽음\n",
    "  - 해당 로우의 데이터 타입을 스파크 데이터 타입에 맞게 분석함\n",
    "  - 운영 환경에서는 데이터를 읽는 시점에 스키마를 엄격하게 지정하는 옵션을 사용해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ccf525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015=spark\\\n",
    ".read\\\n",
    ".option(\"inferSchma\",\"true\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".csv(\"./Spark-The-Definitive-Guide-master/data/flight-data/csv/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7fd2377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "157e7caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>DEST_COUNTRY_NAME</th><th>ORIGIN_COUNTRY_NAME</th><th>count</th><th>string</th></tr>\n",
       "<tr><td>United States</td><td>Romania</td><td>15</td><td>15</td></tr>\n",
       "<tr><td>United States</td><td>Croatia</td><td>1</td><td>1</td></tr>\n",
       "<tr><td>United States</td><td>Ireland</td><td>344</td><td>344</td></tr>\n",
       "<tr><td>Egypt</td><td>United States</td><td>15</td><td>15</td></tr>\n",
       "<tr><td>United States</td><td>India</td><td>62</td><td>62</td></tr>\n",
       "<tr><td>United States</td><td>Singapore</td><td>1</td><td>1</td></tr>\n",
       "<tr><td>United States</td><td>Grenada</td><td>62</td><td>62</td></tr>\n",
       "<tr><td>Costa Rica</td><td>United States</td><td>588</td><td>588</td></tr>\n",
       "<tr><td>Senegal</td><td>United States</td><td>40</td><td>40</td></tr>\n",
       "<tr><td>Moldova</td><td>United States</td><td>1</td><td>1</td></tr>\n",
       "<tr><td>United States</td><td>Sint Maarten</td><td>325</td><td>325</td></tr>\n",
       "<tr><td>United States</td><td>Marshall Islands</td><td>39</td><td>39</td></tr>\n",
       "<tr><td>Guyana</td><td>United States</td><td>64</td><td>64</td></tr>\n",
       "<tr><td>Malta</td><td>United States</td><td>1</td><td>1</td></tr>\n",
       "<tr><td>Anguilla</td><td>United States</td><td>41</td><td>41</td></tr>\n",
       "<tr><td>Bolivia</td><td>United States</td><td>30</td><td>30</td></tr>\n",
       "<tr><td>United States</td><td>Paraguay</td><td>6</td><td>6</td></tr>\n",
       "<tr><td>Algeria</td><td>United States</td><td>4</td><td>4</td></tr>\n",
       "<tr><td>Turks and Caicos ...</td><td>United States</td><td>230</td><td>230</td></tr>\n",
       "<tr><td>United States</td><td>Gibraltar</td><td>1</td><td>1</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+-------------------+-----+------+\n",
       "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|string|\n",
       "+--------------------+-------------------+-----+------+\n",
       "|       United States|            Romania|   15|    15|\n",
       "|       United States|            Croatia|    1|     1|\n",
       "|       United States|            Ireland|  344|   344|\n",
       "|               Egypt|      United States|   15|    15|\n",
       "|       United States|              India|   62|    62|\n",
       "|       United States|          Singapore|    1|     1|\n",
       "|       United States|            Grenada|   62|    62|\n",
       "|          Costa Rica|      United States|  588|   588|\n",
       "|             Senegal|      United States|   40|    40|\n",
       "|             Moldova|      United States|    1|     1|\n",
       "|       United States|       Sint Maarten|  325|   325|\n",
       "|       United States|   Marshall Islands|   39|    39|\n",
       "|              Guyana|      United States|   64|    64|\n",
       "|               Malta|      United States|    1|     1|\n",
       "|            Anguilla|      United States|   41|    41|\n",
       "|             Bolivia|      United States|   30|    30|\n",
       "|       United States|           Paraguay|    6|     6|\n",
       "|             Algeria|      United States|    4|     4|\n",
       "|Turks and Caicos ...|      United States|  230|   230|\n",
       "|       United States|          Gibraltar|    1|     1|\n",
       "+--------------------+-------------------+-----+------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "# import org.apache.spark.sql.functions.col\n",
    "# import org.apache.spark.sql.types.IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "flightData2015.withColumn(\"string\", col(\"count\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c7d4af",
   "metadata": {},
   "source": [
    "- 스칼라와 파이썬에서 사용하는 DataFrame : 불특정 다수의 로우와 칼럼을 가짐\n",
    "  - 로우의 수를 알 수 없는 이유 : 데이터를 읽는 과정이 지연 연산 형태의 트랜스포메이션이기 때문\n",
    "  - 각 컬럼의 데이터 타입을 추론하기 위해 적은 양의 데이터를 읽음\n",
    "  - DataFrame에서 csv파일을 읽어 로컬 배열이나 리스트 형태로 변환하는 과정\n",
    "    - csv파일 -> read -> DataFrame -> take(n) -> Array(row(....),row(....))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b6f63c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count='15'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count='344')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4fbf08",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "- sort 메서드는 DataFrame을 변경하지 않음\n",
    "  - 트랜스포메이션으로 sort 메서드를 사용하면 이전의 DataFrame을 변환해 새로운 DataFrmae을 생성해 반환함\n",
    "  - sort 메서드는 단지 트랜스포메이션이기 때문에 호출 시 데이터에 아무런 변화도 일어나지 않음\n",
    "    - 실행 계획을 만들고 검토하여 클러스터에서 처리할 방법을 알아냄\n",
    "    - 특정 DataFrame 객체에 explain 메서드를 호출하면 DataFrame의 계보나 스파크의 쿼리 실행 계획을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d59afd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Sort [count#38 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#38 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#81]\n",
      "   +- FileScan csv [DEST_COUNTRY_NAME#36,ORIGIN_COUNTRY_NAME#37,count#38] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/hwan0/Spark-The-Definitive-Guide-master/data/flight-data/csv/201..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e848fb3",
   "metadata": {},
   "source": [
    "- 실행 계획은 위에서 아래 방향으로 읽으며 최종 결과는 가장 위에, 데이터 소스는 가장 아래에 있음\n",
    "  - 각 줄의 첫 번째 키워드(Sort, Exchange, FileScan) 주목\n",
    "    - 특정 칼럼을 다른 컬럼과 비교하는 sort 메서드가 넓은 트랜스포메이션으로 동작하는 것을 볼 수 있음\n",
    "    - 실행 계획은 디버깅과 스파크의 실행 과정을 이해하는 데 도움을 주는 도구일 뿐\n",
    "- 트랜스포메이션 실행 계획을 시작하기 위해 액션을 호출함\n",
    "  - 액션을 실행하려면 몇 가지 설정이 필요함\n",
    "  - 스파크는 셔플 수행 시 기본적으로 200개의 셔플 파티션을 생성함\n",
    "  - 이 값을 5로 설정해 셔플의 출력 파티션 수를 줄임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1ba3b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count='1'),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count='1')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")\n",
    "flightData2015.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d7d351",
   "metadata": {},
   "source": [
    "- 스파크의 프로그래밍 모델인 함수형 프로그래밍의 핵심 : 계보를 통해 입력 데이터에 수행한 연산을 전체 파티션에서 어떻게 재연산하는지 알 수 있음\n",
    "  - 함수형 프로그래밍 : 데이터의 변환 규칙이 일정한 경우 같은 입력에 대해 항상 같은 출력을 생성함\n",
    "  \n",
    "- 사용자는 물리적 데이터를 직접 다루지 않음\n",
    "  - 대신 앞서 설정한 셔플 파티션 파라미터와 같은 속성으로 물리적 실행 특성을 제어함\n",
    "  - 앞서 셔플 파티션 수를 5로 설정했기 때문에 5개의 출력 파티션이 생성됨\n",
    "  - 이 값을 변경하면 스파크 잡의 실제 실행 특성을 제어할 수 있음\n",
    "  - 스파크 UI에 접속해 잡의 실행 상태와 스파크 잡의 물리적, 논리적 실행 특성을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b7308",
   "metadata": {},
   "source": [
    "### DataFrame과 SQL\n",
    "- 스파크는 언어에 상관없이 같은 방식으로 트랜스포메이션을 실행할 수 있음\n",
    "  - 사용자가 SQL이나 DataFrame(R, 파이썬, 스칼라, 자바에서)으로 비즈니스 로직을 표현하면 스파크에서 실제 코드를 실행하기 전에 그 로직을 기본 실행 계획(explain 메서드를 호출해 실행 계획을 확인할 수 있음)으로 컴파일함\n",
    "  - 스파크 SQL을 사용하면 모든 DataFrame을 테이블이나 뷰(임시 테이블)로 등록한 후 SQL쿼리를 사용할 수 있음\n",
    "  - SQL 쿼리를 DataFrame 코드와 같은 실행 계획으로 컴파일하므로 둘 사이의 성능 차이는 없음\n",
    "  - createOrReplaceTempView 메서드를 호출하면 모든 DataFrame을 테이블이나 뷰로 만들 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99742f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0132dc10",
   "metadata": {},
   "source": [
    "- 새로운 DataFrame을 반환하는 spark.sql 메서드로 SQL 쿼리를 실행함\n",
    "  - spark는 SparkSession의 변수\n",
    "  - DataFrame에 쿼리를 수행하면 새로운 DataFrame을 반환함\n",
    "  - 로직을 작성할 때 반복적인 느낌이 들지만 매우 효율적임\n",
    "  - 사용자는 어떤 상황에서든 가장 편리한 방식으로 트랜스포메이션을 지정할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d390dd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#36], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#36, 5), ENSURE_REQUIREMENTS, [id=#110]\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#36], functions=[partial_count(1)])\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#36] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/hwan0/Spark-The-Definitive-Guide-master/data/flight-data/csv/201..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#36], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#36, 5), ENSURE_REQUIREMENTS, [id=#129]\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#36], functions=[partial_count(1)])\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#36] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/hwan0/Spark-The-Definitive-Guide-master/data/flight-data/csv/201..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay=spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME,COUNT(1)\n",
    "FROM  flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "dataFrameWay=flightData2015\\\n",
    ".groupby(\"DEST_COUNTRY_NAME\")\\\n",
    ".count()\n",
    "sqlWay.explain()\n",
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6868f7",
   "metadata": {},
   "source": [
    "- 스파크는 빅데이터 문제를 빠르게 해결하는 데 필요한 수백 개의 함수를 제공함\n",
    "  - max 함수 : DataFrame의 특정 컬럼 값을 스캔하면서 이전 최댓값보다 더 큰 값을 찾음\n",
    "    - 필터링을 수행해 단일 로우를 결과로 반환하는 트랜스포메이션임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31db4e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)='986')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT MAX(count) FROM flight_data_2015\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a32e67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)='986')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "flightData2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ce8c1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|         411352.0|\n",
      "|           Canada|           8399.0|\n",
      "|           Mexico|           7140.0|\n",
      "|   United Kingdom|           2025.0|\n",
      "|            Japan|           1548.0|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql=spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME,SUM(count) AS destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16ded9a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"count\" is not a numeric column. Aggregation function can only be applied on a numeric column.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-1dcebc9b405f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mflightData2015\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DEST_COUNTRY_NAME\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"count\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sum(count)\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"destination_total\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"destination_total\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\group.py\u001b[0m in \u001b[0;36m_api\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"count\" is not a numeric column. Aggregation function can only be applied on a numeric column."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "flightData2015\\\n",
    ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".sum(\"count\")\\\n",
    ".withColumnRenamed(\"sum(count)\",\"destination_total\")\\\n",
    ".sort(desc(\"destination_total\"))\\\n",
    ".limit(5)\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60e56a8",
   "metadata": {},
   "source": [
    "- 지향성 비순환 그래프\n",
    "  - 실행 계획 단계\n",
    "  - 액션이 호출되면 결과를 만들어냄\n",
    "  - 각 단계는 불편성을 가진 신규 DataFrame을 생성함\n",
    "    1. 첫 번째 단계\n",
    "       - 데이터를 읽음\n",
    "       - 스파크는 해당 DataFrame이나 자신의 원본 DataFrame에 액션이 호출되기 전까지 데이터를 읽지 않음\n",
    "    2. 두 번째 단계\n",
    "       - 데이터를 그룹화함\n",
    "       - groupBy 메서드 호출되면 최종적으로 그룹화된 DataFrame을 지칭하는 이름을 가진 RelationalGroupedDataset을 반환함\n",
    "       - 기본적으로 키 혹은 키셋을 기준으로 그룹을 만들고 각 키에 대한 집계를 수행함\n",
    "    3. 세 번째 단계\n",
    "       - 집계 유형을 지정하기 위해 컬럼 표현식이나 컬럼명을 인수로 사용하는 sum 메서드를 사용함\n",
    "       - sum 메서드 : 새로운 스키마 정보를 가지는 별도의 DataFrame을 생성함\n",
    "       - 신규 스키마에는 새로 만들어진 각 컬럼의 데이터 타입 정보가 들어 있음\n",
    "       - sum 메서드 역시 트랜스포메이션이므로 아무런 연산도 일어나지 않음\n",
    "       - 새롭게 생성된 결과 스키마를 통해 타입 정보를 추적함\n",
    "    4. 네 번째 단계\n",
    "       - 컬럼명을 변경함\n",
    "       - withColumnRenamed 메서드에 원본 컬럼명과 신규 컬럼명을 인수로 지정함(트랜스포메이션)\n",
    "       - 여전히 연산은 일어나지 않음\n",
    "    5. 다섯 번째 단계\n",
    "       - 데이터를 정렬함\n",
    "       - 결과 DataFrame의 첫 번째 로우를 확인해보면 destination_total 컬럼에서 가장 큰 값을 가짐\n",
    "       - desc 함수\n",
    "         - 역순으로 정렬하기 위해 import함\n",
    "         - 문자열이 아닌 Column 객체를 반환함\n",
    "         - DataFrame 메서드 중 대부분은 문자열 형태의 컬럼명, Column 타입 그리고 표현식을 파라미터로 사용함(Column과 표현식은 사실상 같은 것)\n",
    "    6. 여섯 번째 단계\n",
    "        - limit 메서드로 반환 결과의 수를 제한함\n",
    "        - DataFrame의 전체 데이터 대신 상위 5개 로우를 반환함\n",
    "    7. 액션을 수행함\n",
    "        - DataFrame의 결과를 모으는 프로세스 시작\n",
    "        - 처리가 끝나면 코드를 작성한 프로그래밍 언어에 맞는 리스트나 배열을 반환함\n",
    "   - explain 메서드를 호출해 실행 계획 살필 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08841b45",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"count\" is not a numeric column. Aggregation function can only be applied on a numeric column.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-0d417e5a1677>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mflightData2015\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DEST_COUNTRY_NAME\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"count\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sum(count)\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"destination_total\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"destination_total\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\group.py\u001b[0m in \u001b[0;36m_api\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"count\" is not a numeric column. Aggregation function can only be applied on a numeric column."
     ]
    }
   ],
   "source": [
    "flightData2015\\\n",
    ".groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".sum(\"count\")\\\n",
    ".withColumnRenamed(\"sum(count)\",\"destination_total\")\\\n",
    ".sort(desc(\"destination_total\"))\\\n",
    ".limit(5)\\\n",
    ".explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69fa6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
