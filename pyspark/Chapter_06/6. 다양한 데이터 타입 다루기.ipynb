{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d129b3",
   "metadata": {},
   "source": [
    "# Chapter 6. 다양한 데이터 타입 다루기\n",
    "- 스파크의 표현식을 만드는 방법\n",
    "- 스파크의 다양한 데이터 타입을 다루는 방법\n",
    "  - 불리언 타입\n",
    "  - 수치 타입\n",
    "  - 문자열 타입\n",
    "  - date와 timestamp 타입\n",
    "  - null값 다루기\n",
    "  - 복합 데이터 타입\n",
    "  - 사용자 정의 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb893da3",
   "metadata": {},
   "source": [
    "## 6.1 API는 어디서 찾을까\n",
    "- 스파크는 현재 활발하게 성장하고 있는 프로젝트\n",
    "  - 데이터 변환용 함수를 어떻게 찾는지 알아야 함\n",
    "  - 데이터 변환용 함수를 찾기 위해 핵심적으로 보아야 할 부분\n",
    "    - DataFrame(Dataset) 메서드\n",
    "      - DataFrame은 Row 타입을 가진 Dataset임, 결국에는 Dataset 메서드를 만나게 됨\n",
    "      - DataFrameStatFunctions와 DataFrameNaFunctions 등 Dataset의 하위 모듈은 다양한 메서드를 제공함\n",
    "        - 해당 메서드를 사용해 여러 가지 문제를 해결할 수 있음\n",
    "        - DataFrameStatFuctions : 다양한 통계적 함수를 제공함\n",
    "        - DataFrameNaFuctions : null 데이터를 다루는 데 필요한 함수 제공\n",
    "    - Column 메서드\n",
    "      - alias나 contains 같이 컬럼과 관련된 여러 가지 메서드를 제공함\n",
    "      - org.apache.spark.sql.functions\n",
    "        - 데이터 타입과 관련된 다양한 함수를 제공함\n",
    "        - 해당 패키지를 많이 쓰이므로 보통 전체 패키지를 임포트하고 사용함\n",
    "- 위의 함수들은 대부분의 SQL과 분석 시스템에서도 찾아볼 수 있는 함수\n",
    "- 모든 함수는 데이터 로우의 특정 포맷이나 구조를 다른 형태로 변환하기 위해 존재함\n",
    "  - 함수를 사용해 더 많은 로우를 만들거나 줄일 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5540dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"sample\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b506070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#분석에 사용할 DataFrame을 생성하는 예제\n",
    "df=spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .load(\"./Spark-The-Definitive-Guide-master/data/retail-data/by-day/2010-12-01.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157c7cc",
   "metadata": {},
   "source": [
    "## 6.2 스파크 데이터 타입으로 변환하기\n",
    "- 프로그래밍 언어의 고유 데이터 타입을 스파크 데이터 타입으로 변환\n",
    "- 스파크 데이터 타입으로 변환하는 방법\n",
    "  - lit 함수를 사용함\n",
    "    - 다른 언어의 데이터 타입을 스파크 데이터 타입에 맞게 변환함\n",
    "  - SQL에서는 스파크 데이터 타입으로 변환할 필요가 없으므로 값을 직접 입력해 사용함\n",
    "    - SELECT 5,\"five\",5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d07d149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[5: int, five: string, 5.0: double]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(lit(5),lit('five'),lit(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2499feb2",
   "metadata": {},
   "source": [
    "## 6.3 불리언 데이터 타입 다루기\n",
    "- 불리언\n",
    "  - 모든 필터링 작업의 기반이므로 데이터 분석에 필수적\n",
    "  - 불리언 구문 : and, or, ture, false\n",
    "  - 불리언 구문을 사용해 true 또는 false로 평가되는 논리 문법을 만듦\n",
    "    - 논리 문법 : 데이터 로우를 필터링할 때 필요조건의 일치와 불일치를 판별하는 데 사용됨\n",
    "  - 불리언 식에는 일치 조건뿐만 아니라 작다, 크다와 같은 비교 연산조건을 사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b299be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNo|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.where(col(\"InvoiceNO\")!=536365)\\\n",
    "    .select(\"InvoiceNo\",\"Description\")\\\n",
    "    .show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00445523",
   "metadata": {},
   "source": [
    "- and 메서드나 or 메서드를 사용해 불리언 표현식을 여러 부분에 지정할 수 있음\n",
    "  - 불리언 표현식을 사용하는 경우 항상 모든 표현식을 and 메서드로 묶어 차례대로 필터를 적용해야 함\n",
    "    - 차례대로 필터를 적용해야 하는 이유\n",
    "         - 불리언 문을 차례대로 표현하더라도 스파크는 내부적으로 and 구문을 필터 사이에 추가해 모든 필터를 하나의 문장으로 변환함\n",
    "         - 그런 다음 동시에 모든 필터를 처리함\n",
    "         - 원한다면 and 구문으로 조건문을 만들 수도 있음\n",
    "         - 하지만 차례로 조건을 나열하면 이해하기 쉽고 읽기도 편해짐\n",
    "- or 구문을 사용할 때는 반드시 동일한 구문에 조건을 정의해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "665cc83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "priceFilter=col(\"UnitPrice\")>600\n",
    "descripFilter=instr(df.Description,\"POSTAGE\")>=1\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter|descripFilter).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795b889f",
   "metadata": {},
   "source": [
    "#### instr 함수란?\n",
    "- sql에서는 문자열 속 문자 위치 찾기에서 사용됨\n",
    "- INSTR 함수\n",
    "  - 문자열에서 지정된 문자열을 검색해서 그 위치를 리턴하는 함수\n",
    "  - 위치는 지정된 문자열이 타나내는 제일 첫 번째 위치를 리턴함\n",
    "    - ex) INSTR('CONGRATULATIONS','AT',1,1) -> 'CONGRATULATIONS' 문자열에서 'AT' 문자열을 첫번째 문자부터 찾아서 'AT' 문자열이 처음 나오는 위치를 리턴해라. 첫번째 'AT'에서 'A'의 위치는 여섯번째이므로 6이 리턴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05467c",
   "metadata": {},
   "source": [
    "- 불리언 표현식을 필터링 조건에만 사용하는 것은 아님\n",
    "  - 불리언 컬럼을 사용해 DataFrame을 필터링할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc0ebdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|unitPrice|isExpensive|\n",
      "+---------+-----------+\n",
      "|   569.77|       true|\n",
      "|   607.49|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "DOTCodeFilter=col(\"StockCode\")==\"DOT\"\n",
    "priceFilter=col(\"UnitPrice\")>600\n",
    "descripFilter=instr(col(\"Description\"),\"POSTAGE\")>=1\n",
    "df.withColumn(\"isExpensive\",DOTCodeFilter & (priceFilter|descripFilter))\\\n",
    "    .where(\"isExpensive\")\\\n",
    "    .select(\"unitPrice\",\"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa154fc7",
   "metadata": {},
   "source": [
    "- 필터를 반드시 표현식으로 정의할 필요는 없음\n",
    "  - 별도의 작업 없이 컬럼명을 사용해 필터를 정의할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ce82562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|   Description|UnitPrice|\n",
      "+--------------+---------+\n",
      "|DOTCOM POSTAGE|   569.77|\n",
      "|DOTCOM POSTAGE|   607.49|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df.withColumn(\"isExpensive\",expr(\"NOT UnitPrice<=250\"))\\\n",
    "    .where(\"isExpensive\")\\\n",
    "    .select(\"Description\",\"UnitPrice\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3886f7e",
   "metadata": {},
   "source": [
    "## 6.4 수치형 데이터 타입 다루기\n",
    "- 카운트 : 빅데이터 처리에서 필터링 다음으로 많이 수행하는 작업\n",
    "  - 수치형 데이터 타입을 사용해 연산 방식을 정의하기만 하면 됨\n",
    "- pow\n",
    "  - 수치형 함수\n",
    "  - 표시된 지수만큼 컬럼의 값을 거듭제곱함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20691c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, pow\n",
    "fabricatedQuantity=pow(col(\"Quantity\")*col(\"UnitPrice\"),2)+5\n",
    "df.select(expr(\"CustomerId\"),fabricatedQuantity.alias(\"realQuantity\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1600aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "\"CustomerId\",\n",
    "\"(POWER((Quantity*UnitPrice),2.0)+5) as realQuantity\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aa9c18",
   "metadata": {},
   "source": [
    "- 반올림도 자주 사용한느 수치형 작업 중 하나\n",
    "  - 때로는 소수점 자리를 없애기 위해 Integer 데이터 타입으로 형변환하기도 함\n",
    "  - 스파크는 정확한 계산이 가능한 함수를 제공함\n",
    "  - 정밀도를 사용해 더 세밀한 작업을 수행할 수 있음\n",
    "- 기본적으로 round 함수는 소수점 값이 정확히 중간값 이상이면 반올림함\n",
    "- 내림은 bround 함수를 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd6253ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit,round,bround\n",
    "df.select(round(lit(\"2.5\")),bround(lit(\"2.5\"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d754e70a",
   "metadata": {},
   "source": [
    "- 두 컬럼 사이의 상관관계를 계산하는 것도 수치형 연산 작업 중 하나\n",
    "  - 스파크에서도 DataFrame의 통계용 함수나 메서드를 사용해 피어슨 상관계수를 계산할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48b6932f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(Quantity, UnitPrice)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835551|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "df.stat.corr(\"Quantity\",\"UnitPrice\")\n",
    "df.select(corr(\"Quantity\",\"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d11c5",
   "metadata": {},
   "source": [
    "- 하나 이상의 컬럼에 대한 요약 통계를 계산하는 작업 역시 자주 수행됨\n",
    "  - describe : 요약 통계\n",
    "    - 관련 컬럼에 대한 집계, 평균, 표준편차, 최솟값, 최댓값을 계산함\n",
    "    - 통계 스키마는 변경될 수 있으므로 해당 메서드는 콘솔 확인용으로만 사용해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b144dd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|        InvoiceDate|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|               3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128|               null| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|               null|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|2010-12-01 08:26:00|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|2010-12-01 17:35:00|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c047f85",
   "metadata": {},
   "source": [
    "- 정확한 수치가 필요하다면 함수를 임포트하고 해당 컬럼에 적용하는 방식으로 직접 집계를 수행할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7c56725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,mean,stddev_pop,min,max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a3efc6",
   "metadata": {},
   "source": [
    "- StatFunctions 패키지\n",
    "  - 다양한 통계 함수를 제공함\n",
    "  - stat 속성을 사용해 접근할 수 있으며 다양한 통계값을 계산할 때 사용하는 DataFrame 메서드\n",
    "  - ex) approxQuantile 메서드를 사용해 데이터의 백분위수를 정확하게 계산하거나 근사치를 계산할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b69c4fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.51]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "olName=\"UnitPrice\"\n",
    "quantileProbs=[0.5]\n",
    "relError=0.05\n",
    "\n",
    "df.stat.approxQuantile(\"UnitPrice\",quantileProbs,relError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bfe478",
   "metadata": {},
   "source": [
    "- StatFunctions 패키지\n",
    "  - 교차표나 자주 사용하는 항목 쌍을 확인하는 용도의 메서드도 제공함\n",
    "  - 단, 연산 결과가 너무 크면 화면에 모두 보이지 않을 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b6dde4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|             22578|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21327|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22064|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21080|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|\n",
      "|             22219|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21908|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22818|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|           15056BL|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             72817|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22545|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22988|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|\n",
      "|             22274|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             20750|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|            82616C|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21703|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22899|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|\n",
      "|             22379|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22422|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22769|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22585|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.stat.crosstab(\"StockCode\",\"Quantity\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77e504bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "| StockCode_freqItems|  Quantity_freqItems|\n",
      "+--------------------+--------------------+\n",
      "|[90214E, 20728, 2...|[200, 128, 23, 32...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#해당 열에서 빈번하게 나오는 요소들을 보여준다.\n",
    "df.stat.freqItems([\"StockCode\",\"Quantity\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb9b7c1",
   "metadata": {},
   "source": [
    "- StatFunctions 패키지의 함수인 monotonically_increasing_id\n",
    "  - 모든 로우에 고유 ID 값을 추가함\n",
    "  - 모든 로우에 0부터 시작하는 고윳값을 생성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "819910a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|monotonically_increasing_id()|\n",
      "+-----------------------------+\n",
      "|                            0|\n",
      "|                            1|\n",
      "+-----------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df.select(monotonically_increasing_id()).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e0568",
   "metadata": {},
   "source": [
    "- 스파크의 새로운 버전이 릴리스될 때마다 새로운 함수가 생겨남\n",
    "- 새로 추가된 함수는 스파크 공식 문서에서 확인할 수 있음\n",
    "  - ex) 불규칙적으로 데이터를 생성할 수 있는 rand() 함수나 radn() 함수 등 임의 데이터 생성 함수\n",
    "    - 해당 함수들은 잠재적으로 결정론과 관련된 문제를 가지고 있음\n",
    "    - 스파크 메일링 리스트에서 이와 관련된 논의를 찾아볼 수 있음\n",
    "  - 최선 버전의 StatFunction 패키지는 블롬 필터링이나 스케칭 알고리즘 같은 여러 고급 기법과 관련된 함수를 제공함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2edc362",
   "metadata": {},
   "source": [
    "## 6.5 문자열 데이터 타입 다루기\n",
    "- 문자열을 다루는 작업은 거의 모든 데이터 처리 과정에서 발생함\n",
    "  - 문자열을 다루는 방법 : 로그 파일에 정규 표현식을 사용해 데이터 추출, 데이터 치환, 문자열 존재 여부, 대/소문자 변환 처리 등의 작업을 할 수 있음\n",
    "\n",
    "- 대/소문자 변환 작업\n",
    "  - initcap 함수 : 주어진 문자열에서 공백으로 나뉘는 모든 단어의 첫 글자를 대문자로 변경함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef5beaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|initcap(Description)|\n",
      "+--------------------+\n",
      "|White Hanging Hea...|\n",
      "| White Metal Lantern|\n",
      "|Cream Cupid Heart...|\n",
      "|Knitted Union Fla...|\n",
      "|Red Woolly Hottie...|\n",
      "|Set 7 Babushka Ne...|\n",
      "|Glass Star Froste...|\n",
      "|Hand Warmer Union...|\n",
      "|Hand Warmer Red P...|\n",
      "|Assorted Colour B...|\n",
      "|Poppy's Playhouse...|\n",
      "|Poppy's Playhouse...|\n",
      "|Feltcraft Princes...|\n",
      "|Ivory Knitted Mug...|\n",
      "|Box Of 6 Assorted...|\n",
      "|Box Of Vintage Ji...|\n",
      "|Box Of Vintage Al...|\n",
      "|Home Building Blo...|\n",
      "|Love Building Blo...|\n",
      "|Recipe Box With M...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "df.select(initcap(col(\"Description\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae88ed90",
   "metadata": {},
   "source": [
    "- lower 함수 : 문자열 전체를 소문자로 변경함\n",
    "- upper 함수 : 문자열 전체를 대문자로 변경함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23e65ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------------+\n",
      "|         Description|  lower(Description)|upper(lower(Description))|\n",
      "+--------------------+--------------------+-------------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|     WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern|      WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower,upper\n",
    "df.select(col(\"Description\"),\n",
    "         lower(col(\"Description\")),\n",
    "         upper(lower(col(\"Description\")))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60722ee3",
   "metadata": {},
   "source": [
    "- 문자열 주변의 공백을 제거하거나 추가하는 작업도 가능함\n",
    "  - lpad,ltrim,rpad,rtrim,trim 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "715656d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+---+----------+\n",
      "|  ltrim|  rtrim| trim| lp|        rp|\n",
      "+-------+-------+-----+---+----------+\n",
      "|HELLO  |  HELOO|HELLO|HEL|HELLO     |\n",
      "|HELLO  |  HELOO|HELLO|HEL|HELLO     |\n",
      "+-------+-------+-----+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit,ltrim,rtrim,rpad,lpad,trim\n",
    "df.select(\n",
    "    ltrim(lit(\"  HELLO  \")).alias(\"ltrim\"),\n",
    "    rtrim(lit(\"  HELOO  \")).alias(\"rtrim\"),\n",
    "    trim(lit(\"  HELLO  \")).alias(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"),3,\" \").alias(\"lp\"),\n",
    "    rpad(lit(\"HELLO\"),10,\" \").alias(\"rp\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1870891",
   "metadata": {},
   "source": [
    "- lpad 함수나 rpad 함수에 문자열의 길이보다 작은 숫자를 넘기면 문자열의 오른쪽부터 제거됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c26be9c",
   "metadata": {},
   "source": [
    "### 6.5.1 정규 표현식\n",
    "- 정규 표현식 : 문자열의 존재 여부를 확인하거나 일치하는 모든 문자열을 치환할 때 사용\n",
    "  - 문자열에서 값을 추출하거나 다른 값으로 치환하는 데 필요한 규칙 모음을 정의할 수 있음\n",
    "  - 스파크는 자바 정규 표현식이 가진 강력한 능력을 활용함\n",
    "    - 자바 정규 표현식 문법은 보통 사용하는 언어의 문법과 약간 다르므로 운영 환경에서 정규 표현식을 사용하기 전에 다시 한 번 검토해야함\n",
    "- 정규 표현식을 위해 regexp_extract 함수, regexp_replace 함수를 제공함\n",
    "  - 이 함수들은 값을 추출하고 치환하는 역할을 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e611976f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         color_clean|         Description|\n",
      "+--------------------+--------------------+\n",
      "|COLOR HANGING HEA...|WHITE HANGING HEA...|\n",
      "| COLOR METAL LANTERN| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#regexp_replace 함수를 사용해 'description' 컬럼의 값을 'COLOR'로 치환\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "regex_string=\"BLACK|WHITE|RED|GREEN|BLUE\"\n",
    "df.select(\n",
    "    regexp_replace(col(\"Description\"),regex_string,\"COLOR\").alias(\"color_clean\"),\n",
    "    col(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f525127",
   "metadata": {},
   "source": [
    "- 주어진 문자를 다른 문자로 치환해야 할 때도 있음\n",
    "  - translate 함수로 문자 치환 가능\n",
    "    - 해당 연산은 문자 단위로 이루어짐\n",
    "    - 교체 문자열에서 색인된 문자에 해당하는 모든 문자를 치환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "447abb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------------+\n",
      "|translate(Description, LEET, 1337)|         Description|\n",
      "+----------------------------------+--------------------+\n",
      "|              WHI73 HANGING H3A...|WHITE HANGING HEA...|\n",
      "|               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|\n",
      "+----------------------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "df.select(translate(col(\"Description\"),\"LEET\",\"1337\"),col(\"Description\"))\\\n",
    "    .show(2)\n",
    "#해당 예제의 경우 L=1, E=3, T=7로 치환됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2233fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|color_clean|         Description|\n",
      "+-----------+--------------------+\n",
      "|      WHITE|WHITE HANGING HEA...|\n",
      "|      WHITE| WHITE METAL LANTERN|\n",
      "+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#처음 나타난 색상 이름을 추출하는 것과 같은 작업을 수행할 수도 있음\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "extract_str=\"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "df.select(\n",
    "    regexp_extract(col(\"Description\"),extract_str,1).alias(\"color_clean\"),\n",
    "    col(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0958330e",
   "metadata": {},
   "source": [
    "- 때로는 값 추출 없이 단순히 값의 존재 여부를 확인하고 싶을 때가 있음\n",
    "  - contains 메서드를 사용함\n",
    "    - 인수로 입력된 값이 컬럼의 문자열에 존재하는지 불리언 타입으로 반환함\n",
    "  - instr 함수\n",
    "    - 파이썬과 SQL에서는 해당 함수를 통해 값의 존재 여부를 확인함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7980bd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "containsBlack=instr(col(\"Description\"),\"BLACK\")>=1\n",
    "containsWhite=instr(col(\"Description\"),\"WHITE\")>=1\n",
    "df.withColumn(\"hasSimpleColor\",containsBlack|containsWhite)\\\n",
    "    .where(\"hasSimpleColor\")\\\n",
    "    .select(\"Description\").show(3,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a41e5f",
   "metadata": {},
   "source": [
    "- 동적으로 인수의 개수가 변하는 상황을 스파크가 처리하는 방법\n",
    "  - varargs: 값 목록을 인수로 변환해 함수에 전달할 때\n",
    "    - 이 기능을 사용해 임의 길이의 배열을 효율적으로 다룰 수 있음\n",
    "    - ex) select 메서드와 varargs를 함께 사용해 원하는 만큼 동적으로 컬럼을 생성할 수 있음\n",
    "  - 파이썬은 인수의 개수가 동적으로 변하는 상황을 아주 쉽게 해결할 수 있음\n",
    "    - locate 함수 : 문자열의 위치(위치는 1부터 시작)를 정수로 반환\n",
    "      - 그런 다음 위치 정보를 불리언 타입으로 변환함\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6dd15c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr,locate\n",
    "simpleColors=[\"black\",\"white\",\"red\",\"green\",\"blue\"]\n",
    "def color_locator(column,color_string):\n",
    "    return locate(color_string.upper(),column)\\\n",
    "            .cast(\"boolean\")\\\n",
    "            .alias(\"is_\"+color_string)\n",
    "selectedColumns=[color_locator(df.Description,c) for c in simpleColors]\n",
    "selectedColumns.append(expr(\"*\")) #column 타입이어야 함\n",
    "\n",
    "df.select(*selectedColumns).where(expr(\"is_white OR is_red\"))\\\n",
    "    .select(\"Description\").show(3,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715276c",
   "metadata": {},
   "source": [
    "- locate 함수는 쉽게 확장할 수 있음\n",
    "  - 컬럼이나 불리언 필터를 프로그래밍 방식으로 생성할 수 있음\n",
    "  - ex) locate 함수를 확장해 입력값의 최소공배수를 구하거나 소수 여부를 판별할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab8fddd",
   "metadata": {},
   "source": [
    "## 6.6 날짜와 타임스탬프 데이터 타입 다루기\n",
    "- 날짜와 시간은 프로그래밍 언어와 데이터베이스 분야의 변함없는 과제\n",
    "  - 계속해서 시간대를 확인해야 하며, 포맷이 올바르고 유효한지 확인해야 함\n",
    "- 스파크는 이런 복잡함을 피하고자 두 가지 종류의 시간 관련 정보만 집중적으로 관리함\n",
    "  - 하나, 달력 형태의 날짜\n",
    "  - 다른 하나, 날짜와 시간을 모두 가지는 타임스탬프\n",
    "  - inferSchema 옵션이 활성화된 경우, 날짜와 타임스탬프를 포함해 컬럼의 데이터 타입을 최대한 정확하게 식별하려 시도함\n",
    "    - 특정 날짜 포맷을 명시하지 않아도 자체적으로 식별해 데이터를 읽을 수 있음\n",
    "- 날짜와 타임스탬프를 다루는 작업\n",
    "  - 문자열을 다루는 작업과 관련이 있음\n",
    "  - 날짜나 시간을 문자열로 저장하고 런타임에 날짜 타입으로 변환하는 경우가 많기 때문\n",
    "  - 데이터베이스나 구조적 데이터를 다룰 때는 이러한 작업이 드물지만 텍스트나 CSV 파일을 다룰 때는 많이 발생함\n",
    "  \n",
    "  \n",
    "- 스파크는 날짜와 시간을 최대한 올바른 형태로 읽기 위해 노력함\n",
    "  - 만약 특이한 포맷의 날짜와 시간 데이터를 어쩔 수 없이 다뤄야 한다면 각 단계별로 어떤 데이터 타입과 포맷을 유지하는지 정확히 알고 트랜스포메이션을 적용해야 함\n",
    "  - TimestampType 클래스 : 초 단위 정밀도까지만 지원함\n",
    "  - Long 데이터 타입 :밀리세컨드나 마이크로 세컨드 단위를 다룰 때 해당 데이터 타입으로 변환해 처리하는 우회 정책을 사용해야 함\n",
    "\n",
    "\n",
    "- 스파크는 특정 기점에 데이터 포맷이 약간 특이하게 변할 수 있음\n",
    "  - 이러한 문제를 피하려면 파싱이나 변환 작업을 해야 함\n",
    "  - 자바의 날짜와 타임스탬프를 사용해서 표준 체계를 따름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "654716f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#오늘 날짜와 현재 타임스탬프값을 구하는 예제\n",
    "from pyspark.sql.functions import current_date,current_timestamp\n",
    "dateDF=spark.range(10)\\\n",
    "    .withColumn(\"today\",current_date())\\\n",
    "    .withColumn(\"now\",current_timestamp())\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc414c",
   "metadata": {},
   "source": [
    "- 위 예제로 만들어진 DataFrame을 사용해 오늘을 기준으로 5일 전후의 날짜를 구해보겠음\n",
    "  - date_sum 함수와 date_add 함수는 컬럼과 더하거나 뺄 날짜 수를 인수로 전달해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a74b90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2021-11-14|        2021-11-24|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add,date_sub\n",
    "dateDF.select(date_sub(col(\"today\"),5),date_add(col(\"today\"),5)).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba146bf5",
   "metadata": {},
   "source": [
    "- 두 날짜의 차이를 구하는 작업도 자주 발생함\n",
    "  - datediff 함수 : 두 날짜 사이의 일 수를 반환\n",
    "  - months_between 함수 : 두 날짜 사이의 개월 수를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca0bc0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_age, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -16.67741935|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff,months_between,to_date\n",
    "\n",
    "dateDF.withColumn(\"week_age\",date_sub(col(\"today\"),7))\\\n",
    "    .select(datediff(col(\"week_age\"),col(\"today\"))).show(1)\n",
    "dateDF.select(\n",
    "    to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "    to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n",
    "    .select(months_between(col(\"start\"),col(\"end\"))).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4113acb",
   "metadata": {},
   "source": [
    "- to_date 함수\n",
    "  - 문자열을 날짜로 변환할 수 있으며 필요에 따라 날짜 포맷도 함께 저장할 수 있음\n",
    "  - SimpleDataFormat 클래스 : 함수의 날짜 포맷은 반드시 자바가 지원하는 포맷을 사용해야 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f3dd459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|to_date(date)|\n",
      "+-------------+\n",
      "|   2017-01-01|\n",
      "+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date,lit\n",
    "spark.range(5).withColumn(\"date\",lit(\"2017-01-01\"))\\\n",
    "    .select(to_date(col(\"date\"))).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb15681b",
   "metadata": {},
   "source": [
    "- 스파크는 날짜를 파싱할 수 없다면 에러 대신 null 값을 반환함\n",
    "  - 다단계 처리 파이프라인에서는 조금 까다로울 수 있음\n",
    "  - 데이터 포맷이 지정된 데이터에서 또 다른 포맷의 데이터가 나타날 수 있음\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb86b9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|to_date(2016-20-12)|to_date(2017-12-11)|\n",
      "+-------------------+-------------------+\n",
      "|               null|         2017-12-11|\n",
      "+-------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#이해를 돕기 위해 년-월-일 형태가 아닌 년-일-월 형태의 날짜 포맷을 사용하면\n",
    "#스파크는 날짜를 파싱할 수 없어 null 값을 반환함\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9fdcbc",
   "metadata": {},
   "source": [
    "- 지정한 날짜 형식에 맞춰 데이터가 들어온다면 특별한 문제가 발생하지 않음\n",
    "  - 날짜 형식을 지키지 않ㅇ느 데이터가 들어온다면 디버깅하기 매우 어려움\n",
    "  - ex)날짜(2017-12-11)가 의도한 날짜인 11월 12일 대신 12월 11일로 표시되고 있음\n",
    "    - 스파크는 날짜가 뒤섞여 있거나 데이터가 잘못되었는지 판단할 수 없으므로 오류를 발생시키지 않음\n",
    "- 위의 문제를 완전히 회피할 수 있는 견고한 방법\n",
    "  - 첫 번째 단계 : 자바의 SimpleDateFormat 표준에 맞춰 날짜 포맷을 지정\n",
    "    - to_date 함수 : 필요에 따라 날짜 포맷을 지정\n",
    "    - to_timestamp 함수 : 반드시 날짜 포맷을 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29ef3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "dateFormat='yyyy-dd-MM'\n",
    "cleanDateDF=spark.range(1).select(\n",
    "    to_date(lit(\"2017-12-11\"),dateFormat).alias(\"date\"),\n",
    "    to_date(lit(\"2017-20-12\"),dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc1f97f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|to_timestamp(date, yyyy-dd-MM)|\n",
      "+------------------------------+\n",
      "|           2017-11-12 00:00:00|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to_timestamp항상 날짜 포맷을 지정해줘야 함\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "cleanDateDF.select(to_timestamp(col(\"date\"),dateFormat)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaa2128",
   "metadata": {},
   "source": [
    "- 올바른 포맷과 타입의 날짜나 타임스탬프를 사용한다면 매우 쉽게 비교할 수 있음\n",
    "  - 날짜를 비교할 때는 날짜나 타임스탬프 타입을 사용하거나 yyyy-MM-dd 포맷에 맞는 문자열을 지정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d8cb4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.filter(col(\"date2\")>lit(\"2017-12-12\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f1382",
   "metadata": {},
   "source": [
    "- 스파크가 리터럴로 인식하는 문자열을 지정해 날짜를 비교할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef94c159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.filter(col(\"date2\")>\"2017-12-12\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f20141",
   "metadata": {},
   "source": [
    "## 6.7 null 값 다루기\n",
    "- DataFrame에서 빠져 있거나 비어 있는 데이터를 표현할 때는 항상 null 값을 사용하는 것이 좋음\n",
    "  - 스파크에서는 빈 문자열이나 대체 값 대신 null 값을 사용해야 최적화를 수행할 수 있음\n",
    "- DataFrame의 하위 패키지인 .na를 사용하는 것이 DataFrame에서 null 값을 다루는 기본 방식\n",
    "  - 연산을 수행하면서 스파크가 null 값을 제어하는 방법을 명시적으로 지정하는 몇 가지 함수도 있음\n",
    "  \n",
    "- null 값을 다루는 두 가지 방법\n",
    "  - 명시적으로 null 값 제거\n",
    "  - 전역 또는 컬럼 단위로 null 값을 특정 값으로 채워 넣기\n",
    "  \n",
    "### 6.7.1 coalesce\n",
    "- coalesce 함수 : 인수로 지정한 여러 컬럼 중 null이 아닌 첫 번째 값을 반환\n",
    "  - 모든 컬럼이 null이 아닌 값을 가지는 경우 **첫 번째 컬럼의 값을 반환함**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4382a718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|coalesce(Description, CustomerId)|\n",
      "+---------------------------------+\n",
      "|             WHITE HANGING HEA...|\n",
      "|              WHITE METAL LANTERN|\n",
      "|             CREAM CUPID HEART...|\n",
      "|             KNITTED UNION FLA...|\n",
      "|             RED WOOLLY HOTTIE...|\n",
      "|             SET 7 BABUSHKA NE...|\n",
      "|             GLASS STAR FROSTE...|\n",
      "|             HAND WARMER UNION...|\n",
      "|             HAND WARMER RED P...|\n",
      "|             ASSORTED COLOUR B...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             FELTCRAFT PRINCES...|\n",
      "|             IVORY KNITTED MUG...|\n",
      "|             BOX OF 6 ASSORTED...|\n",
      "|             BOX OF VINTAGE JI...|\n",
      "|             BOX OF VINTAGE AL...|\n",
      "|             HOME BUILDING BLO...|\n",
      "|             LOVE BUILDING BLO...|\n",
      "|             RECIPE BOX WITH M...|\n",
      "+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "df.select(coalesce(col(\"Description\"),col(\"CustomerId\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca433ff",
   "metadata": {},
   "source": [
    "### 6.7.2 ifnull. nullif,nvl,nvl2\n",
    "- coalesce 함수와 유사한 결과를 얻을 수 있는 몇 가지 SQL 함수\n",
    "  - ifnull 함수\n",
    "    - 첫 번째 값이 null이면 두 번째 값을 반환함\n",
    "    - 첫 번째 값이 null이 아니면 첫 번째 값을 반환함\n",
    "  - nullif 함수\n",
    "    - 두 값이 같으면 null을 반환함\n",
    "    - 두 값이 다르면 첫 번째 값을 반환함\n",
    "  - nvl 함수\n",
    "    - 첫 번째 값이 null이면 두 번째 값을 반환함\n",
    "    - 첫 번째 값이 null이 아니면 첫 번째 값을 반환함\n",
    "  - nvl2 함수\n",
    "    - 첫 번째 값이 null이면 세 번째 인수로 지정된 값을 반환함\n",
    "    - 첫 번째 값이 null이 아니면 두 번째 값을 반환함\n",
    "- 위의 함수들은 DataFrame의 select 표현식으로 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3432046",
   "metadata": {},
   "source": [
    "### 6.7.3 drop\n",
    "- drop 메서드\n",
    "  - null 값을 가진 로우를 제거하는 가장 간단한 함수\n",
    "  - 기본적으로 null 값을 가진 모든 로우 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b1d5ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop()\n",
    "df.na.drop(\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3491ebf",
   "metadata": {},
   "source": [
    "- drop 메서드의 인수\n",
    "  - any\n",
    "    - 지정 시 로우의 컬럼값 중 하나라도 null 값을 가지면 해당 로우 제거\n",
    "  - all\n",
    "    - 모든 컬럼의 값이 null이거나 NaN인 경우에만 해당 로우 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fa29add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a891f",
   "metadata": {},
   "source": [
    "- drop 메서드에 배열 형태의 컬럼을 인수로 전달해 적용할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1823535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop(\"all\",subset=['StockCode','InvoiceNo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0276028e",
   "metadata": {},
   "source": [
    "### 6.7.4 fill\n",
    "- fill 함수\n",
    "  - 하나 이상의 컬럼을 특정 값으로 채울 수 있음\n",
    "  - 채워 넣을 값과 컬럼 집합으로 구성된 맵을 인수로 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7cb211c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#string 데이터 타입의 컬럼에 존재하는 null 값을 다른 값으로 채워 넣는 방법\n",
    "df.na.fill(\"All Null values become this string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cad4471",
   "metadata": {},
   "source": [
    "- df.na.fill(5:Integer) 같은 방식을 사용해 Integer 데이터 타입의 컬럼에 존재하는 null 값을 다른 값으로 채워 넣을 수 있음\n",
    "  - Double 데이터 타입의 컬럼에는 df.na.fill(5:double) 같은 방식을 사용할 수 있음\n",
    "  - 다수의 컬럼에 적용하고 싶다면 적용하고자 하는 컬럼명을 배열로 만들어 인수로 사용함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8248c7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.fill(\"all\",subset=['StockCode','InvoiceNo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c7bc4",
   "metadata": {},
   "source": [
    "- 파이썬의 경우, 딕셔너리 형태의 값을 하나 만들어서 넣으면 다수의 컬럼에 fill 메서드를 적용할 수 있음\n",
    "  - 키 : 컬럼명, 값: null 값을 채우는 데 사용할 값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "deb9d157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_cols_vals={\"StockCode\":5,\"Description\":\"No Value\"}\n",
    "df.na.fill(fill_cols_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3a3fb",
   "metadata": {},
   "source": [
    "### 6.7.5 replace\n",
    "- drop 메서드와 fill 메서드 외에도 null 값을 유연하게 대처할 방법이 있음\n",
    "  - 조건에 따라 다른 값으로 대체하는 것\n",
    "- replace 메서드\n",
    "  - 해당 메서드 사용 시 변경하고자 하는 값과 원래 값의 데이터 타입이 같아야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37e4019f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.replace([\"\"],[\"UNKNOWN\"],\"Description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12aac4",
   "metadata": {},
   "source": [
    "## 6.8 정렬하기\n",
    "- 5장에서 설명한 것처럼???\n",
    "  - asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last 함수를 사용해 DataFrame을 정렬할 때 null 값이 표시되는 기준을 지정할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0891922a",
   "metadata": {},
   "source": [
    "## 6.9 복합 데이터 타입 다루기\n",
    "- 복합 데이터 타입을 사용하면 해결하려는 문제에 더욱 적합한 방식으로 데이터를 구성하고 구조화할 수 있음\n",
    "  - 복합 데이터 타입 종류: 구조체, 배열, 맵\n",
    "  \n",
    "### 6.9.1 구조체\n",
    "- 구조체 : DataFrame 내부의 DataFrame으로 생각할 수 있음\n",
    "  - 쿼리문에서 다수의 컬럼을 괄호로 묶어 구조체를 만들 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee57cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#복합 데이터 타입을 가진 DataFrame 만들기\n",
    "from pyspark.sql.functions import struct\n",
    "complexDF=df.select(struct(\"Description\",\"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72944ce8",
   "metadata": {},
   "source": [
    "- 다른 DataFrame을 조회하는 것과 동일하게 사용할 수 있음\n",
    "- 유일한 차이점은 문법에 점(.)을 사용하거나 getField 메서드를 사용한다는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3073914b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[complex.Description: string]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complexDF.select(\"complex.Description\")\n",
    "complexDF.select(col(\"complex\").getField(\"Description\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe22826",
   "metadata": {},
   "source": [
    "- \\*문자를 사용해 모든 값을 조회할 수 있으며 모든 컬럼을 DataFrame의 최상위 수준으로 끌어올릴 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9ec145c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Description: string, InvoiceNo: string]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complexDF.select(\"complex.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5df42",
   "metadata": {},
   "source": [
    "### 6.9.2 배열\n",
    "- 한 가지 사례 : 데이터에서 Description 컬럼의 모든 단어를 하나의 로우로 변환\n",
    "  - Description 컬럼을 복합 데이터 타입인 배열로 변환\n",
    "\n",
    "#### split\n",
    "- split : 배열로 변환하는데 사용하는 함수\n",
    "  - split 함수에 구분자를 인수로 전달해 배열로 변환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eee23181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|split(Description,  , -1)|\n",
      "+-------------------------+\n",
      "|     [WHITE, HANGING, ...|\n",
      "|     [WHITE, METAL, LA...|\n",
      "+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "df.select(split(col(\"Description\"),\" \")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d8b42a",
   "metadata": {},
   "source": [
    "- split 함수 : 스파크에서 복합 데이터 타입을 마치 또 다른 컬럼처럼 다룰 수 있는 매우 강력한 기능\n",
    "  - 파이썬과 유사한 문법을 사용해 배열값을 조회할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9980653d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(col(\"Description\"),\" \").alias(\"array_col\"))\\\n",
    "    .selectExpr(\"array_col[0]\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa727b26",
   "metadata": {},
   "source": [
    "#### 배열의 길이\n",
    "- 배열의 크기를 조회해 배열의 길이를 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df684dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|size(split(Description,  , -1))|\n",
      "+-------------------------------+\n",
      "|                              5|\n",
      "|                              3|\n",
      "+-------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size\n",
    "df.select(size(split(col(\"Description\"),\" \"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9769b0",
   "metadata": {},
   "source": [
    "#### array_contains\n",
    "- array_contains 함수 : 배열에 특정 값이 존재하는지 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3d1dce2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|array_contains(split(Description,  , -1), WHITE)|\n",
      "+------------------------------------------------+\n",
      "|                                            true|\n",
      "|                                            true|\n",
      "+------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "df.select(array_contains(split(col(\"Description\"),\" \"),\"WHITE\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f51ab3",
   "metadata": {},
   "source": [
    "#### explode\n",
    "- 복합 데이터 타입의 배열에 존재하는 모든 값을 로우로 변환\n",
    "- 배열 타입의 컬럼을 입력받음\n",
    "  - 입력된 컬럼의 배열값에 포함된 모든 값을 로우로 변환함\n",
    "  - 나머지 컬럼값은 중복되어 표시됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6397be23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+\n",
      "|         Description|InvoiceNo|exploded|\n",
      "+--------------------+---------+--------+\n",
      "|WHITE HANGING HEA...|   536365|   WHITE|\n",
      "|WHITE HANGING HEA...|   536365| HANGING|\n",
      "+--------------------+---------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split,explode\n",
    "df.withColumn(\"splitted\",split(col(\"Description\"),\" \"))\\\n",
    "    .withColumn(\"exploded\",explode(col(\"splitted\")))\\\n",
    "    .select(\"Description\",\"InvoiceNo\",\"exploded\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff245b",
   "metadata": {},
   "source": [
    "### 6.9.3 맵\n",
    "- 맵 : map 함수와 컬럼의 키-값 쌍을 이용해 생성함\n",
    "  - 배열과 동일한 방법으로 값을 선택할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "522c6825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         complex_map|\n",
      "+--------------------+\n",
      "|{WHITE HANGING HE...|\n",
      "|{WHITE METAL LANT...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "df.select(create_map(col(\"Description\"),col(\"InvoiceNo\")).alias(\"complex_map\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f39c7f5",
   "metadata": {},
   "source": [
    "- 적합한 키를 사용해 데이터를 조회할 수 있으며 해당 키가 없다면 null 값을 반환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce533f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|                            null|\n",
      "|                          536365|\n",
      "+--------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(create_map(col(\"Description\"),col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    "    .selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c026c",
   "metadata": {},
   "source": [
    "- map 타입은 분해하여 컬럼으로 변환할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c3ac905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                 key| value|\n",
      "+--------------------+------+\n",
      "|WHITE HANGING HEA...|536365|\n",
      "| WHITE METAL LANTERN|536365|\n",
      "+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(create_map(col(\"Description\"),col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    "    .selectExpr(\"explode(complex_map)\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33063753",
   "metadata": {},
   "source": [
    "## 6.10 JSON 다루기\n",
    "- 스파크는 JSON 데이터를 다루기 위한 몇 가지 고유 기능을 지원함\n",
    "  - 문자열 형태의 JSON을 직접 조작할 수 있으며 JSON을 파싱하거나 JSON 객체로 만들 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d2b325ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSON 컬럼을 생성하는 예제\n",
    "jsonDF=spark.range(1).selectExpr(\"\"\"\n",
    "'{\"myJSONKey\":{\"myJSONValue\":[1,2,3]}}' as JsonString\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be29de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
