{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "출처 : https://wikidocs.net/book/9379"
      ],
      "metadata": {
        "id": "QdS7ZP4BlhgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#파이토치 기초 코드\n",
        "## 1. torch\n",
        "- 메인 네임스페이스\n",
        "- 텐서 등의 다양한 수학 함수가 포함되어 있음\n",
        "- Numpy와 유사함\n",
        "- 코드 : import torch\n",
        "\n",
        "## 2. torch.autograd\n",
        "- 자동 미분을 위함 함수들이 포함되어져 있음\n",
        "\n",
        "## 3. torch.nn\n",
        "- 신경망을 구축하기 위한 다양한 데이터 구조나 레이어 등이 정의되어져 있음\n",
        "- EX) RNN,LSTM과 같은 레이어, ReLU와 같은 활성화 함수, MSELoss와 같은 손실 함수들이 있음\n",
        "- 코드 : import torch.nn as nn\n",
        "\n",
        "## 4. torch.optim\n",
        "- 확률적 경사 하강법을 중심으로 한 파라미터 최적화 알고리즘이 구현되어져 있음\n"
      ],
      "metadata": {
        "id": "DNlZkR81wqh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. tensor\n",
        "- tensor(텐서) : 다차원 배열을 나타내는 자료형\n",
        "- Numpy의 ndarray와 유사한 API를 제공하며 GPU를 이용한 연산도 지원함\n",
        "- 텐서를 사용하여 머신러닝 모델에서 데이터를 표현하고 연산을 수행함\n",
        "\n",
        "- tensor 메서드\n",
        "  - 파이썬 리스트나 넘파이 배열을 Pytorch 텐서로 변환해주는 역할을 함\n",
        "  - 메서드\n",
        "    - 코드 : torch.tensor(data,dtype=None,device=None,requiers_grad=False)\n",
        "    - data : 텐서로 변환하고자 하는 데이터\n",
        "      - 파이썬 리스트나 넘파이 배열 등이 사용될 수 있음\n",
        "    - dtype(optional) : 텐서의 데이터 타입을 지정함\n",
        "      - 기본값 : None\n",
        "      - 자동으로 데이터 타입을 인식함\n",
        "    - device(optional) : 텐서가 사용될 디바이스를 지정함\n",
        "      - 기본값 : None\n",
        "      - CPU 상에서 계산됨\n",
        "    - requires_grad(optional) : 해당 텐서가 그래디언트를 계싼할지 여부를 지정함\n",
        "      - 기본값 : False"
      ],
      "metadata": {
        "id": "b3Ti51hyTr-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "data = [[1, 2, 3], [4, 5, 6]]\n",
        "x = torch.tensor(data, dtype=torch.float32)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "AjgyRTqUUxjj",
        "outputId": "866f79d6-ca70-4c64-e582-9b572171aa13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 생성\n",
        "- torch.tensor() 함수\n",
        "  - 파이썬 리스트나 넘파이 배열을 입력받아 텐서를 생성함\n",
        "- torch.zeros(), torch.ones(), torch.rand() 함수를 사용하여 모든 원스가 0,1 또는 랜덤한 값으로 채워진 텐서를 생성할 수 있음\n",
        "  - 인자로 텐서의 크기를 입력함\n",
        "\n",
        "- 텐서의 크기를 입력해 텐서를 얻는 여러 방법\n",
        "  1. .rand 함수\n",
        "    - 균등 분포(uniform distribution)에서 난수를 생성함\n",
        "    - 최솟값과 최댓값 사이의 값들이 동일한 확률로 선택되는 분포\n",
        "    - 0과 1 사이의 값이 동일한 확률로 선택되는 무작위 값을 생성\n",
        "  2. .randn 함수\n",
        "    - 정규 분포(normal distribution)에서 난수를 생성\n",
        "    - 대표적인 연속 확률 분포로 평균과 분산을 가지는 범위에서 난수를 생성함\n",
        "    - 평균 0, 분산 1인 정규 분포에서 무작위 값을 생성함\n",
        "- .Tensor나 .empty 함수\n",
        "  - 메모리 상에 할당된 임의의 값으로 초기화된 텐서를 생성함\n",
        "  - .tensor와는 다르게 대문자로 시작하는 .Tensor는 기본적으로 FloatTensor와 동일함\n",
        "    - 이외에도 정수형 텐서를 만드는 IntTensor, Bool형 텐서를 만드는 BoolTensor가 있음\n",
        "  - 입력된 데이터를 실수형 텐서로 만들거나 empty나 rand, randn처럼 텐서의 크기를 입력해 새로운 텐서를 만들 수도 있음\n",
        "  "
      ],
      "metadata": {
        "id": "xyB9XnRGTr7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 1차원 텐서 생성\n",
        "x = torch.tensor([1, 2, 3])\n",
        "print(x)\n",
        "\n",
        "# 2차원 텐서 생성\n",
        "y = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(y)\n",
        "\n",
        "# 모든 원소가 0인 3차원 텐서 생성\n",
        "z = torch.zeros((2, 3, 4))\n",
        "print(z)\n",
        "\n",
        "# 모든 원소가 1인 4차원 텐서 생성\n",
        "w = torch.ones((2, 2, 2, 2))\n",
        "print(w)\n",
        "\n",
        "# 랜덤한 값으로 채워진 3x3 텐서 생성\n",
        "r = torch.rand((3, 3))\n",
        "print(r)\n",
        "\n",
        "# 균등 분포에서 난수 생성\n",
        "x = torch.rand((2, 3))\n",
        "print(x)\n",
        "\n",
        "# 정규 분포에서 난수 생성\n",
        "y = torch.randn((2, 3))\n",
        "\n",
        "# .Tensor 예제1\n",
        "a = torch.Tensor([1, 2, 3])\n",
        "print(a)\n",
        "\n",
        "# .empty 예제\n",
        "b = torch.empty(2, 3)\n",
        "print(b)\n",
        "\n",
        "# .Tensor 예제2\n",
        "c = torch.Tensor(2,3)\n",
        "print(c)"
      ],
      "metadata": {
        "id": "HsOUQdSDgR23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1) .from_numpy()와 .tensor()의 차이\n",
        "- 파이토치에서 NumPy 배열을 사용하는 방법\n",
        "  - torch.from_numpy()\n",
        "  - torch.tensor()\n",
        "- torch.from_numpy()\n",
        "  - NumPy 배열을 Pytorch Tensor로 변환함\n",
        "  - 기존 Numpy배열과 데이터를 공유하며 반환된 Tensor를 수정하면 Numpy 배열도 수정됨\n",
        "  - Numpy 배열과 Tensor가 메모리 상에서 동일한 위치를 참조함\n",
        "  - 사용하기 좋은 상황\n",
        "    - 데이터 크기가 크고 계산 비용이 많이 드는 경우\n",
        "- torch.tensor()\n",
        "  - Numpy 배열과 유사한 새로운 Pytorch tensor를 생성함\n",
        "  - 새로운 Tensor와 Numpy 배열은 데이터를 공유하지 않으며 Tensor를 수정하더라도 Numpy 배열은 변경되지 않음\n",
        "  - 사용하기 좋은 상황\n",
        "    - 이미지 처리와 같은 경우에는 데이터 크기가 크지만 데이터를 공유할 필요가 없음\n",
        "\n",
        "- PyTorch Tensor를 NumPy 배열로 변환할 때도 .numpy()로 변환하면 데이터를 공유하고 .array()로 변환할 때는 데이터를 공유하지 않음\n",
        "\n",
        "\n",
        "### 1-2) torch.randn\n",
        "- 파이토치에서 정규 분포를 따르는 난수를 생성하는 함수\n",
        "- 정규 분포에서 무작위로 추출한 난수를 생성하여 텐서를 반환함\n",
        "- 함수의 사용법\n",
        "  - torch.randn(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
        "  - *size : 생성하려는 텐서의 크기를 나타내는 가변 인수로 텐서의 차원 크기를 지정함. 예를 들어 torch.randn(3,4) : 3x4 크기의 2차원 텐서를 생성함\n",
        "  - out(optional) : 결과를 저장할 출력 텐서\n",
        "    - 기본값 : None\n",
        "    - out 인수를 지정하면 해당 텐서에 결과가 저장됨\n",
        "  - dtype(optional) : 생성되는 텐서의 데이터 타입을 지정하는 인수\n",
        "    - 기본값 : None\n",
        "    - 자동으로 추론됨\n",
        "  - layout(optional) : 생성되는 텐서의 레이아웃을 지정하는 인수\n",
        "    - 기본값 : torch.strided\n",
        "  - device(optional) : 생성되는 텐서의 디바이스를 지정하는 인수\n",
        "    - 기본값 : None\n",
        "    - 현재 사용 중인 디바이스로 자동으로 설정됨\n",
        "  - requires_grad(optional) : 생성되는 텐서의 기울기를 계산할 지 여부를 지정하는 인수\n",
        "    - 기본값 : False\n",
        "  \n",
        "- torch.randn 함수\n",
        "  - 정규 분포에서 무작위로 추출한 난수를 가지고 지정된 크기의 텐서를 생성함\n",
        "  - ex) torch.randn(3,4) : 3x4 크기의 2차원 텐서를 생성하며 텐서의 각 요소는 평균이 0이고 표준 편차가 1인 정규 분포에서 무작위로 추출한 난수로 초기화됨"
      ],
      "metadata": {
        "id": "e9ets8eGTr1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 2x3 크기의 텐서 생성\n",
        "tensor = torch.randn(2, 3)\n",
        "\n",
        "print(tensor)\n",
        "tensor([[-0.9127, -1.1321,  0.0607],\n",
        "        [ 0.7705, -0.8189, -0.6153]])\n",
        "\n",
        "# 평균을 2, 표준편차를 1.5로 바꾸고 싶은 경우\n",
        "mean = 2.0\n",
        "std = 1.5\n",
        "size = (3, 4)  # 원하는 사이즈 지정\n",
        "\n",
        "# 평균이 2이고 표준 편차가 1.5인 정규 분포에서 무작위로 샘플 생성\n",
        "samples = torch.randn(size) * std + mean\n",
        "\n",
        "print(samples)\n"
      ],
      "metadata": {
        "id": "DuJaVuMiojRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) 속성\n",
        "- 텐서의 4가지 속성\n",
        "  1. 데이터 타입(dtype)\n",
        "  2. 디바이스(device)\n",
        "  3. 크기(size)\n",
        "  4. 모양(shape)\n",
        "1. 데이터 타입 : 텐서 안에 있는 데이터의 자료형을 나타냄\n",
        "  - ex) torch.FloatTensor : 32비트 부동소수점을 저장함\n",
        "  - ex) torch.LongTensor : 64비트 정수를 저장함\n",
        "2. 디바이스 : 텐서가 저장되는 장치를 나타냄\n",
        "  - CPU나 GPU를 사용할 수 있음\n",
        "  - torch.device를 사용하여 GPU를 지정할 수 있음\n",
        "\n",
        "3. 크기 : 텐서 안에 있는 데이터의 개수를 나타냄\n",
        "  - ex) torch.Size([3,4])\n",
        "  - numel() 메소드를 사용하여 크기를 출력\n",
        "\n",
        "4. 모양 : 텐서의 차원을 나타냄\n",
        "  - 2차원 텐서는 행렬\n",
        "  - 3차원 텐서는 직육면체\n",
        "  - size() 속성을 사용하여 모양을 출력\n",
        "  - view()를 사용하여 모양을 변경\n",
        "\n",
        "- Numpy 배열의 경우\n",
        "  - size 함수 : 총 원소 개수를 반환하는 함수\n",
        "    - 1차원 배열이면 배열의 길이(length)\n",
        "    - 2차원 배열이면 행과 열의 곱이 반환\n",
        "- Pytorch의 경우\n",
        "  - size 함수 : 텐서의 크기를 반환하는 함수\n",
        "    - 반환되는 크기 정보를 튜플 형태로 제공함\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "PN3nLuEuosg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 데이터 타입\n",
        "x = torch.tensor([1, 2, 3])\n",
        "print(x.dtype)  # torch.int64\n",
        "\n",
        "# 디바이스\n",
        "device = torch.device('cpu')\n",
        "x = torch.tensor([1, 2, 3], device=device)\n",
        "print(x.device)  # cpu\n",
        "\n",
        "# 크기\n",
        "x = torch.tensor([1, 2, 3])\n",
        "print(x.numel())  # 3\n",
        "\n",
        "# 모양\n",
        "x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
        "print(x.shape, x.size())  # torch.Size([3, 2]) torch.Size([3, 2])\n",
        "\n",
        "# 모양 변경\n",
        "x = torch.Tensor([1, 2, 3, 4])\n",
        "y = x.view(2, 2) #x.reshape(2,2)\n",
        "print(y.size()) #torch.Size([2, 2])"
      ],
      "metadata": {
        "id": "w4hEEF2ZojLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) 연산\n",
        "- 텐서의 차원을 변경하는 연산 : unsqueeze(), squeeze(), reshape()\n",
        "- 텐서의 값을 정렬하는 연산 : sort(), argsort()"
      ],
      "metadata": {
        "id": "ZuKUAmA4qmGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 텐서 생성\n",
        "a = torch.tensor([[1, 2], [3, 4]])\n",
        "b = torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "# 덧셈\n",
        "c = a + b\n",
        "print(c)\n",
        "\n",
        "# 뺄셈\n",
        "d = a - b\n",
        "print(d)\n",
        "\n",
        "# 곱셈\n",
        "e = a * b\n",
        "print(e)\n",
        "\n",
        "# 나눗셈\n",
        "f = a / b\n",
        "print(f)\n",
        "\n",
        "# 행렬곱\n",
        "g = torch.mm(a, b)\n",
        "print(g)\n",
        "\n",
        "# 점곱\n",
        "h = torch.dot(torch.tensor([1, 2]), torch.tensor([3, 4]))\n",
        "print(h)\n",
        "tensor([[ 6.,  8.],\n",
        "        [10., 12.]])\n",
        "tensor([[-4., -4.],\n",
        "        [-4., -4.]])\n",
        "tensor([[ 5., 12.],\n",
        "        [21., 32.]])\n",
        "tensor([[0.2000, 0.3333],\n",
        "        [0.4286, 0.5000]])\n",
        "tensor([[19., 22.],\n",
        "        [43., 50.]])\n",
        "tensor(11.)"
      ],
      "metadata": {
        "id": "uFVQsXPKsBUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 요소별 최대값 계산\n",
        "i = torch.tensor([[1, 2], [3, 4]])\n",
        "j = torch.tensor([[4, 3], [2, 1]])\n",
        "k = torch.max(i, j)\n",
        "print(k)\n",
        "\n",
        "# 차원 축소\n",
        "l = torch.tensor([[1, 2], [3, 4]])\n",
        "m = torch.sum(l, dim=0)\n",
        "print(m)\n",
        "\n",
        "# 전치\n",
        "n = torch.tensor([[1, 2], [3, 4]])\n",
        "o = torch.transpose(n, 0, 1)\n",
        "print(o)"
      ],
      "metadata": {
        "id": "2ECchdpKsBSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인덱싱\n",
        "p = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
        "q = p[1, 0]\n",
        "print(q)\n",
        "\n",
        "# 슬라이싱\n",
        "r = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "s = r[:, :2]\n",
        "print(s)\n",
        "\n",
        "# 조건부 인덱싱\n",
        "t = torch.tensor([1, 2, 3, 4, 5])\n",
        "u = t[t > 3]\n",
        "print(u)"
      ],
      "metadata": {
        "id": "bOiMcsrDsBQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 텐서 생성\n",
        "x = torch.tensor([1, 2, 3, 4])\n",
        "\n",
        "# 텐서 차원 변경\n",
        "y = x.unsqueeze(0)\n",
        "print(y)\n",
        "\n",
        "y = x.unsqueeze(1)\n",
        "print(y)\n",
        "\n",
        "y = x.squeeze()\n",
        "print(y)\n",
        "\n",
        "# 텐서 reshape\n",
        "y = x.reshape(2, 2)\n",
        "print(y)\n",
        "\n",
        "# 텐서 정렬\n",
        "x = torch.tensor([3, 2, 1])\n",
        "y = torch.tensor([4, 5, 6])\n",
        "z = torch.tensor([1, 2, 3])\n",
        "a, b = torch.sort(x)\n",
        "print(a, b)\n",
        "\n",
        "a, b = torch.sort(y)\n",
        "print(a, b)\n",
        "\n",
        "a, b = torch.sort(z, descending=True)\n",
        "print(a, b)"
      ],
      "metadata": {
        "id": "zYFn0oW3sGVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-1) 병합\n",
        "- 파이토치의 tensor와 넘파이 numpy는 매우 유사한 속성들을 갖음\n",
        "- np.concatenate = np.hstack = torch.cat = torch.hstack\n",
        "  - 가로 방향으로 병합\n",
        "- np.vstack = torch.vstack\n",
        "  - 세로 방향으로 병합"
      ],
      "metadata": {
        "id": "HShCd73isMcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#numpy\n",
        "arr1 = np.array([[1, 2], [3, 4]])\n",
        "arr2 = np.array([[5, 6], [7, 8]])\n",
        "print(np.concatenate((arr1, arr2), axis=1))\n",
        "print(np.vstack((arr1, arr2)))\n",
        "print(np.hstack((arr1, arr2)))\n",
        "#torch\n",
        "a1 = torch.tensor(arr1)\n",
        "a2 = torch.tensor(arr2)\n",
        "print(torch.cat((a1, a2), axis=1))\n",
        "print(torch.vstack((a1, a2)))\n",
        "print(torch.hstack((a1, a2)))"
      ],
      "metadata": {
        "id": "aDbvx7EjgGEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-2) 유용한 함수들\n",
        "- torch.eq함수\n",
        "  - 두 개의 입력 텐서를 받아 같은 모양의 텐서를 반환하며 두 텐서의 원소 간의 동등성을 비교하여 불린 값으로 채워진 텐서를 반환함\n",
        "- torch.softmax 함수\n",
        "  - 입력값을 확률 형태로 변환해주는 함수\n",
        "  - 다중 클래스 분류 문제에서 예측된 점수를 확률로 변환하는 데 사용됨\n",
        "  - 함수의 사용 형식\n",
        "    - 코드 : torch.softmax(input,dim=None,dtype=None)\n",
        "    - input : 소프트맥스 함수를 적용하려는 입력 텐서\n",
        "    - dim(선택적) : 소프트맥스를 계산할 차원(Dimension)\n",
        "      - 기본값 : None\n",
        "      - None인 경우 입력 텐서의 마지막 차원이 사용됨\n",
        "    - dtype(선택적) : 출력 텐서의 데이터 타입\n",
        "      - 기본값 : None\n",
        "      - 입력 텐서의 데이터 타입과 동일한 데이터 타입으로 출력 텐서가 생성됨\n",
        "  - 입력 텐서 input의 각 원소에 대해 소프트맥스를 계산하여 확률 값으로 변환된 텐서를 반환함\n",
        "  - 반환된 텐서는 원본 텐서와 같은 모양을 가지며 각 원소의 합이 1인 확률 분포로 변환됨\n",
        "  - dim 인자를 사용하여 소프트맥스를 계산할 차원을 지정할 수 있음"
      ],
      "metadata": {
        "id": "_XSsWWOWgFgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.eq\n",
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([1, 2, 3])\n",
        "c = torch.tensor([1, 2, 4])\n",
        "\n",
        "result1 = torch.eq(a, b)\n",
        "result2 = torch.eq(a, c)\n",
        "\n",
        "#torch.softmax\n",
        "a = torch.tensor([1.0, 2.0, 3.0])  # 입력 텐서\n",
        "softmax_output = torch.softmax(a, dim=0)  # dim=0으로 소프트맥스 계산\n",
        "print(softmax_output)  # 출력: tensor([0.0900, 0.2447, 0.6652])"
      ],
      "metadata": {
        "id": "sHmE_jI1hUm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-3) 인플레이스 연산\n",
        "- 원본 텐서의 값을 변경\n",
        "- 메모리를 절약할 수 있는 장점이 있음\n",
        "- 파이토치에서 많이 사용되는 연산 중 하나\n",
        "- 인플레이스 연산은 다음과 같아 _가 연산자 뒤에 붙은 메소드를 사용하여 수행할 수 있음\n",
        "  - add_\n",
        "  - sub_\n",
        "  - mul_\n",
        "  - div_\n",
        "  - pow_\n",
        "  - sqrt_\n",
        "  - round_\n",
        "  - floor_\n",
        "  - ceil_\n",
        "  - clamp_\n",
        "  - fill_\n",
        "- 주의할 점\n",
        "  - 인플레이스 연산을 사용하면 원본 데이터가 변경됨\n",
        "  - 반드시 복사본을 생성한 뒤에 인플레이스 연산을 사용해야 함\n",
        "\n",
        "\n",
        "- .fill() vs .full()\n",
        "  - 텐서의 값을 채우는데 사용됨\n",
        "  - .fill() 함수\n",
        "    - 텐서의 모든 요소를 지정된 값으로 채움\n",
        "    - 인플레이스 연산임\n",
        "  - .full() 함수\n",
        "    - 주어진 크기와 지정된 값으로 새로운 텐서를 생성함\n",
        "    - 인플레이스 연산이 아니기 때문에 새로운 텐서가 생성됨\n",
        "    - 기존 텐서를 유지한 채 지정된 값으로 텐서를 새로 생성하고 싶을 때 유용함\n",
        "    "
      ],
      "metadata": {
        "id": "g_UtA2kjhS9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 인플레이스 연산 예제\n",
        "a = torch.tensor([1, 2, 3])\n",
        "b = a\n",
        "a.add_(torch.tensor([1, 1, 1]))\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "#.fill()\n",
        "# 3x3 크기의 텐서 생성\n",
        "x = torch.zeros(3, 3)\n",
        "\n",
        "# 모든 요소를 1로 채우기\n",
        "x.fill_(1)\n",
        "\n",
        "#.full()\n",
        "x = torch.full((3, 3), 5)"
      ],
      "metadata": {
        "id": "lr9kl9sejR8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dataset과 DataLoader\n",
        "- Dataset과 DataLoader 클래스를 사용하는 이유\n",
        "  - 데이터를 효율적으로 처리하고 모델 학습에 사용하기 위한 데이터 파이프라인을 구축하기 위해서임\n",
        "- 위의 두 개 클래스는 파이토치에서 제공하는 데이터 로딩 및 전처리 기능을 제공함\n",
        "  - 모델 학습에 필요한 데이터를 배치 단위로 처리할 수 있도록 도와줌\n",
        "\n",
        "## 1) Dataset\n",
        "- Dataset 클래스 : 데이터 로딩과 전처리를 담당\n",
        "  - 사용자가 정의하는 데이터셋에 대한 인터페이스를 제공함\n",
        "  - 사용자는 Dataset 클래스를 상속받아 자신의 데이터셋에 맞게 커스텀 데이터셋 클래스를 구현함\n",
        "  - 데이터셋의 샘플을 가져오고 전처리를 수행하며 샘플의 개수를 반환하는 등의 기능을 제공함\n",
        "- Dataset 클래스는 다음과 같은 메서드를 구현해야함\n",
        "  - len() : 데이터셋의 전체 샘플 개수를 반환하는 메서드\n",
        "    - 정수 값을 반환함\n",
        "  - getitem(idx) : 인덱스 idx에 해당하는 샘플을 가져오는 메서드\n",
        "    - 인덱스에 따라 샘플 데이터와 레이블 등을 반환함\n",
        "    - 데이터셋을 효율적으로 로딩하고 전처리할 수 있음\n",
        "    - 모델 학습에 필요한 데이터를 제공할 수 있음\n",
        "    - DataLoader 클래스와 함께 사용하여 데이터의 배치 처리와 데이터 로딩의 병렬 처리 등을 간편하게 구현할 수 있음\n",
        "- Dataset 클래스는 다양한 종류의 데이터셋에 사용될 수 있음\n",
        "  - 이미지, 텍스트, 오디오, 비디오 등\n",
        "  "
      ],
      "metadata": {
        "id": "DM3YhXdijYv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        \"\"\"\n",
        "        커스텀 이미지 데이터셋 클래스의 생성자입니다.\n",
        "\n",
        "        Args:\n",
        "            file_paths (list): 이미지 파일 경로의 리스트\n",
        "            labels (list): 이미지 레이블의 리스트\n",
        "            transform (callable, optional): 이미지에 적용할 전처리 함수\n",
        "        \"\"\"\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        데이터셋의 전체 샘플 개수를 반환합니다.\n",
        "        \"\"\"\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        인덱스에 해당하는 샘플을 가져옵니다.\n",
        "\n",
        "        Args:\n",
        "            idx (int): 샘플의 인덱스\n",
        "\n",
        "        Returns:\n",
        "            image (torch.Tensor): 이미지 데이터의 텐서\n",
        "            label (torch.Tensor): 이미지 레이블의 텐서\n",
        "        \"\"\"\n",
        "        # 이미지 파일을 불러옴\n",
        "        image = Image.open(self.file_paths[idx])\n",
        "\n",
        "        # 이미지에 전처리 함수를 적용 (예: Resize, RandomCrop, ToTensor 등)\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # 이미지 레이블을 텐서로 변환\n",
        "        label = torch.tensor(self.labels[idx])\n",
        "\n",
        "        return image, label\n",
        "# -> DataLoader 클래스와 함께 사용하여 데이터의 배치 처리와 병렬 처리를 간편하게 구현할 수 있음"
      ],
      "metadata": {
        "id": "etyvumaFJAOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-1) Custom Dataset과 transforms\n",
        "- torchvision.transforms을 사용하여 이미지 전처리 작업을 정의함\n",
        "- Custom Dataset 클래스의 \\_\\_getitem\\_\\_ 메서드에서 이미지 데이터에 transforms를 적용하게 됨\n"
      ],
      "metadata": {
        "id": "tLWEEQWXJmST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, file_list, label_list, img_T=None):\n",
        "        self.file_list = file_list\n",
        "        self.label_list = label_list\n",
        "        self.img_T = img_T\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.file_list[idx])\n",
        "        label = self.label_list[idx]\n",
        "\n",
        "        if self.img_T is not None: # 이미지 데이터에 self.img_T 적용하고 텐서와 레이블을 반환하도록 정의함\n",
        "            image = self.img_T(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)"
      ],
      "metadata": {
        "id": "1KOuQI6pJ8hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CustomImageDataset 클래스 인스턴스 생성\n",
        "# CustomImageDataset 클래스를 인스턴스화할 때 img_T 인자에 torchvision.transforms에서 정의한\n",
        "# 전처리 작업을 적용하여 이미지 데이터를 전처리할 수 있음\n",
        "dataset = CustomImageDataset(file_list, label_list, img_T=img_T)"
      ],
      "metadata": {
        "id": "iflasEDbKOOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-2) torchvision.transforms\n",
        "- 파이토치에서 이미지 데이터의 전처리 및 데이터 증강을 위해 제공하는 모듈\n",
        "- 기능\n",
        "  - Resize : 이미지의 크기를 조절함\n",
        "  - RandomResizedCrop : 이미지를 무작위로 자르고 크기를 조절함\n",
        "  - RandomHorizontalFlip : 이미지를 무작위로 수평으로 뒤집음\n",
        "  - RandomVerticalFlip : 이미지를 무작위로 수직으로 뒤집음\n",
        "  - ToTensor : 이미지를 텐서로 변환함\n",
        "  - Normalize : 이미지를 정규화함\n",
        "  - ColorJitter : 이미지의 색상을 무작위로 조정함\n",
        "  - RandomRotation : 이미지를 무작위로 회전함\n",
        "  - RandomCrop : 이미지를 무작위로 자름\n",
        "  - Grayscale : 이미지를 흑백으로 변환함\n",
        "  - RandomSizedCrop : 이미지를 무작위로 자르고 크기를 조절함\n",
        "- 이 외에도 다양한 전처리 및 데이터 증강 함수들이 있어서 다양한 이미지 데이터 처리 작업에 유용하게 사용될 수 있음\n"
      ],
      "metadata": {
        "id": "RNm7vjKGKlbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "\n",
        "# 이미지 전처리 작업을 정의\n",
        "preprocess = T.Compose([\n",
        "    T.Resize((256, 256)), # 이미지의 종횡비는 유지됨 (ex) 원본 이미지의 크기가 32x64였다면 Resize(16)을 적용하면 16x32 크기의 이미지가 생성됨\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5), (0.5))# T.Normalize([0.5],[0.5])\n",
        "])\n",
        "\n",
        "# 이미지에 전처리 작업 적용\n",
        "image = Image.open('image.jpg')\n",
        "imge = preprocess(image)"
      ],
      "metadata": {
        "id": "Edlxcv1zLoWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1-3) torchvision.utils.save_image\n",
        "- 이미지 텐서를 파일로 저장하는 함수\n",
        "- 주로 딥러닝 모델이 생성한 이미지를 저장할 때 사용됨\n",
        "- torchvision.utils.save_image(tensor,filename,nrow=8,padding=2,normalize=False,range=None,scale_each=False,pad_value=0)\n",
        "  - tensor : 저장할 이미지 텐서\n",
        "    - shape : (batch_size, channels, height, width) 형태\n",
        "  - filename : 저장할 파일의 경로와 이름\n",
        "  - nrow : 저장할 이미지들을 한 줄에 몇 개씩 보여줄 것인지 결정하는 인자\n",
        "    - 기본값 : 8\n",
        "  - padding : 이미지들 사이의 간격을 몇 개의 픽셀로 할 것인지를 결정하는 인자\n",
        "    - 기본값 : 2\n",
        "  - normalize : 이미지의 값을 0,1로 정규화할 것인지 결정하는 인자\n",
        "    - 기본값 : True\n",
        "  - range : 이미지를 정규화할 때 사용할 범위를 결정하는 인자\n",
        "    - 기본값 : None\n",
        "      - 입력된 텐서의 값 범위를 그대로 사용함\n",
        "  - scale_each : 이미지를 정규화할 때 각 이미지마다 다른 범위를 사용할지 여부를 결정하는 인자\n",
        "    - 기본값 : False\n",
        "  - pad_value : 이미지의 테두리를 채우는 값\n",
        "    - 기본값 0임\n",
        "\n"
      ],
      "metadata": {
        "id": "YQP67RbpL8Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "# Create a tensor of shape (3, 64, 64) representing a single RGB image\n",
        "img = torch.randn(3, 64, 64)\n",
        "\n",
        "# Save the image to a file\n",
        "vutils.save_image(img, 'my_image.png')"
      ],
      "metadata": {
        "id": "OBvLPBtlWCUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) DataLoader\n",
        "- 파이토치에서 제공하는 데이터 로딩 유틸리티\n",
        "- 모델 학습 시에 데이터를 배치 단위로 로드하여 효율적인 학습을 가능하게 해주는 클래스\n",
        "- 데이터 로딩의 성능을 최적화할 수 있음\n",
        "- 반복 가능한 객체로 for루프를 사용하여 데이터를 배치 단위로 간단하게 로드할 수 있음\n",
        "  - 배치 단위로 로드된 데이터는 모델에 전달되어 학습이 이루어짐\n",
        "- DataLoader의 중요한 인자들과 기능\n",
        "  - dataset : 데이터를 로드할 데이터셋 객체를 지정\n",
        "    - torch.utils.data.Dataset 클래스를 상속받은 사용자 정의 데이터셋 클래스를 사용하거나 torchvision의 내장 데이터셋 클래스를 사용할 수 있음\n",
        "      - 기본값 : None\n",
        "  - batch_size : 한 번에 로드할 배치의 크기를 지정함\n",
        "    - 작은 배치 크기는 더 많은 메모리를 사용하지만 더 자주 모델이 업데이트되고 더 높은 학습 속도를 제공함\n",
        "      - 기본값 : 1\n",
        "  - shuffle : 데이터를 섞을지 여부를 지정함\n",
        "    - True로 설정할 경우, 데이터가 매 에폭마다 섞여서 모델이 각 배치에서 다양한 데이터를 학습하도록 도와줌\n",
        "    - 기본값 : False\n",
        "  - num_workers : 데이터 로딩에 사용할 워커의 수를 지정함\n",
        "    -  병렬 처리를 통해 데이터 로딩 속도를 향상시키는 데 사용됨\n",
        "    - 기본값 : 0\n",
        "  - pin_memory : GPU 메모리에 데이터를 고정할지 여부를 지정함\n",
        "    - GPU를 사용하는 경우, True로 설정하면 데이터가 CPU와 GPU 간에 더 빠르게 복사되어 학습 속도를 향상시킬 수 있음\n",
        "    - 기본값 : False\n",
        "  - collate_fn : 배치를 생성하기 전에 데이터를 결합하는 함수를 지정함\n",
        "    - 기본값 : None\n",
        "    - 데이터셋이 출력하는 원시 데이터의 리스트를 배치로 결합함\n",
        "    - 필요에 따라 사용자 정의 결합 함수를 지정하여 배치를 구성할 수 있음\n",
        "  - drop_last : 마지막 배치의 크기가 batch_size보다 작을 경우 해당 배치를 무시할지 여부를 지정함\n",
        "    - True로 설정할 경우 마지막 배치를 무시함\n",
        "    - 기본값 : False\n",
        "\n",
        "- pin_memory\n",
        "  - 파이토치의 DataLoader 클래스의 인자 중 하나\n",
        "  - GPU를 사용하는 경우 데이터를 GPU 메모리에 고정시키는 옵션임\n",
        "  - CPU와 GPU 간의 데이터 복사가 최소화되어 데이터 로딩 속도를 향상시킬 수 있음\n",
        "  - 작동 방식\n",
        "    1. DataLoader가 데이터를 로딩하여 CPU 메모리에 올림\n",
        "    2. pin_memory가 True로 설정된 경우, CPU 메모리에 있는 데이터를 GPU 메모리에 고정시킴\n",
        "    3. 모델이 데이터를 GPU에서 사용할 때, 데이터는 GPU 메모리에서 직접 읽히므로 CPU와 GPU 간의 데이터 복사가 생략됨\n",
        "  - 주의사항\n",
        "    - 사용 시, GPU 메모리를 고려하여 메모리 용량을 조절하는 것이 중요함\n",
        "    - 큰 용량의 데이터를 로딩할 경우, GPU 메모리에 고정시키면 GPU 메모리 부족으로 인한 Out of Memory(OOM) 에러가 발생할 수 있음\n",
        "      - 사용하는 하드웨이의 GPU 메모리 용량과 데이터의 크기를 고려하여 pin_memory 옵션을 설정해야 함"
      ],
      "metadata": {
        "id": "pfeCZbgLWBGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CustomImageDataset 객체 생성\n",
        "dataset = CustomImageDataset(file_list, label_list, transform=transform)\n",
        "\n",
        "# DataLoader 생성\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "# 데이터 로딩을 위한 반복문\n",
        "for images, labels in dataloader:\n",
        "    # images와 labels를 이용하여 모델 학습 수행\n",
        "    pass"
      ],
      "metadata": {
        "id": "erSPGr4EwEyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model 만들기\n",
        "- 파이토치에서 모델을 만드는 방법\n",
        "  - nn.Module 클래스를 상속하여 모델 클래스를 정의\n",
        "    - 명시적으로 forward() 메서드를 정의해야 함\n",
        "    - forward() 메서드를 정의해야지 순전파 동작을 정의할 수 있음\n",
        "  - nn.Sequential 클래스를 사용하여 모델을 정의\n",
        "    - 자동으로 순차적으로 레이어들이 연결되어 순전파 동작이 정의됨"
      ],
      "metadata": {
        "id": "FrQ1-79Hv-b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nn.Module 클래스를 상속하여 모델 클래스를 정의하는 방법:\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        # 모델의 레이어들을 정의하고 초기화\n",
        "        self.layer1 = nn.Linear(784, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(256, 64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.layer3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 모델의 순전파 동작을 구현\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        "\n",
        "# nn.Sequential 클래스를 사용하여 모델을 정의하는 방법:\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(784, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 10)\n",
        ")\n"
      ],
      "metadata": {
        "id": "JigP5AaNwJHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#n.Module과 nn.Sequential을 함께 사용하여 모델을 정의할 수도 있음\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        # nn.Sequential을 사용하여 레이어들을 조합\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(784, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # nn.Sequential로 정의한 레이어들의 순전파 동작이 자동으로 수행됨\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "#위의 예시처럼 사용 시, nn.Module을 상속한 클래스를 통해 모델의 다양한 기능들을 추가하고\n",
        "#nn.Sequential을 사용하여 레이어들을 간단하게 조합할 수 있음"
      ],
      "metadata": {
        "id": "R6UsrVyiwwyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1) nn.BatchNorm1d\n",
        "- 파이토치 라이브러리의 하나의 정규화 모듈\n",
        "- 인공 신경망에서 배치 정규화를 수행하는데 사용됨\n",
        "  - 배치 정규화 : 입력 데이터를 평균과 표준편차로 정규화하여 모델이 더 잘 수렴하도록 돕는 방법 중 하나\n",
        "- 1차원 입력에 대해서만 정규화를 수행\n",
        "  - 생성자에서 num_features 인자를 받음\n",
        "    - 해당 인자는 입력 데이터의 채널 수를 나타냄\n",
        "- 코드\n",
        "  - torch.nn.BatchNorm1d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)\n",
        "    - num_features : 입력 데이터의 채널 수를 지정하는 인자\n",
        "      - 반드시 지정해야함\n",
        "    - eps : 분모에 더해지는 작은 값\n",
        "      - 0으로 나누는 것을 방지하기 위한 인자\n",
        "      - 기본값 : 0.1\n",
        "    - momentum : 이전 배치의 평균과 분산값을 얼마나 반영할지를 지정하는 인자\n",
        "      - 기본값 : 0.1\n",
        "    - affine : 정규화된 값을 확대 및 이동시킬지 여부를 지정하는 인자\n",
        "      - 기본값 : True\n",
        "    "
      ],
      "metadata": {
        "id": "dGMgHJTnxDls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(100, 50)\n",
        "        self.bn = nn.BatchNorm1d(num_features=50)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "CwvABp501Hd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) nn.BatchNorm2d\n",
        "- 파이토치의 배치 정규화를 수행하는 클래스\n",
        "- 2차원 이미지 데이터에 대한 배치 정규화를 적용할 수 있음\n",
        "- 형식\n",
        "  - 코드 : nn.BatchNorm2d(num_features,eps=1e-05,momentum=0.1, affine=True, track_running_stats=True)\n",
        "    - num_features: 입력 채널의 개수(정수)\n",
        "    - eps : 분모에 더해지는 작은 값(스칼라)\n",
        "      - 기본값 : 1e-05\n",
        "    - momentum : 배치 정규화의 이동 평균 계산 시 사용되는 모멘텀 값(스칼라)\n",
        "      - 기본값 : 0.1\n",
        "    - affine : 스케일과 시프트 매개변수를 학습할지 여부(불리언)\n",
        "      - 기본값 : True\n",
        "    - track_running_stats : 배치 정규화의 통계량을 추적할지 여부(불리언)\n",
        "      - 기본값 : True"
      ],
      "metadata": {
        "id": "utW8pk0l1QrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (3) nn.Conv1d\n",
        "- 파이토치에서 1차원 컨볼루션 레이어를 정의하는 클래스\n",
        "- 입력 데이터의 한 방향(주로 시계열 데이터에서는 시간 축)으로 컨볼루션 연산을 수행함\n",
        "- 코드\n",
        "  - nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
        "    - in_channels : 입력 데이터의 채널 개수\n",
        "      - ex) 입력 데이터가 RGB 이미지인 경우 in_channels : 3\n",
        "    - out_channels : 출력 데이터의 채널 개수\n",
        "      - 컨볼루션 필터의 개수를 의미\n",
        "      - 출력 데이터가 몇 개의 특징 맵으로 변환되는지를 결정함\n",
        "    - kernel_size : 컨볼루션 필터(커널)의 크기\n",
        "      - 정수 또는 튜플 형태로 지정할 수 있음\n",
        "        - ex) kernel_size=3 : 3개의 연속된 입력 값에 대해 컨볼루션 연산을 수행\n",
        "        - ex) kernel_size=(3,5) : 3개의 연속된 입력 값에 대해 한 방향으로 5개의 컨볼루션 연산을 수행\n",
        "    - padding : 입력 데이터에 대해 가상의 패딩을 추가하는 것\n",
        "      - 컨볼루션 연산의 경계 효과를 조절함\n",
        "      - 정수 또는 튜플 형태로 지정할 수 있음\n",
        "        - ex) padding=1 : 입력 데이터에 한 칸의 패딩을 추가\n",
        "        - ex) padding=(1,2) : 입력 데이터에 한 방향으로 한 칸의 패딩을 추가하고 다른 방향으로 두 칸의 패딩을 추가함\n",
        "    - dilation : 컨볼루션 필터 내의 값 사이의 간격을 조절하여 더 넓은 영역을 감지할 수 있도록 함\n",
        "      - 정수 또는 튜플 형태로 지정할 수 있음\n",
        "    - groups : 입력 데이터와 출력 데이터의 채널을 그룹화하여 연산을 수행하는 것\n",
        "      - 다양한 네트워크 아키텍처를 구성하는데 사용됨\n",
        "    - bias : 편향 사용 여부를 결정하는 불리언 값\n",
        "      - 기본값 : True로 편향이 사용됨\n",
        "      - False : 편향을 사용하지 않을 수도 있음\n",
        "      \n"
      ],
      "metadata": {
        "id": "lyvHpT8t2QnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 입력 데이터의 크기: (배치 크기, 채널, 시퀀스 길이)\n",
        "input_size = (16, 3, 100)\n",
        "\n",
        "# 1차원 컨볼루션 레이어 정의\n",
        "conv1d = nn.Conv1d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "# 입력 데이터 생성\n",
        "input_data = torch.randn(input_size)\n",
        "\n",
        "# 컨볼루션 연산 수행\n",
        "output = conv1d(input_data)\n",
        "\n",
        "# 출력 데이터의 크기: (배치 크기, 출력 채널, 출력 시퀀스 길이)\n",
        "print(\"Output size:\", output.size())\n",
        "\n",
        "Output size: torch.Size([16, 16, 100])"
      ],
      "metadata": {
        "id": "fe7BpT0WPOHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (4) nn.Conv2d\n",
        "- 파이토치에서 제공하는 2D 컨볼루션 레이어 클래스\n",
        "- 이미지나 2D 데이터의 특징 추출에 주로 사용되며 합성곱 신경망에서 핵심적인 레이어 중 하나\n",
        "- 코드\n",
        "  - nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
        "    - in_channels : 입력 채널의 개수\n",
        "      - ex) RGB 이미지의 경우 3개의 채널을 가지므로 in_channels는 3이 됨\n",
        "      - int\n",
        "    - out_channels : 출력 채널의 개수(컨볼루션 필터의 개수)\n",
        "      - 이 값이 클수록 더 복잡한 특징을 학습할 수 있지만 모델의 파라미터 수가 증가하게 됨\n",
        "      - int\n",
        "    - kernel_size : 컨볼루션 필터의 크기\n",
        "      - ex) 3x3 필터의 경우 kernel_size는 3 또는 (3,3)으로 지정할 수 있음\n",
        "      - int 또는 tuple\n",
        "    - stride : 필터의 이동 간격\n",
        "      - 기본값 : 1\n",
        "      - 더 큰 값으로 설정하면 출력 특징 맵의 크기가 작아짐\n",
        "      - int 또는 tuple, optional\n",
        "    - padding : 입력 데이터의 가장자리에 추가되는 패딩의 크기\n",
        "      - 기본값 : 0\n",
        "      - 패딩을 사용하면 출력 특징 맵의 크기를 보존하면서 입력 데이터의 가장자리 정보를 유지할 수 있음\n",
        "      - int 또는 tuple, optional\n",
        "    - dilation : 필터의 간격을 더 크게 두어 더 넓은 영역의 정보를 가져오는 데 사용됨\n",
        "      - 기본값 : 1\n",
        "      - 값이 커질수록 필터의 영역이 더 넓어지게 됨\n",
        "      - int 또는 tuple, optional\n",
        "    - groups : 입력 및 출력 채널을 묶는 개수\n",
        "      - 기본값 : 1\n",
        "      - 값이 크면 채널 간의 관련성을 줄이는 효과가 있음\n",
        "      - int, optional\n",
        "    - bias : 편향을 사용할지 여부를 결정하는 플래그\n",
        "      - 기본값 : Tue\n",
        "      - False로 설정하면 편향이 사용되지 않음\n",
        "      - bool, optional\n"
      ],
      "metadata": {
        "id": "HTfCtcmoBRy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 입력 데이터의 크기: (배치 크기, 채널, 높이, 너비)\n",
        "input_size = (64, 3, 32, 32)\n",
        "\n",
        "# Conv2d 레이어 정의\n",
        "conv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "# 입력 데이터 생성\n",
        "input_data = torch.randn(input_size)\n",
        "\n",
        "# 컨볼루션 연산 수행\n",
        "output = conv(input_data)\n",
        "\n",
        "# 출력 데이터의 크기 출력\n",
        "print(\"Output size:\", output.size())\n",
        "\n",
        "Output size: torch.Size([64, 64, 32, 32])"
      ],
      "metadata": {
        "id": "O7PvKX2g1PrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (5) nn.Flatten()\n",
        "- Pytorch의 텐서를 1차원으로 평탄화하는 클래스\n",
        "- 다차원 텐서를 1차원으로 변환하여 다층 퍼셉트론 등의 신경망 레이어에 입력으로 제공할 수 있게 해줌\n",
        "- 입력 텐서를 평탄화하는 작업을 수행하므로 별도의 인수나 기본 값이 없음\n"
      ],
      "metadata": {
        "id": "v2QtHwlAP4tD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ex) 크기가 (batch_size, num_channels, height, width)인 4차원 입력 텐서를 평탄화하여 1차원으로 변환\n",
        "x = torch.randn(batch_size, num_channels, height, width)\n",
        "flatten = nn.Flatten()\n",
        "x_flatten = flatten(x) # 똑같은 결과 : x = x.view(x.size(0), -1)"
      ],
      "metadata": {
        "id": "dbRZLRpF6Y57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (6) nn.Linear\n",
        "- 파이토치에서 사용되는 선형 변환(linear transformation)을 수행하는 클래스\n",
        "- fully conncted layer 또는 dense layer라고도 불림\n",
        "- nn.Linear 클래스트의 생성자(\\_\\_init\\_\\_) 인수\n",
        "  - in_features(int) : 입력 텐서의 크기\n",
        "    - 입력 텐서의 차원 또는 특성의 수\n",
        "  - out_features(int) : 출력 텐서의 크기\n",
        "    - 출력 텐서의 차원 또는 특성의 수\n",
        "  - bias(bool,optional) : 편향을 사용할지 여부를 지정\n",
        "    - 기본값 : True\n",
        "- 두 개의 행렬 가중치와 편향을 학습하며 입력 텐서를 선형 변환하여 출력 텐서를 생성함\n",
        "  - 선형 변환은 입력 텐서와 가중치 행렬의 행렬 곱을 계산하고 편향을 더하는 연산으로 이루어짐"
      ],
      "metadata": {
        "id": "1yrLRMnX6jHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 입력 텐서의 크기가 10이고 출력 텐서의 크기가 20인 선형 변환을 수행하는 nn.Linear 모듈 생성\n",
        "linear = nn.Linear(10, 20)\n",
        "\n",
        "# 입력 텐서 생성 (크기가 10인 벡터)\n",
        "input_tensor = torch.randn(1, 10)\n",
        "\n",
        "# 선형 변환 수행 (입력 텐서를 출력 텐서로 변환)\n",
        "output_tensor = linear(input_tensor)\n",
        "\n",
        "print(\"Input Tensor Size: \", input_tensor.size())\n",
        "print(\"Output Tensor Size: \", output_tensor.size())\n",
        "\n",
        "Input Tensor Size:  torch.Size([1, 10])\n",
        "Output Tensor Size:  torch.Size([1, 20])"
      ],
      "metadata": {
        "id": "TMmFLf356Yzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (7) nn.MaxPool1d\n",
        "- 파이토치 라이브러리에서 제공하는 1차원 최대 풀링 연산을 수행하는 클래스\n",
        "- 1차원 신호(예 : 오디오, 텍스트)를 처리하는 컨볼루션 신경망에서 주로 사용됨\n",
        "- Max 풀링 : 피처 맵의 공간 차원을 줄이는 역할을 함\n",
        "  - 컨볼루션 연산을 통해 추출된 특징들을 압축하고 불필요한 정보를 줄이는 효과를 얻을 수 있음\n",
        "  - 입력 피처 맵에서 최대값을 선택하여 출력 피처 맵을 생성함\n",
        "- 주요한 매개변수\n",
        "  - kernel_size : 풀링 윈도우의 크기를 나타내는 정수 값 또는 튜플\n",
        "    - 입력 신호에서 추출할 최대값을 결정하는데 사용됨\n",
        "    - 일반적으로 사용하는 값 : 2 또는 3과 같은 작은 정수 값\n",
        "  - stride : 풀링 윈도우의 이동 간격을 나타내는 정수 값 또는 튜플임\n",
        "    - 풀링 연산의 겹침을 조절\n",
        "    - 일반적으로 kernel_size와 같은 값을 사용함\n",
        "    - 기본값 : None으로 kernel_size와 같은 값 설정됨\n",
        "  - padding : 입력 신호 주위에 추가할 패딩의 크기를 나타내는 정수 값 또는 튜플임\n",
        "    - 입력 신호의 경계 부분에서 풀링 윈도우가 넘어갈 때 발생하는 정보 손실을 줄이는 역할을 함\n",
        "    - 기본값 : 0\n",
        "    - 일반적으로 0 또는 1과 같은 작은 값이 사용됨\n",
        "- 입력 텐서의 크기를 변환하여 출력 텐서를 생성함\n",
        "  - 입력 텐서의 크기와 풀링 윈도우의 크기, 이동 간격, 패딩의 크기 등에 따라 출력 텐서의 크기가 결정됨\n",
        "  - 피처 맵의 크기를 다운샘플하고 컨볼루션 신경망에서의 공간적인 계산을 줄여 연산 효율성을 높일 수 있음\n",
        "  "
      ],
      "metadata": {
        "id": "yvrBrHfv7qyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 입력 텐서 생성 (배치 크기: 1, 채널: 1, 시퀀스 길이: 10)\n",
        "input_tensor = torch.randn(1, 1, 10)\n",
        "\n",
        "# MaxPool1d 인스턴스 생성\n",
        "maxpool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "# 최대 풀링 수행\n",
        "output_tensor = maxpool(input_tensor)\n",
        "\n",
        "# 입력 텐서와 출력 텐서의 크기 확인\n",
        "print(\"Input tensor size:\", input_tensor.size())\n",
        "print(\"Output tensor size:\", output_tensor.size())\n",
        "\n",
        "Input tensor size: torch.Size([1, 1, 10])\n",
        "Output tensor size: torch.Size([1, 1, 5])"
      ],
      "metadata": {
        "id": "mliea4Oz9i12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (8) nn.MaxPool2d\n",
        "- 코드 : torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
        "- 파라미터\n",
        "  - kernel_size : 풀링 윈도우의 크기를 나타내는 정수 값 또는 튜플\n",
        "  - stride : 풀링 윈도우의 이동 간격을 나타내는 정수 값 또는 튜플\n",
        "    - 기본값 : None\n",
        "  - padding : 입력 신호 주위에 추가할 패딩의 크기를 나타내는 정수 값 또는 튜플\n",
        "    - 기본값 : 0\n",
        "  - dilation : 커널 사이 간격 사이즈를 조절함\n",
        "    - 기본값 : 1\n",
        "  - return_indices : True 일 경우 최대 인덱스 를 반환\n",
        "    - 기본값 : False\n",
        "  - ceil_mode : True 일 경우, Output shape에 대하여 내림 대신, 올림을 사용\n"
      ],
      "metadata": {
        "id": "B_vpPRxY9dXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pool of square window of size=3, stride=2\n",
        "m = nn.MaxPool2d(3, stride=2)\n",
        "# pool of non-square window\n",
        "m = nn.MaxPool2d((3, 2), stride=(2, 1))\n",
        "input = torch.randn(20, 16, 50, 32)\n",
        "output = m(input)"
      ],
      "metadata": {
        "id": "uGlcLwV-_jaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (9) nn.ModuleList()\n",
        "- 파이토치에서 사용되는 모듈들을 리스트 형태로 관리하는 클래스\n",
        "- 동적으로 모듈들을 추가하거나 삭제할 수 있음\n",
        "- nn.Module을 상속한 클래스 내에서 사용됨\n",
        "- 파이토치 모델의 서브 모듈들을 리스트 형태로 정의하고 해당 리스트를 모델 클래스의 속성으로 사용함\n",
        "  - 이렇게 정의된 nn.Modulelist는 자동으로 모델의 파라미터들과 함께 관리되며 모델의 forward 연산에서 호출될 수 있음"
      ],
      "metadata": {
        "id": "_Jo1HQ2xE4xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ex) nn.ModuleList를 사용하여 여러 개의 레이어를 동적으로 관리하는 모델을 정의\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.linears = nn.ModuleList()\n",
        "        for i in range(5):   # nn.ModuleList를 사용하여 nn.Linear 모듈을 5개 추가\n",
        "            self.linears.append(nn.Linear(10, 20))\n",
        "\n",
        "    def forward(self, x):  # forward 함수에서 nn.ModuleList에 있는 모듈들을 순차적으로 호출하여 forward 연산을 수행\n",
        "        for layer in self.linears:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "b4K7iJakGUra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (10) nn.ReLU\n",
        "- 파이토치에서 사용되는 ReLU(Rectified Linear Unit) 활성화 함수를 구현한 클래스\n",
        "- 입력 텐서의 각 요소에 대해 다음과 같은 수식을 적용하여 활성화 함수를 계산함\n",
        "  - ReLU(x) = max(0,x)\n",
        "    - x : 입력 텐서의 값\n",
        "    - 양수인 경우 그대로 반환되고 음수인 경우 0으로 클리핑됨\n",
        "- inplace 인수\n",
        "  - ReLU 함수의 연산을 \"in-place\"로 수행하게 됨\n",
        "  - inplace=True로 설정 시, 입력 텐서의 메모리를 직접 수정하여 연산 속도를 향상시키는 효과가 있음\n",
        "  - 기본값은 False로 되어 있어서 기본적으로 원본 텐서를 수정하지 않고 새로운 텐서를 반환함\n",
        "  - 주의할 점\n",
        "    - inplace=True를 사용할 경우, 원본 텐서가 직접 수정되기 때문에 연산 이후에 원본 텐서를 사용하는 다른 연산에 영향을 줄 수 있다는 점\n",
        "    \n"
      ],
      "metadata": {
        "id": "wK-5Fop_GpR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ReLU 레이어 인스턴스화\n",
        "relu = nn.ReLU()  # inplace=False가 기본값\n",
        "\n",
        "# ReLU 연산 적용\n",
        "x = torch.randn(5)\n",
        "print('x: ',x)\n",
        "y = relu(x)  # 원본 x는 수정되지 않고, 새로운 텐서 y를 반환\n",
        "print('after ReLU()')\n",
        "print('x: ',x)\n",
        "print('y: ',y)\n",
        "print('-'*60)\n",
        "\n",
        "# inplace=True로 설정한 ReLU 연산\n",
        "x = torch.randn(5)\n",
        "print('x: ',x)\n",
        "relu_inplace = nn.ReLU(inplace=True)\n",
        "y = relu_inplace(x)  # 원본 x가 직접 수정\n",
        "print('after ReLU(inplace=True)')\n",
        "print('x: ',x)\n",
        "print('y: ',y)"
      ],
      "metadata": {
        "id": "7WKZtG3jILEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (11) nn.LeakyReLU\n",
        "- ReLU 함수와 유사하지만 입력 값이 음수일 때 기울기를 0이 아닌 작은 값으로 유지함\n",
        "  - ReLU 함수에서 발생하는 죽은 뉴런 문제를 완화할 수 있음\n",
        "- 생성자에서 negative_slope 인자를 받음\n",
        "  - 이 값은 입력 값이 음수일 때 사용할 기울기 값을 결정함\n",
        "  - 보통 0.01이나 0.2와 같은 작은 값이 사용됨\n",
        "- 코드 : torch.nn.LeakyReLU(negative_slope=0.01, inplace=False)\n",
        "  - negative_slope : 음수 기울기 값을 지정하는 인자\n",
        "    - 기본값 : 0.01\n",
        "  - inplace : 연산을 직접 입력 데이터에 수행할지 여부를 지정하는 인자\n",
        "    - 기본값 : False\n",
        "    \n"
      ],
      "metadata": {
        "id": "ppY7_FCBIMzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(100, 50)\n",
        "        self.relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "    # 입력값을 받아 각 층을 거쳐 출력값을 계산하는데 은닉층의 활성화 함수로 LeakyReLU를 사용하여 비선형성을 추가함\n",
        "        return x"
      ],
      "metadata": {
        "id": "Huyu_YDuXiwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model 학습\n",
        "- 주요 내용 : 옵티마이저(optimizer)를 활용한 가중치 업데이트 과정\n",
        "- optim.zero_grad()\n",
        "  - 옵티마이저에 연결된 모델의 모든 가중치에 대한 그래디언트를 초기화함\n",
        "  - 새로운 미니배치에 대한 그래디언트를 계산하기 전에 이전 미니배치의 그래디언트가 남아있는 것을 방지하기 위해 사용됨\n",
        "- loss.backward()\n",
        "  - 손실에 대한 역전파를 수행함\n",
        "  - 역전파는 손실을 최소화하기 위해 모델의 각 가중치에 대한 그래디언트를 계산하는 과정\n",
        "    - 그래디언트는 가중치를 업데이트하는데 사용됨\n",
        "- optim.step()\n",
        "  - 옵티마이저를 사용하여 모델의 가중치를 업데이트함\n",
        "  - 역전파를 통해 계산된 그래디언트를 바탕으로 모델의 가중치를 조정하여 손실을 최소화하는 방향으로 모델을 학습시킴\n",
        "  - 옵티마이저의 설정에 따라 가중치를 업데이트하는 방식이 다양하게 사용될 수 있음\n",
        "  - 경사하강법을 사용하는 경우, 가중치는 학습률과 그래디언트의 곱에 따라 업데이트될 수 있음\n",
        "- optim.zero_grad(), loss_backward(), optim.step() 세 가지 단계를 반복적으로 수행하면 모델이 학습 데이터를 기반으로 가중치를 업데이트하여 최적의 모델 파라미터를 찾아가는 과정인 학습이 이루어짐"
      ],
      "metadata": {
        "id": "wafHeV8eXx8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 모델 정의\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 5)  # 예시로 입력 차원이 10, 출력 차원이 5인 선형 레이어를 사용\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# 모델 초기화\n",
        "model = MyModel()\n",
        "\n",
        "# 손실 함수 정의\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 옵티마이저 정의\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.001)  # 예시로 Adam 옵티마이저를 사용, 학습률은 0.001로 설정\n",
        "\n",
        "# 데이터 로딩, 예시로 데이터는 X와 y로 구성되어 있다고 가정\n",
        "# X와 y를 DataLoader를 사용하여 미니배치로 로딩하고, 모델에 입력으로 전달하는 과정은 생략\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10  # 예시로 10 에포크로 학습을 진행\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # 미니배치 단위로 데이터를 순전파하여 예측값 생성\n",
        "    outputs = model(X)\n",
        "\n",
        "    # 손실 계산\n",
        "    loss = loss_fn(outputs, y)\n",
        "\n",
        "    # 역전파\n",
        "    optim.zero_grad()  # 그래디언트 초기화\n",
        "    loss.backward()  # 손실에 대한 역전파 수행\n",
        "    optim.step()  # 옵티마이저를 사용하여 모델의 가중치 업데이트\n",
        "\n",
        "    # 학습 과정 출력\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "# 모델 학습 완료 후 예측 등의 후처리 작업 수행"
      ],
      "metadata": {
        "id": "iVKuiUb2eOb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) torch.optim\n",
        "- Pytorch에서 제공하는 최적화 알고리즘을 포함하는 모듈\n",
        "- 최적화 : 딥러닝 모델의 학습 과정에서 모델의 가중치와 편향을 업데이트하여 손실 함수를 최소화하는 과정\n",
        "- 해당 모듈은 이러한 최적화 알고리즘을 구현하고 있어 모델의 가중치와 편향을 업데이트하기 위해 사용됨\n",
        "- 해당 모듈은 다양한 최적화 알고리즘을 지원하며 주요한 최적화 알고리즘들을 포함하고 있음\n",
        "- 주요 최적화 알고리즘\n",
        "  - Stochastic Gradient Descent(SGD)\n",
        "    - 가장 기본적인 최적화 알고리즘\n",
        "    - 각각의 파라미터에 대해 학습률을 곱한 값을 사용하여 가중치를 업데이트하는 방식\n",
        "    - 코드 : torch.optim.SGD(params, lr, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False, foreach=None, differentiable=False)\n",
        "  - Adam(Adaptive Moment Estimation)\n",
        "    - 학습률을 각 파라미터마다 적응적으로 조절하는 최적화 알고리즘\n",
        "    - 현재 그래디언트와 이전 그래디언트의 지수 가중 평균을 이용하여 가중치를 업데이트함\n",
        "    - 코드 : torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False, *, foreach=None, maximize=False, capturable=False, differentiable=False, fused=None)\n",
        "  - RMSprop(Root Mean Square Propagation)\n",
        "    - 그래디언트의 제곱값의 이동 평균을 이용하여 학습률을 조절하는 최적화 알고리즘\n",
        "    - Adam과 유사한 방식으로 학습률을 조절함\n",
        "    - 코드 : torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False, foreach=None, maximize=False, differentiable=False)\n",
        "  - Adagrad(Adaptive Gradient Descent)\n",
        "    - 각 파라미터에 대한 학습률을 조절하는 방식으로 모델을 업데이트함\n",
        "    - 이전 그래디언트의 제곱의 누적값을 사용하여 학습률을 조절함\n",
        "    - torch.optim.Adagrad로 사용할 수 있음\n",
        "    - 코드 : torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10, foreach=None, *, maximize=False, differentiable=False)\n",
        "  - AdamW(Adam with Weight Decay)\n",
        "    - Adam 옵티마이저에 가중치 감쇠(L2 정규화)를 추가한 버전\n",
        "    - 가중치 감쇠를 통해 모델을 규제하는 효과를 얻을 수 있음\n",
        "    - 코드 : torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False, *, maximize=False, foreach=None, capturable=False, differentiable=False, fused=None)\n",
        "- 각 최적화 알고리즘은 다양한 하이퍼파라미터를 가지고 있어 사용자는 필요에 따라 학습률(lr), 가중 평균 계수(momentum, beta 등), 그래디언트 클리핑(gradient clipping) 등을 조절하여 최적화 알고리즘의 동작을 제어할 수 있음\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "w4ttrIBseU8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SGD\n",
        "import torch.optim as optim\n",
        "\n",
        "# 모델 정의\n",
        "model = YourModel()\n",
        "\n",
        "# 최적화 알고리즘 선택 및 하이퍼파라미터 설정\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# 학습 과정에서 가중치 업데이트\n",
        "optimizer.zero_grad()  # 그래디언트 초기화\n",
        "outputs = model(inputs)\n",
        "loss = criterion(outputs, targets)\n",
        "loss.backward()  # 그래디언트 계산\n",
        "optimizer.step()  # 가중치 업데이트\n",
        "\n",
        "#Adam\n",
        "model = YourModel()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, eps=1e-07)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "outputs = model(inputs)\n",
        "loss = criterion(outputs, targets)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "#RMSprop\n",
        "model = YourModel()\n",
        "\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.01, eps=1e-07,momentum=0.9)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "outputs = model(inputs)\n",
        "loss = criterion(outputs, targets)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "#adagrad\n",
        "model = YourModel()\n",
        "\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=0.01, eps=1e-07)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "outputs = model(inputs)\n",
        "loss = criterion(outputs, targets)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "#adamw\n",
        "model = YourModel()\n",
        "\n",
        "optimizer = optim.adamw(model.parameters(), lr=0.01, eps=1e-07)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "outputs = model(inputs)\n",
        "loss = criterion(outputs, targets)\n",
        "loss.backward()\n",
        "optimizer.step()\n"
      ],
      "metadata": {
        "id": "PUd1DnxZg1xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1) Adagrad\n",
        "- Adagrad(Adaptive Gradient Descent) 옵티마이저의 구현체\n",
        "- 학습률을 각 파라미터에 대해 독립적으로 조절하는 방식으로 모델을 업데이트하는 옵티마이저임\n",
        "- 이전 그래디언트의 제곱의 누적 값을 사용하여 학습률을 조절하므로 그래디언트가 많이 업데이트된 파라미터는 학습률이 감소하게 되고 그래디언트가 적게 업데이트된 파라미터는 학습률이 증가하게 됨\n",
        "  - 데이터셋에 따라 학습률을 자동으로 조절하여 최적의 학습 성능을 얻을 수 있음\n",
        "- 기본 형태와 인수들의 기본값\n",
        "  - torch.optim.Adagrad(params,lr=0.01,lr_decay=0, weight_decay=0,initial_accumulator_value=0, eps=1e-10)\n",
        "    - params : 최적화할 파라미터들의 iterable\n",
        "    - lr : 학습률(learning rate)\n",
        "      - 기본값 : 0.01\n",
        "    - lr_decay : 학습률 감소율\n",
        "      - 기본값 : 0\n",
        "    - weight_decay : 가중치 감쇠(L2 정규화) 계수\n",
        "      - 기본값 : 0\n",
        "    - initial_accumulator_value: 그래디언트 제곱의 누적 값 초기화\n",
        "      - 기본값 : 0\n",
        "    - eps : 분모를 0으로 나누는 것을 방지하기 위한 작은 상수값\n",
        "      - 기본값 : 1e-10\n",
        "- 희소한 데이터셋에 적합할 수 있음\n",
        "\n",
        "### (2) Adam\n",
        "- Adam(Adaptive Moment Estimation) 현재 가장 널리 사용되는 옵티마이저\n",
        "- 그래디언트의 지수적인 이동 평균을 사용하여 학습률을 조절하는 방식으로 모델을 업데이트함\n",
        "- 경사 하강법 알고리즘을 기반으로 하면서도 모멘텀 및 학습률 감소와 같은 개선된 기능을 추가한 최적화 알고리즘\n",
        "- 주요 인수들\n",
        "  - torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "  - params: 최적화할 모델의 파라미터들을 전달\n",
        "  - lr: 학습률\n",
        "    - 기본값 : 0.001\n",
        "  - betas: Adam 알고리즘에서 사용되는 두 개의 모멘텀 계수(beta1, beta2)를 튜플 형태로 전달\n",
        "    - 기본값 : (0.9, 0.999)\n",
        "  - eps: 분모를 보호하기 위한 작은 값(epsilon)\n",
        "    - 기본값 : 1e-08\n",
        "  - weight_decay: 가중치 감소(L2 정규화)를 적용하는데 사용되는 가중치 감소 계수\n",
        "    - 기본값 : 0\n",
        "  - amsgrad: AMSGrad 알고리즘을 사용할지 여부를 결정하는 불리언 값\n",
        "    - 기본값 : False\n",
        "- 장점 : 학습률을 자동으로 조절하고, 모멘텀을 활용하여 이전의 경사 정보를 이용하여 더 빠르게 수렴할 수 있음\n",
        "\n",
        "\n",
        "### (3) AdamW\n",
        "- AdamW(Adam with Weight Decay) 옵티마이저의 구현체\n",
        "- Adam 옵티마이저의 변형으로 가중치 감쇠를 적용하는 것이 특징\n",
        "  - 가중치 감쇠 효과 : 모델의 가중치를 감소시킴으로써 모델의 복잡성을 제어하고 오버피팅 완화\n",
        "- 기본 형태와 인수들의 기본값\n",
        "  - torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "  - params : 최적화할 파라미터들의 iterable\n",
        "  - lr: 학습률\n",
        "    - 기본값 : 0.001\n",
        "  - betas: 감마 값들 (beta1, beta2)로 이루어진 튜플\n",
        "    - 기본값 : (0.9, 0.999)\n",
        "  - eps: 분모를 0으로 나누는 것을 방지하기 위한 작은 상수값\n",
        "    - 기본값 : 1e-08\n",
        "  - weight_decay: 가중치 감쇠(L2 정규화) 계수\n",
        "    - 기본값 : 0\n",
        "  - amsgrad: AMSGrad 알고리즘을 사용할지 여부\n",
        "    - 기본값 : False\n",
        "- 모델의 일반화 성능을 향상시키는데 사용될 수 있음\n",
        "\n",
        "### (4) RMSprop\n",
        "- Root Mean Square Propagation의 약자\n",
        "- 딥러닝에서 널리 사용되는 최적화 알고리즘 중 하나\n",
        "  - 주로 순환신경망(RNN)과 같이 긴 시퀀스 데이터를 다룰 때 사용됨\n",
        "- 경사의 크기를 지수 이동 평균을 사용하여 조절하며 경사의 크기에 따라 각각의 파라미터를 업데이트함\n",
        "  - 이전 기울기의 크기와 현재 기울기의 크기를 비교하여 기울기의 크기가 크게 변하는 경우 더 작은 학습률을 적용하여 안정적인 학습을 할 수 있음\n",
        "- 인수와 기본값\n",
        "  - torch.optim.RMSprop(params, lr=0.01, momentum=0, alpha=0.99, eps=1e-8, centered=False, weight_decay=0, momentum_decay=0)\n",
        "  - params : 최적화할 파라미터들의 iterable\n",
        "    - 일반적으로 모델의 model.parameters()를 전달함\n",
        "  - lr : 학습률로 업데이트 스텝의 크기를 결정함\n",
        "    - float, optional\n",
        "    - 기본값 : 0.01\n",
        "  - momentum : 모멘텀을 사용하여 업데이트에 관성을 부여함\n",
        "    - float, optional\n",
        "    - 기본값 : 0(0이면 모멘텀을 사용하지 않음)\n",
        "  - alpha : RMSprop에서 이동 평균을 계산할 때 사용되는 계수로 경사의 크기를 조절함\n",
        "    - float, optional\n",
        "    - 기본값 : 0.99\n",
        "    - 1에 가까울수록 이동 평균이 빠르게 갱신되어 빠른 학습이 가능하지만 불안정할 수 있음\n",
        "  - eps : 분모를 0으로 나누는 것을 방지하지 위한 작은 상수\n",
        "    - float, optional\n",
        "    - 기본값 : 1e-8\n",
        "  - centered : True로 설정하면 중앙화된 RMSprop을 사용하여 업데이트함\n",
        "    - bool, optional\n",
        "    - 기본값 : False\n",
        "  - weight_decay : 가중치 감쇠를 적용함\n",
        "    - L2 정규화를 통해 가중치를 규제함\n",
        "    - float, optional\n",
        "    - 기본값 : 0\n",
        "  - momentum_decay : momentum_decay 적용\n",
        "    - float, optional\n",
        "    - 기본값 : 0\n",
        "    - 일반적으로 0.9 이하의 값으로 설정됨\n",
        "- 주요 특징\n",
        "  - 학습률 감쇠\n",
        "    - 경사의 크기에 따라 학습률을 조절하여 안정적인 학습을 수행함\n",
        "    - 경사의 크기가 크게 변하는 경우 더 작은 학습률을 사용하여 모델이 더 빠르게 수렴하도록 도와줌\n",
        "  - 이동 평균 사용\n",
        "    - 이전 기울기의 크기와 현재 기울기의 크기를 비교하여 경사의 크기를 조절함\n",
        "    - 각각의 파라미터별로 적절한 학습률을 적용할 수 있음\n",
        "  - 파라미터별 업데이트\n",
        "    - 각각의 파라미터별로 학습률을 조절하여 업데이트함\n",
        "    - 모델의 각 파라미터가 서로 다른 학습 속도로 업데이트되어 모델의 학습을 더욱 개선할 수 있음\n",
        "\n",
        "### (5) SGD\n",
        "- 확률적 경사 하강법(Stochastic Gradient Descent, SGD) 최적화 알고리즘을 구현\n",
        "- 딥러닝 모델의 학습에서 가장 기본적으로 사용되는 최적화 알고리즘 중 하나\n",
        "- 경사하강법의 확률적인 버전\n",
        "  - 각 업데이트 스텝마다 무작위로 선택된 일부 샘플(mini-batch)에 대한 손실 함수의 그래디언트를 사용하여 모델을 업데이트함\n",
        "- 인수들의 기본값\n",
        "  - torch.optim.SGD(params, lr=<required>, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
        "  - params: 최적화할 모델의 파라미터들을 전달\n",
        "  - lr: 학습률\n",
        "  - momentum: 모멘텀 값\n",
        "    - 기본값 : 0\n",
        "  - dampening: 모멘텀에 적용되는 감쇠 값\n",
        "    - 기본값 : 0\n",
        "  - weight_decay: 가중치 감소(L2 정규화)를 적용하는데 사용되는 가중치 감소 계수\n",
        "    - 기본값 : 0(비활성화)\n",
        "  - nesterov: 네스테로프 모멘텀을 사용할지 여부를 결정\n",
        "    - 기본값 : False(비활성화)\n",
        "    - 불리언 값\n",
        "- 해당 클래스는 다음과 같이 사용됨\n",
        "  - optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "  - model.parameters(): 최적화할 모델의 파라미터들을 전달\n",
        "    - 모델의 가중치와 편향들을 포함\n",
        "  - lr: 학습률\n",
        "    - 가중치를 업데이트할 때 사용되는 스칼라 값\n",
        "    - 학습률은 모델이 얼마나 빠르게 수렴할지를 결정하는 하이퍼파라미터로, 너무 작으면 학습이 느리게 되고, 너무 크면 발산할 수 있음\n",
        "  - momentum (선택적): 모멘텀(momentum) 값 설정\n",
        "    - SGD의 변형인 모멘텀 최적화(Momentum Optimization)에서 사용되는 하이퍼파라미터\n",
        "    - 이전 그래디언트의 가중 평균을 사용하여 현재 그래디언트를 업데이트하는 방법\n",
        "    - 기본값 : 0\n",
        "    - 값이 0보다 크면 모멘텀 최적화가 적용됨.\n",
        "    - 일반적으로 0.9와 같은 값이 사용됨\n",
        "  "
      ],
      "metadata": {
        "id": "LZQv2l0eh7Ha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) 손실함수\n",
        "- 주로 모델이 예측한 값과 실제 타깃 사이의 차이를 측정하여 모델의 학습을 도와주는 역할을 함\n",
        "- 일반적인 손실 함수\n",
        "  - torch.nn.MSELoss : 평균 제곱 오차(MSE)를 계산하는 손실 함수\n",
        "    - 회귀 문제에 사용됨\n",
        "  - torch.nn.L1Loss : 평균 절대 오차(MAE)를 계산하는 손실 함수\n",
        "    - 회귀 문제에 사용됨\n",
        "  - torch.nn.CrossEntropyLoss: 교차 엔트로피 손실를 계산하는 손실 함수\n",
        "    - 분류 문제에 사용됨\n",
        "    - 일반적으로 다중 클래스 분류에서 사용되며, 모델의 출력값과 타깃 클래스 간의 차이 계산함\n",
        "  - torch.nn.BCELoss: 이진 교차 엔트로피 손실를 계산하는 손실 함수\n",
        "    - 이진 분류 문제에 사용됨\n",
        "    - torch.sigmoid 함수를 포함하여 활성화 함수를 적용하지 않은 로짓(logit) 값에 대해 교차 엔트로피 손실을 계산함 - 로짓 값이 아직 확률 값으로 변환되지 않은 상태에서 손실을 계산하므로, 안정적인 계산을 제공하고 수치적인 불안정성을 피할 수 있음\n",
        "  - torch.nn.BCEWithLogitsLoss: 이진 교차 엔트로피 손실을 계산하는 손실 함수\n",
        "    - 시그모이드 함수를 적용하지 않은 모델의 출력값과 이진 타깃 간의 차이를 계산함\n",
        "    - 시그모이드 함수를 적용하지 않기 때문에, 모델의 출력값을 로짓(logit) 형태로 입력받아 계산하는 특징이 있음\n",
        "  - torch.nn.NLLLoss: 음의 로그 우도 손실을 계산하는 손실 함수\n",
        "    - 분류 문제에 사용됨\n",
        "    - 로그 소프트맥스 출력을 사용하는 다중 클래스 분류에서 사용됨\n",
        "  - torch.nn.KLDivLoss: 쿨백-라이블러 발산을 계산하는 손실 함수\n",
        "    - 두 개의 확률 분포를 입력으로 받아, 첫 번째 확률 분포가 두 번째 확률 분포와 얼마나 다른지를 측정하는 KL 발산 값을 계산함\n",
        "    - 주로 생성 모델의 분포와 실제 데이터의 분포 간의 차이를 측정하는 등의 용도나 분포 간의 정규화를 위해 사용됨"
      ],
      "metadata": {
        "id": "7hNZxKeKnj5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 예측값과 타깃 데이터 생성\n",
        "predictions = torch.randn((16, 10)) # 예측값 (16, 10) 크기의 텐서\n",
        "targets = torch.randint(0, 10, (16,)) # 타깃값 (16,) 크기의 텐서\n",
        "\n",
        "# 평균 제곱 오차(MSE) 손실\n",
        "mse_loss = nn.MSELoss()\n",
        "loss_mse = mse_loss(predictions, targets)\n",
        "print(\"MSE Loss:\", loss_mse.item())\n",
        "\n",
        "# 평균 절대 오차(MAE) 손실\n",
        "mae_loss = nn.L1Loss()\n",
        "loss_mae = mae_loss(predictions, targets)\n",
        "print(\"MAE Loss:\", loss_mae.item())\n",
        "\n",
        "# 교차 엔트로피 손실\n",
        "cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "loss_cross_entropy = cross_entropy_loss(predictions, targets)\n",
        "print(\"Cross Entropy Loss:\", loss_cross_entropy.item())\n",
        "\n",
        "# 이진 교차 엔트로피 손실\n",
        "binary_cross_entropy_loss = nn.BCELoss()\n",
        "sigmoid = nn.Sigmoid()\n",
        "predictions_sigmoid = sigmoid(predictions) # 시그모이드 함수 적용\n",
        "loss_binary_cross_entropy = binary_cross_entropy_loss(predictions_sigmoid, targets.float())\n",
        "print(\"Binary Cross Entropy Loss:\", loss_binary_cross_entropy.item())\n",
        "\n",
        "# 이진 교차 엔트로피 손실 (시그모이드 함수 적용하지 않음)\n",
        "binary_cross_entropy_with_logits_loss = nn.BCEWithLogitsLoss()\n",
        "loss_binary_cross_entropy_with_logits = binary_cross_entropy_with_logits_loss(predictions, targets.float())\n",
        "print(\"Binary Cross Entropy with Logits Loss:\", loss_binary_cross_entropy_with_logits.item())\n",
        "\n",
        "# 음의 로그 우도(NLL) 손실\n",
        "nll_loss = nn.NLLLoss()\n",
        "log_softmax = nn.LogSoftmax(dim=1)\n",
        "predictions_log_softmax = log_softmax(predictions) # 로그 소프트맥스 함수 적용\n",
        "loss_nll = nll_loss(predictions_log_softmax, targets)\n",
        "print(\"NLL Loss:\", loss_nll.item())\n",
        "\n",
        "# 쿨백-라이블러 발산(KLD) 손실\n",
        "kld_loss = nn.KLDivLoss()\n",
        "softmax = nn.Softmax(dim=1)\n",
        "targets_softmax = softmax(targets.unsqueeze(1).float()) # 타깃값을 확률 분포로 변환\n",
        "predictions_softmax = softmax(predictions) # 예측값을 확률 분포로 변환\n",
        "loss_kld = kld_loss(torch.log(predictions_softmax), targets_softmax)\n",
        "print(\"KLD Loss:\", loss_kld.item())"
      ],
      "metadata": {
        "id": "Xo3qyMsepCUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1) BCELoss\n",
        "- 이진 분류 문제에서 사용되는 이진 교차 엔트로피(Binary Cross Entropy) 손실 함수를 구현한 것\n",
        "- 예측값과 타겟값 사이의 교차 엔트로피를 계산하여 손실 값을 얻는데 사용됨\n",
        "- 인수와 기본값\n",
        "  - torch.nn.BCELoss(weight=None, reduction='mean', pos_weight=None)\n",
        "  - weight : 각 클래스에 대한 가중치를 지정하는 인수\n",
        "    - 기본값 : None\n",
        "    - 클래스 불균형이 있는 경우에 사용될 수 있음\n",
        "  - reduction : 손실 값을 어떻게 줄일지를 지정하는 인수\n",
        "    - 기본값 : 'mean'\n",
        "    - 옵션 : 'sum', 'none'\n",
        "  - pos_weight : 양성 클래스에 대한 가중치를 지정하는 인수\n",
        "    - 기본값 : None\n",
        "    - 양성 클래스의 중요도를 조절할 수 있음\n",
        "  \n",
        "### (2) CrossEntropyLoss\n",
        "- 다중 클래스 분류 문제에서 주로 사용됨\n",
        "- 모델의 예측값과 실제 레이블 간의 차이를 계산하여 모델을 학습시키는데 사용됨\n",
        "- 소프트맥스 함수를 내장하고 있어 모델의 출력값에 소프트맥스를 적용할 필요 없이 모델의 출력값과 레이블을 입력으로 받아 손실을 계산함\n",
        "- 인수\n",
        "  - torch.nn.CrossEntropyLoss(weight=None, ignore_index=-100, reduction='mean')\n",
        "  - weight : 클래스별 가중치를 적용할 경우 사용하는 가중치 텐서\n",
        "    - 기본값 : None\n",
        "  - ignore_index : 무시할 클래스의 인덱스\n",
        "    - 기본값 : -100\n",
        "  - reduction : 손실을 감소할 방법을 지정\n",
        "    - 선택 옵션 : 'mean'(평균), 'sum'(합), 'none'(감소하지 않음)\n",
        "    - 기본값 : 'mean'\n",
        "  \n",
        "### (3) MSELoss\n",
        "- 평균 제곱 오차(Mean Squared Error, MSE) 손실 함수를 구현한 것\n",
        "- MSE\n",
        "  - 예측값과 실제 타겟값의 차이의 제곱을 평균한 것\n",
        "  - 회귀 문제에서 일반적으로 사용되는 손실 함수\n",
        "- 인수와 기본값\n",
        "  - torch.nn.MSELoss(reduction='mean')\n",
        "  - reduction: 손실 값을 어떻게 줄일지를 지정하는 인수\n",
        "    - 기본값 : 'mean'\n",
        "    - 옵션 : 'sum', 'none'\n",
        "  \n",
        "### (4) NLLLoss\n",
        "- Negative Log Likelihood(NLL) 손실 함수를 구현한 것\n",
        "- 주로 다중 클래스 분류 문제에서 사용되며 모델의 출력 확률 분포와 실제 타깃 레이블 사이의 차이를 측정하여 모델을 학습하는데 사용됨\n",
        "- 인수와 기본값\n",
        "  - torch.nn.NLLLoss(weight=None, ignore_index=-100, reduction='mean')\n",
        "  - weight: 각 클래스에 대한 가중치를 지정하는 인수\n",
        "    - 클래스 불균형을 처리할 때 사용될 수 있음\n",
        "    - 기본값 : None\n",
        "      - 모든 클래스에 대한 동일한 가중치를 사용함\n",
        "  - ignore_index: 손실 계산 시 무시할 레이블의 인덱스를 지정하는 인수\n",
        "    - 기본값 : -100\n",
        "    - ex) 패딩이 있는 시퀀스 데이터를 처리할 때, 패딩 토큰에 해당하는 레이블을 무시할 수 있음\n",
        "  - reduction: 손실 값을 어떻게 줄일지를 지정하는 인수\n",
        "    - 기본값 : 'mean'\n",
        "    - 옵션 : 'sum', 'none'\n",
        "- 모델의 출력 확률 분포와 실제 타깃 레이블 사이의 차이를 측정하여 모델을 학습하는데 사용됨\n",
        "- 모델의 출력은 로그 확률로 주어져야 함\n",
        "  - 타깃 레이블은 정수값으로 주어져야 함\n",
        "  - 모델이 출력하는 로그 확률 중 실제 타깃 레이블에 해당하는 값의 로그 확률을 사용하여 NLL 손실이 계산됨\n",
        "- NLL 손실은 실제 타깃 레이블에 해당하는 로그 확률의 음수값으로 정의\n",
        "  - 모델의 예측이 실제 타깃 레이블과 일치할수록 손실 값이 낮아지게 됨\n",
        "  - 모델의 예측이 정확하게 타깃 레이블을 예측하는데 중요한 역할을 함\n",
        "\n"
      ],
      "metadata": {
        "id": "_rNotUDPpISM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) model.parameters()\n",
        "- Pytorch 모델의 학습 가능한 파라미터들을 반환하는 메소드\n",
        "- 모델의 모든 학습 가능한 파라미터들을 Python의 generator 객체로 반환함\n",
        "  - 이 generator 객체를 optim.Optimizer 클래스의 생성자에 전달하여 해당 파라미터들이 최적화되도록 할 수 있음\n",
        "- model.parameters() 메소드를 호출하면 모델의 학습 가능한 파라미터들이 반환됨\n",
        "  - 이 파라미터들은 모델의 모든 레이어 및 모듈에서 정의된 파라미터들을 포함하며 모델이 학습 중에 업데이트되는 값들임\n",
        "  - 이러한 학습 가능한 파라미터들은 모델이 데이터로부터 학습을 통해 최적의 가중치와 편향값을 찾아가는 과정에서 업데이트되는 값들\n",
        "- model.parameters()를 사용하여 반환된 파라미터들은 최적화 알고리즘 (예: SGD, Adam 등)을 통해 업데이트되는 대상이 됨\n",
        "  - 학습 데이터와 손실 함수를 통해 계산된 그라디언트를 사용하여 업데이트됨\n",
        "  - 모델이 학습 데이터에 맞게 가중치와 편향값을 조정하며, 모델의 예측 성능을 향상시키는 것이 가능함\n",
        "\n",
        "## 4) model.train()\n",
        "- 모델의 학습 모드를 설정하는 메소드임\n",
        "- 모델을 학습하기 위해 호출되는 함수\n",
        "- 모델의 파라미터 업데이트 및 그래디언트 계산을 가능하게 해줌\n",
        "- 다음과 같은 동작들을 수행함\n",
        "  - 드롭아웃 및 배치 정규화와 같이 학습과정에서 사용되는 정규화 기법들이 동작하도록 함\n",
        "    - 모델이 데이터에 대해 일반화를 향상시키도록 도와줌\n",
        "  - 그래디언트 계산을 가능하게 함\n",
        "    - 학습 모드에서는 모델의 파라미터 업데이트를 위해 역전파를 수행하고 그래디언트를 계산함\n",
        "  - 모델의 파라미터를 업데이트할 수 있도록 함\n",
        "    - 학습 모드에서는 모델의 파라미터를 최적화 알고리즘에 따라 업데이트할 수 있음\n",
        "- 보통 모델을 학습하기 전에 호출되며 모델이 훈련 데이터에 대해 학습을 시작할 준비를 하는 데 사용됨\n",
        "\n",
        "\n",
        "- model.train() 함수를 호출하여 모델을 학습 모드로 설정 시\n",
        "  - 모델이 훈련 데이터에 대해 학습을 진행하고 그래디언트를 계산하여 파라미터를 업데이트할 수 있도록 준비됨\n",
        "- model.eval() 함수를 호출하여 모델을 평가 모드로 설정 시\n",
        "  - 모델이 테스트 데이터에 대해 추론을 수행하고 그래디언트를 계산하지 않도록 설정됨"
      ],
      "metadata": {
        "id": "JV0ZIgwnyq4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 정의\n",
        "class MyModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        # 모델의 레이어들을 정의\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 모델의 forward 연산을 정의\n",
        "        pass\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "model = MyModel()\n",
        "\n",
        "# 모델을 학습 모드로 설정\n",
        "model.train()\n",
        "\n",
        "# 모델을 사용하여 학습 데이터에 대해 forward 연산 및 역전파 수행\n",
        "# 그래디언트 계산 및 파라미터 업데이트 수행\n"
      ],
      "metadata": {
        "id": "BUyHQWb01DsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 모델 추론\n",
        "- 아래의 코드 설명\n",
        "  - MyModel : 가상의 모델 클래스\n",
        "    - 3개의 fully connected layer로 구성되어 있음\n",
        "  - 데이터 생성 및 데이터셋 분리 : 가상의 데이터를 생성하고 학습 데이터와 검증 데이터로 분리함\n",
        "  - 모델 초기화 : MyModel 클래스의 인스턴스를 생성하고 손실 함수와 최적화 알고리즘을 정의함\n",
        "  - Early stopping 관련 변수 초기화 : Early stopping을 위한 변수들을 초기화함\n",
        "    - 가장 낮은 검증 손실 값을 저장\n",
        "    - 개선이 없는 epoch의 수를 세기 위한 변수들이 있음\n",
        "  - ReduceLROnPlateau 관련 변수 초기화 : 학습률 조절을 위한 ReduceLROnPlateau 스케줄러를 초기화함\n",
        "  - 모델 학습 루프 : 지정한 epoch 수만큼 모델을 학습함\n",
        "    - 학습 데이터로 forward, backward, optimizer step을 수행하고 검증 데이터로 검증 손실을 계산함\n",
        "    - 검증 손실이 이전 최적 손실보다 낮으면 모델을 저장\n",
        "    - 개선이 없는 경우, Early stopping을 수행함\n",
        "    - 검증 손실을 이용하여 학습률을 조절함\n",
        "  - 저장된 모델 로딩 : Early stopping 시점에서 저장한 최적 모델을 로딩함\n",
        "  - 모델 추론 : 로딩한 모델을 이용하여 테스트 데이터에 대한 예측을 수행함"
      ],
      "metadata": {
        "id": "B40Z2PME1bK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#파이토치에서 Early Stopping, 모델 저장 및 로딩, ReduceLROnPlateau를 한 번에 구현한 예시\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 가상의 모델 클래스 정의\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "        self.fc3 = nn.Linear(10, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# 가상의 데이터 생성\n",
        "X = torch.randn(100, 10)\n",
        "y = torch.randint(0, 2, (100,))\n",
        "\n",
        "# 데이터셋 분리\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 모델 초기화\n",
        "model = MyModel()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Early stopping 관련 변수 초기화\n",
        "best_val_loss = float('inf')\n",
        "patience = 3\n",
        "num_epochs = 100\n",
        "no_improvement = 0\n",
        "\n",
        "# ReduceLROnPlateau 관련 변수 초기화\n",
        "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "# 모델 학습 루프\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"epoch {epoch}\")\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 모델 검증\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val)\n",
        "        val_loss = criterion(val_outputs, y_val)\n",
        "\n",
        "    # Early stopping 체크\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        no_improvement = 0\n",
        "        print('save best weight and bias')\n",
        "        torch.save(model.state_dict(), 'best_model.pth') # 모델 저장\n",
        "    else:\n",
        "        no_improvement += 1\n",
        "\n",
        "    # Early stopping\n",
        "    if no_improvement == patience:\n",
        "        print(f'Early stopping after {epoch+1} epochs.')\n",
        "        break\n",
        "\n",
        "    # Learning rate 조절\n",
        "    lr_scheduler.step(val_loss)\n",
        "\n",
        "# 저장된 모델 로딩\n",
        "best_model = MyModel()\n",
        "best_model.load_state_dict(torch.load('best_model.pth'))\n",
        "best_model.eval()\n",
        "\n",
        "# 모델 추론\n",
        "X_test = torch.randn(10, 10)\n",
        "with torch.no_grad():\n",
        "    test_outputs = best_model(X_test)\n",
        "    _, predicted = torch.max(test_outputs, 1)\n",
        "    predicted = predicted.numpy()\n",
        "    print(f'Predicted labels: {predicted}')"
      ],
      "metadata": {
        "id": "ANtgCbd27Yhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) model.eval()\n",
        "- Pytorch에서 모델을 평가 모드로 전환하는 메서드\n",
        "- 별도의 인수를 받지 않음\n",
        "- 단순히 model.eval()을 호출하면 모델이 평가 모드로 전환됨\n",
        "  - 모델이 평가 모드로 전환되면 드롭아웃이 비활성화되고 배치 정규화의 이동 평균과 이동 분산이 업데이트되지 않음\n",
        "- 주로 테스트 데이터나 검증 데이터를 사용하여 평가할 때 사용됨\n",
        "- 평가 모드에서는 모델이 추론 시에 동일한 동작을 수행하도록 설정되어 있어 모델의 성능 평가에 불필요한 노이즈를 줄이고 일관된 결과를 얻을 수 있음\n",
        "- 모델을 학습하는 동안 사용한 모델 객체를 추론할 때 model.eval()을 호출하여 추론 모드로 전환\n",
        "- 추론이 끝난 후에는 다시 model.train()을 호출하여 학습 모드로 전환하는 것이 일반적\n",
        "\n",
        "## 2) with torch.inference_mode():\n",
        "- Pytorch에서 제공하는 컨텍스트 매니저\n",
        "- 추론 과정에서 모델의 성능을 최적화하기 위해 사용되는 모드\n",
        "  - 추론 시에는 학습 시에 사용되는 일부 기능들을 비활성화하여 모델의 실행 속도와 효율성을 향상시킬 수 있음\n",
        "- torch.inference_mode()는 별도의 인수를 받지 않음\n",
        "  - with 문 안에 들어가는 것만으로 모델을 추론 모드로 전환함\n",
        "  - with 문을 빠져나가면 모델은 다시 기존의 모드(학습 모드 또는 평가 모드)로 돌아감\n",
        "  - 드롭아웃을 비활성화하고 배치 정규화의 이동 평균과 이동 분산을 업데이트하지 않는 등의 추론 관련 동작을 수행함\n",
        "    - 모델이 추론 시에는 학습 시와 다른 동작을 하도록 설정할 수 있음\n",
        "- 효과\n",
        "  - Autograd 비활성화: 추론 시에는 모델의 그래디언트 계산이 필요하지 않기 때문에, autograd(자동 미분) 엔진을 비활성화하여 연산 속도를 향상시킬 수 있음\n",
        "  - 메모리 최적화: 추론 시에는 중간 결과값을 유지할 필요가 없기 때문에, 중간 결과값을 메모리에 저장하지 않고 바로 해제하여 메모리 사용량을 줄일 수 있음\n",
        "  - 장치 복사 최적화: 추론 시에는 모델의 입력과 출력을 복사하는 불필요한 연산을 제거하여 추론 속도를 향상시킬 수 있음\n",
        "\n",
        "- PyTorch 1.10 이후에 추가된 상대적으로 최신의 기능\n",
        "  - 컨텍스트 매니저로 사용되어 with 문 내에서만 추론 모드를 유지\n",
        "  - 장점 : 더 세밀한 메모리 최적화와 장치 복사 최적화를 수행할 수 있음\n"
      ],
      "metadata": {
        "id": "J2j8hsNY8-Vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 모델 정의 및 가중치 로드\n",
        "model = MyModel()\n",
        "model.load_state_dict(torch.load('model_weights.pth'))\n",
        "\n",
        "# 추론 모드로 실행\n",
        "with torch.inference_mode():\n",
        "    # 추론 작업 수행\n",
        "    output = model(input_tensor)\n"
      ],
      "metadata": {
        "id": "g5aUSdZY-4Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) ReduceLROnPlateau\n",
        "- 파이토치(PyTorch)의 학습률 감소 기법 중 하나\n",
        "- 검증 손실이 더 이상 개선되지 않을 때 학습률을 동적으로 감소시켜 모델의 학습을 돕는 기법\n",
        "- 학습 중에 주기적으로 검증 데이터셋의 손실을 모니터링하고, 미리 정의된 조건에 따라 학습률을 감소시킴\n",
        "  - 학습률을 조절하여 모델이 더 빠르게 수렴하도록 도와주며, 학습 과정에서의 안정성과 성능을 향상시킬 수 있음\n",
        "- 인자\n",
        "  - torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=1e-4, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-8, verbose=False)\n",
        "  - optimizer: 옵티마이저(optimizer) 객체\n",
        "    - 필수 인수\n",
        "  - mode: 성능 개선을 어떻게 측정할지를 지정하는 문자\n",
        "    - 기본값 : 'min'\n",
        "      - 검증 손실값이 감소할 때 성능이 개선되었다고 판단\n",
        "    - 다른 옵션\n",
        "      - 'max' : 검증 정확도가 증가할 때 성능 개선으로 간주\n",
        "      - 'auto' :자동으로 모드를 선택\n",
        "  - factor: learning rate를 감소시킬 비율\n",
        "    - 기본값 : 0.1\n",
        "    - 검증 손실값이 개선되지 않을 때 현재 learning rate에 0.1을 곱하여 감소시킴\n",
        "  - patience: 검증 손실값이 개선되지 않은 상태를 얼마나 허용할 것인지를 설정하는 정수값\n",
        "    - 기본값 : 10\n",
        "      - 10번의 연속적인 epoch 동안 검증 손실값이 개선되지 않으면 learning rate를 감소시킴\n",
        "  - threshold: learning rate를 감소시키기 위한 기준 값\n",
        "    - threshold_mode에 따라 다르게 해석됨\n",
        "    - 기본값 : 1e-4\n",
        "    - 상대적인 변화율(rel)이 기준 값보다 작으면 learning rate를 감소시킴\n",
        "  - threshold_mode: threshold의 해석 방법을 지정하는 문자열\n",
        "    - 'rel' : 상대적인 변화율\n",
        "    - 'abs' : 절대적인 변화량\n",
        "    - 기본값 : 'rel'\n",
        "  - cooldown: learning rate를 감소시킴\n",
        "    - 새로운 learning rate를 적용하기 전에 몇 epoch 동안 학습을 일시 정지하는 횟수를 지정하는 정수값\n",
        "    - 기본값 : 0\n",
        "      - 일시 정지 없이 즉시 새로운 learning rate를 적용함\n",
        "  - min_lr: learning rate를 감소시킬 최소값을 지정하는 실수값\n",
        "    - 기본값 : 0\n",
        "    - learning rate가 이 값보다 작아지지 않도록 합니다.\n",
        "  - eps: learning rate를 감소시킬 때, 최소값으로 설정할 수 있는 작은 값\n",
        "    - 기본값 : 1e-8\n",
        "    - learning rate가 이 값보다 작아지지 않도록 함\n",
        "  - verbose: True로 설정하면 감소된 learning rate에 대한 정보를 출력\n",
        "    - 기본값 : False\n",
        "      - 출력되지 않도록 설정되어 있습니다."
      ],
      "metadata": {
        "id": "M-WO415p_dH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# 옵티마이저 정의\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# ReduceLROnPlateau 스케줄러 정의\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
        "\n",
        "# 학습 루프에서 검증 손실을 모니터링하고 스케줄러 업데이트\n",
        "for epoch in range(num_epochs):\n",
        "    train()  # 모델 학습\n",
        "    val_loss = validate()  # 검증 손실 계산\n",
        "    scheduler.step(val_loss)  # 검증 손실을 기반으로 학습률 조절\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# 최적화할 모델과 옵티마이저 정의\n",
        "model = ...\n",
        "optimizer = ...\n",
        "\n",
        "# ReduceLROnPlateau 스케줄러 초기화\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1, verbose=True)\n",
        "\n",
        "# 학습 루프\n",
        "for epoch in range(max_epoch):\n",
        "    # 학습 코드\n",
        "    # ...\n",
        "\n",
        "    # 검증 데이터셋을 사용하여 모델의 성능 평가\n",
        "    val_loss = ...\n",
        "\n",
        "    # ReduceLROnPlateau 스케줄러에 현재 검증 손실값 전달\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # 조기 종료 체크\n",
        "    if scheduler.num_bad_epochs > scheduler.patience:\n",
        "        print(f'Early stopping at epoch {epoch}...')\n",
        "        break"
      ],
      "metadata": {
        "id": "fDxl9hoRIRKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-1) num_bad_epochs\n",
        "- scheduler.num_bad_epochs : Pytorch에서 학습률 스케줄러를 사용할 때, 현재 학습률이 개선되지 않은(epoch의 손실이 향상되지 않은) 연속적인 epoch의 수를 나타내는 변수\n",
        "  - 해당 변수 : 주로 ReduceLROnPlateau 스케줄러와 함께 사용됨, 학습 중에 학습률을 동적으로 조절하는 데 사용됨\n",
        "\n",
        "- ReduceLROnPlateau 스케줄러\n",
        "  - 일정한 조건(ex : 검증 손실의 개선이 멈춘 경우)을 만족할 때, 학습률을 조정하는 스케줄러\n",
        "  - optimizer.step()을 호출한 후에 호출되어야 함\n",
        "  - 학습률을 업데이트하기 위해 현재의 손실 값을 기준으로 조건을 평가함\n",
        "- num_bad_epochs 변수\n",
        "  - 연속적으로 손실이 개선되지 않은(epoch의 손실이 향상되지 않은) epoch의 수를 나타냄\n",
        "  - ReduceLROnPlateau 스케줄러에서 정의된 patience 인수와 관련이 있음\n",
        "    - patience : 개선이 멈춘 후 추가적인 epoch 횟수를 지정함\n",
        "    - num_bad_epochs가 patience보다 크거나 같아지면 학습률이 조정됨\n"
      ],
      "metadata": {
        "id": "XWX_KuwTIVYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Optimizer 초기화 및 모델과 연결\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "# ReduceLROnPlateau 스케줄러 설정\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, verbose=True)\n",
        "\n",
        "# 학습 중간에 손실 값을 사용하여 스케줄러 업데이트\n",
        "for epoch in range(10):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss = evaluate(model, val_loader, criterion)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "# patience를 3으로 설정했으므로 만약 연속적으로 3개의 epoch 동안 검증 손실이 개선되지 않는다면\n",
        "# 스케줄러는 학습률을 조정할 것임\n",
        "# num_bad_epochs 변수는 이러한 조건을 평가하고 개선이 멈춘 경우에는 1씩 증가하게 됨\n",
        "# 스케줄러가 호출될 때마다 num_bad_epochs가 업데이트되며, 개선이 이루어지면 0으로 초기화\n",
        "# ReduceLROnPlateau 스케줄러는 일정한 조건을 만족하지 않는 연속적인 epoch들을 추적하고, 일정 횟수 이상의 연속적인 개선이 없을 경우 학습률을 조정하는 기능을 수행"
      ],
      "metadata": {
        "id": "CSSQQ7U5lK2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 단일 퍼셉트론 XOR"
      ],
      "metadata": {
        "id": "mm1JCUqV0an_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqSL5l52v2G5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# torch.cuda.is_available() : 파이토치에서 GPU를 사용할 수 있는지 볼 때 사용하는 코드\n",
        "# .to(device) : GPU를 사용하게 하는 코드\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777) # torch random 값 생성\n",
        "\n",
        "if device =='cuda':\n",
        "  torch.cuda.manual_seed_all(777) # 2개 이상의 gpu를 사용할 때\n",
        "  # torch.cuda.manual_sedd(random_seed) -> 현 gpu에서의 random값 생성\n",
        "\n",
        "X=torch.FloatTensor([[0,0],[0,1],[1,0],[1,1]]).to(device)\n",
        "Y=torch.FloatTensor([[0],[1],[1],[0]]).to(device)\n",
        "\n",
        "#layer\n",
        "linear=nn.Linear(2,1,bias=True)\n",
        "sigmoid=torch.nn.Sigmoid()\n",
        "\n",
        "#model\n",
        "model=nn.Sequential(\n",
        "    linear,\n",
        "    sigmoid\n",
        ").to(device)"
      ]
    }
  ]
}