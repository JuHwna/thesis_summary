{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#tensorflow 기초\n",
        "## 1) Sequential\n",
        "- Sequential()\n",
        "  - 간단한 순차적인 구조를 가진 모델을 쉽게 구성할 수 있음\n",
        "  - 입력층부터 출력층까지 차례대로 쌓아가는 것으로 모델을 구성함\n",
        "  - 사용법\n",
        "    - Sequential 객체를 생성\n",
        "    - add() 메서도를 사용하여 입력층과 은닉층, 출력층(layer)을 추가할 수 있음\n",
        "    - 제공하는 다양한 층들(Dense, Conv2D, Maxpooling2D, LSTM 등)을 사용하여 모델을 구성할 수 있음\n",
        "    - 이후, compile() 메서도를 사용하여 모델을 컴파일함\n",
        "  - 장단점\n",
        "    - 장점 : 간단한 순차적인 구조를 가진 모델을 쉽게 구성할 수 있어 초보자들이 쉽게 사용할 수 있음\n",
        "    - 단점\n",
        "      - 입력과 출력이 여러 개인 복잡한 모델을 구성하기는 어려움\n",
        "      - 다양한 네트워크 구조를 표현하기에는 한계가 있음\n",
        "        - 순차적인 구조이기 때문에 각 층이 한 번에 하나의 입력을 받고 하나의 출력을 생성\n",
        "- Functional API\n",
        "  - 다중 입력, 다중 출력 등의 복잡한 모델을 쉽게 구성할 수 있음\n",
        "  - 해당 api를 사용하여 모델을 구성할 때는 입력층과 출력층을 따로 정의하고 이를 연결하는 층들을 정의하여 모델을 구성함\n",
        "    - 입력층의 입력값 : 모델의 첫 번째 층을 정의할 때 사용\n",
        "    - 출력층의 출력값 : 모델을 컴파일할 때 사용\n",
        "  - 장점\n",
        "    - Sequential() 보다 유연하게 모델을 구성할 수 있음\n",
        "      - 다중 입력, 다중 출력 등의 복잡한 모델을 쉽게 구성할 수 있음\n",
        "      - 복잡한 모델 구성에 적합, 다양한 네트워크 구조를 구현하기에 용이"
      ],
      "metadata": {
        "id": "RtzXZJg9fbJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential"
      ],
      "metadata": {
        "id": "-k4fW7q5m3GX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8js4ockterYI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tensorflow 2 공식 문서에서의 예제"
      ],
      "metadata": {
        "id": "NI0HX24shTG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 번째 예제\n",
        "# 1. Sequential() 모델 생성\n",
        "model = tf.keras.Sequential()\n",
        "# 2. 입력층 및 은닉층 추가 -> input 사용 O, activation 함수 입력X\n",
        "model.add(tf.keras.Input(shape=(16,)))\n",
        "model.add(tf.keras.layers.Dense(8))\n",
        "\n",
        "# 두 번째 예제\n",
        "# tf.keras.Input 생략 가능\n",
        "# 이 경우 모델은 훈련/평가 메서드를 처음 호출할 때까지 가중치가 없음(아직 빌드가 되지 않았기 때문)\n",
        "# 1. Sequential() 모델 생성\n",
        "model = tf.keras.Sequential()\n",
        "# 2. 은닉층 추가 -> input 사용X, activation 함수 입력X\n",
        "model.add(tf.keras.layers.Dense(8))\n",
        "model.add(tf.keras.layers.Dense(4))\n",
        "\n",
        "# 세 번째 예제\n",
        "# 입력을 지정하면 레이어를 추가할 때 모델이 빌드됨. 레이어를 추가할 때 계속해서 모델이 빌드됨\n",
        "# 1. Sequential() 모델 생성\n",
        "model = tf.keras.Sequential()\n",
        "# 2. 입력층 및 은닉층 추가 -> input 사용 O, activation 함수 입력X\n",
        "model.add(tf.keras.Input(shape=(16,)))\n",
        "model.add(tf.keras.layers.Dense(4))\n",
        "len(model.weights)\n",
        "# Returns \"2\"\n",
        "\n",
        "# 네 번째 예제\n",
        "# the delayed-build pattern (입력 shape을 지정하지 않음)을 사용하는 경우,\n",
        "# `build(batch_input_shape)`을 호출하여 모델을 수동으로 빌드하도록 선택할 수 있음, you can\n",
        "# 1. Sequential() 모델 생성\n",
        "model = tf.keras.Sequential()\n",
        "# 2. 은닉층 추가\n",
        "model.add(tf.keras.layers.Dense(8))\n",
        "model.add(tf.keras.layers.Dense(4))\n",
        "# 3. 입력층 build\n",
        "model.build((None, 16))\n",
        "len(model.weights)\n",
        "# Returns \"4\"\n",
        "\n",
        "# 다섯 번째 예제\n",
        "# the delayed-build pattern (입력 shape을 지정하지 않음)을 사용하는 경우\n",
        "# fit, eval, predict를 처음 호출할 때 모델이 빌드됨\n",
        "# 또는 일부 입력 데이터에 대해 모델을 처음 호출할 때 빌드됨.\n",
        "# 1. Sequential() 모델 생성\n",
        "model = tf.keras.Sequential()\n",
        "# 2. 은닉층 추가\n",
        "model.add(tf.keras.layers.Dense(8))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "# 3. 모델 컴파일\n",
        "model.compile(optimizer='sgd', loss='mse')\n",
        "# 4. 모델 빌드 및 학습\n",
        "model.fit(x, y, batch_size=32, epochs=10)"
      ],
      "metadata": {
        "id": "4IsdzXqqhWIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 머신러닝 케라스 다루기 기초 버전"
      ],
      "metadata": {
        "id": "nFdkWLRrla_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 번째 버전 - sequential 생성 후, add를 통한 추가로 모델 생성\n",
        "#1. Sequential 모델 생성\n",
        "model = Sequential()\n",
        "# 2. 입력층과 은닉층 추가\n",
        "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
        "# Dense와 같은 일부 2D 레이어의 경우, 입력 형태를 input_dim 인자를 통해 지정할 수 있으며 일부 의 임시적인 3D 레이어는 input_dim과 input_length 인자를 지원합니다.\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "# 3. 출력층 추가\n",
        "model.add(Dense(units=10, activation='softmax'))\n",
        "# 4. 모델 컴파일\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 두 번째 버전 - sequential 생성 시 안에다가 층을 같이 입력\n",
        "model = Sequential([\n",
        "    Dense(units=64, activation='relu', input_dim=100),\n",
        "    Dense(units=32, activation='relu'),\n",
        "    Dense(units=10, activation='softmax')\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "YGASrm0DgkqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functional API"
      ],
      "metadata": {
        "id": "jnLEwf0Im7bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(Sequential대신에 Model과 Input를 import한다.)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense"
      ],
      "metadata": {
        "id": "7zS2OHOfm9Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 입력층 정의\n",
        "inp = Input(shape=(100,))\n",
        "# 2. 은닉층 정의\n",
        "x = Dense(units=64, activation='relu')(inp)\n",
        "x = Dense(units=32, activation='relu')(x)\n",
        "# 3. 출력층 정의\n",
        "out = Dense(units=10, activation='softmax')(x)\n",
        "# 4. 모델 구성\n",
        "model = Model(inputs=inp, outputs=out)\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "vEUiMyBjnK_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## input_shape\n",
        "- 모델의 첫 번째 레이어에만 지정해주며 이후의 레이어에서는 자동으로 입력 데이터의 형태를 추정함\n",
        "- 따라서, 모델의 첫 번째 레이어에서 input_shape을 지정할 때, 입력 데이터의 형태와 맞추어 튜플을 입력해주어야 함\n",
        "\n",
        "### 2차원 데이터\n",
        "- 2차원 데이터 : (num_samples, num_features)\n",
        "  - num_samples : 데이터의 샘플 수\n",
        "  - num_features : 각 샘플의 특성 수\n",
        "- 입력 형태 : input_shape=(num_features,)\n",
        "  - MNIST 숫자 이미지 분류\n",
        "    - 이미지가 28x28 크기의 흑백 이미지\n",
        "    - 입력 데이터 : (num_samples,28*28)\n",
        "      - 28*28 -> 이미지의 픽셀 수\n",
        "      - 각 픽셀의 값 : 이미지의 강도를 나타내는 값\n",
        "- input_shape 대신 input_dim을 사용할 수도 있음\n",
        "  - input_dim = num_features\n",
        "    - num_features : 데이터의 특성 개수를 나타내는 정수값\n",
        "- 입력\n",
        "  - 최소 2차원(행렬) 이상의 넘파이배열나 텐서로 만들어야 함\n",
        "  - 넘파이배열은 Dense 내부에서 텐서로 변환하여 사용됨\n",
        "\n",
        "### 3차원 데이터\n",
        "- 3차원 배열(num_samples, num_time_steps, num_features)\n",
        "  - input_shape=(num_time_steps,num_features)\n",
        "  - num_time_steps : 시계열 데이터의 시간 스텝 수\n",
        "\n",
        "### 4차원 데이터\n",
        "- 일반적으로 이미지 데이터의 경우 4차원 배열 형태로 입력됨\n",
        "- EX) CIFAR-10 데이터셋\n",
        "  - 32x32 크기의 컬러 이미지(RGB=3)로 구성되어 있음\n",
        "  - 입력 데이터 : (num_samples,32,32,3)\n",
        "    - num_samples : 데이터 샘플 수\n",
        "    - 32x32 크기의 이미지가 3개의 채널을 가짐\n",
        "    "
      ],
      "metadata": {
        "id": "8Xrsb4rZog3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2차원 데이터"
      ],
      "metadata": {
        "id": "yKzKN8wroJdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model=Sequential()\n",
        "#모델의 첫 번째 레이어에서 input_shape을 지정 - 2차원 데이터\n",
        "model.add(Dense(256,activation='relu',input_shape=(28*28,)))\n",
        "model.add(Dense(10,activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "lJd4x3TGoDdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3차원 데이터"
      ],
      "metadata": {
        "id": "H5ThvFLGoccH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "model=Sequential()\n",
        "#모델의 첫 번째 레이어에서 input_shape을 지정 - 3차원 데이터\n",
        "model.add(LSTM(256,input_shape=(390,3)))\n",
        "model.add(Dense(1))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ZwqjZe4uoeAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4차원 데이터"
      ],
      "metadata": {
        "id": "fYZoujqMowJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
        "\n",
        "model=Sequential()\n",
        "\n",
        "#모델의 첫 번째 레이어에서 input_shape을 지정 - 4차원 데이터\n",
        "model.add(Conv2D(32,(3,3),activate='relu',input_shape=(32,32,3)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10,activation='softmax'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "gQYFuuQ9ovsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf.keras.backend.clear_session()\n",
        "- backend 엔진과 함께 사용될 수 있음\n",
        "- backend,clear_session() 함수\n",
        "  - 현재 세션에 존재하는 모든 텐서 객체들을 삭제함\n",
        "  - 메모리 누수를 방지하고 모델을 여러 번 훈련할 때 이전에 사용되었던 가중치 등을 초기화하는데 사용됨\n",
        "- 일반적으로 모델을 훈련한 후에는 backend.clear_session() 함수를 호출하여 현재 세션을 클리어해주는 것이 좋음\n",
        "  - 이전에 사용했던 메모리를 반환하고 새로운 모델을 빌드하거나 훈련할 때 발생할 수 있는 문제를 방지할 수 있음\n"
      ],
      "metadata": {
        "id": "Ph28XYz_pbNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "hP73_wO5p8c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils.to_categorical\n",
        "- to_categorical 함수\n",
        "  - 정수형 클래스 레이블을 원-핫 인코딩 벡터로 변환하는 함수\n",
        "  - 단, 음수값을 ㅋ르래스 레이블로 가지고 있으면 오류가 남\n",
        "    - 함수를 사용하기 전에 클래스 레이블을 확인하고 음수 값이 없는지 확인 필요\n",
        "  - one-hot encoding\n",
        "    - 클래스 레이블을 벡터 형태로 표현하는 기법\n",
        "    - 각 클래스 레이블에 해당하는 차원만 1이고 나머지 차원은 0인 벡터를 만드는 것\n",
        "  - 기본적으로 num_classes=None 매개 변수를 가지고 있음\n",
        "    - 이 매개 변수는 명시적으로 지정하지 않는 한 자동으로 유츄됨\n",
        "    "
      ],
      "metadata": {
        "id": "V-UpU-NKqhI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "labels = [0, 1, 2, 1, 0]\n",
        "one_hot_labels = to_categorical(labels, num_classes=3)\n",
        "\n",
        "print(one_hot_labels)\n",
        "\"\"\"\n",
        "[[1. 0. 0.]\n",
        " [0. 1. 0.]\n",
        " [0. 0. 1.]\n",
        " [0. 1. 0.]\n",
        " [1. 0. 0.]]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bArOKFWjqDvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 02) Conv1D\n",
        "- tf.keras.layers.Conv1D\n",
        "  - 1차원 입력 데이터에 대해 합성곱(convolution) 연산을 수행하는 레이어임\n",
        "  - 주로 시계열 데이터나 자연어 처리 분야에서 사용됨\n",
        "- Conv1D\n",
        "  - 입력으로 3D 텐서를 받음\n",
        "  - 이 텐서는 (batch_size,sequence_length,input_dim)의 형태\n",
        "    - batch_size : 한 번에 처리할 데이터 샘플의 개수\n",
        "    - sequence_length(time_steps) : 입력 시퀀스(time)의 길이\n",
        "    - input_dim(feature) : 입력 시퀀스의 특성 수(ex: 단어 임베딩 벡터의 차원)\n",
        "  - 매개 변수\n",
        "    - filters : 필터의 수(즉, 출력 채널의 수)\n",
        "    - kernel_size : 필터의 크기\n",
        "    - strides ; 필터를 이동시키는 간격(기본값은 1)\n",
        "    - padding : 경계 처리 방법(기본값은 'valid)\n",
        "      - valid : 해당 없음\n",
        "      - same : 아웃풋이 원래 인풋과 동일한 길이를 갖도록 인풋을 패딩\n",
        "      - casual : 제로 패딩을 사용해 아웃풋이 원래의 인풋과 같은 길이를 갖도록 함\n",
        "    - activation : 활성화 함수(기본값은 None)\n",
        "    - use_bias : 바이어스(편향) 벡터를 사용할지 여부를 결정\n",
        "    - kernel_initializer : kernel 가중치 행렬을 초기화하는데 사용되는 방법\n",
        "      - zero : 모든 값이 0인 텐서를 생성하는 초기값 설정기\n",
        "      - ones : 모든 값이 1인 텐서를 생성하는 초기값 설정기\n",
        "      - constant : 모든 값이 특정 상수인 텐서를 생성하는 초기값 설정기\n",
        "      - random_uniform : 균등분포에 따라 텐서를 생성하는 초기값 설정기\n",
        "      - random_normal : 정규분포에 따라 텐서를 생성하는 초기값 설정기\n",
        "      - truncatednormal : 절단된 정규분포에 따라 텐서를 생성하는 초기값 설정기\n",
        "      - variancescaling : 가중치의 형태에 따라 규모를 조절할 수 있는 초기값 설정기\n",
        "      - orthogonal : 무작위 직교행렬을 생성하는 초기값 설정기\n",
        "      - identity : 단위행렬을 생성하는 초기값 설정기\n",
        "      - lecun_uniform : LeCun 균등분포 초기값 설정기\n",
        "      - glorot_normal : Glorot 정규분포 초기값 설정기, Xavier 정규분포 초기값 설정기\n",
        "      - glorot_uniform : Glorot 균등분포 초기값 설정기, Xavier 균등분포 초기값 설정기\n",
        "      - he_normal : He 정규분포 초기값 설정기\n",
        "      - lecun_normal : LeCun 정규분포 초기값 설정기\n",
        "      - he_uniform : He 균등분포 분산 스케일링 초기값 설정기\n",
        "    - bias_initializer : 바이어스 벡터를 초기화하는데 사용되는 방법\n",
        "    - kernel_regularizer : 커널 가중치 행렬에 적용되는 규제함수\n",
        "    - bias_ regularizer : 바이어스 벡터에 적용되는 규제 함수\n",
        "    - activity_regularizer : 레이어 출력에 적용되는 규제 함수\n",
        "    - kernel_constraint : 커널 가중치에 적용되는 제약 함수\n",
        "    - bias_constraint : 바이어스 벡터에 적용되는 제약 함수\n",
        "  - 출력 차원을 계산하는 방법\n",
        "    - filters : 필터의 개수, 즉 출력 공간의 차원 수\n",
        "    - kernel_size : 필터의 크기\n",
        "    - padding : 'same' or 'valid' or 'casual'\n",
        "    - strides : 필터의 이동 크기\n",
        "  - 출력의 크기 : (batch_size,output,filters)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "NqBWEcgVsnNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ex\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "x = np.random.randn(1000, 100, 1)\n",
        "y = tf.keras.layers.Conv1D(32, 3, activation='relu',input_shape=(100, 1))(x)\n",
        "print(y.shape)  #->(1000, 98, 32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3Fl7jJ8eEAY",
        "outputId": "5c52c51d-985c-4d8c-8a4b-5e60f5f243e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 98, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y2 = tf.keras.layers.Conv1D(32, 3, activation='relu')(y)\n",
        "print(y2.shape)#->(1000, 96, 32)"
      ],
      "metadata": {
        "id": "33yPG1JueU-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#padding = 'valid'인 경우:\n",
        "output = (time_steps - kernel_size + 1) / strides\n",
        "\n",
        "#padding = 'same'인 경우:\n",
        "output = ceil(time_steps / strides)\n",
        "#여기서 ceil 함수는 주어진 실수보다 크거나 같은 가장 작은 정수를 반환합니다."
      ],
      "metadata": {
        "id": "LuMXnj6xeWrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 03) MaxPooling1D\n",
        "- Convolution Neural Network(CNN)에서 사용되는 하나의 레이어 타입\n",
        "- 1D 데이터(시계열 데이터 또는 한 줄의 문자열과 같은 데이터)에서 최대 풀링(Max Pooling) 연산을 수행함\n",
        "- 코드 : tf.keras.layers.pooling.MaxPooling1D(=tf.keras.layers.MaxPool1D)\n",
        "  - 입력 데이터에서 일정한 간격으로 구역을 나누고 각 구역에서 가장 큰 값을 선택하여 출력 데이터를 생성함\n",
        "  - 입력 데이터 크기를 줄이고 필터링 된 데이터의 중요한 특징을 추출함\n",
        "\n",
        "- 매개변수\n",
        "  - tf.keras.layers.MaxPool1D(\n",
        "    pool_size=2,\n",
        "    strides=None,\n",
        "    padding='valid',\n",
        "    data_format='channels_last',\n",
        "    **kwargs)\n",
        "- MaxPool1D(pool_size)로 코딩하면\n",
        "  - strides와 padding 인자는 기본값으로 설정되며 pool_size 인자만 지정됨\n",
        "  - 출력 차원은 입력 시퀀스의 길이를 pool_size로 나눈 값이 됨\n",
        "  - ex)\n",
        "    - 입력 시퀀스의 크기가 (batch_size,timesteps,features)이고 MaxPool1D(2)로 코딩한 경우, 출력 차원은 (batch_size, timesteps/2,features)가 됨\n",
        "    - strides=2,padding='valid'로 설정한 것과 동일한 효과를 얻을 수 있음\n",
        "- 장점\n",
        "  - 입력 데이터를 작게 만들면서도 입력 데이터의 중요한 특징을 보존할 수 있어서 CNN에서 일반적으로 사용됨\n",
        "  - 모델의 파라미터 수를 줄이고, 과적합을 방지하면서, 모델의 정확성을 향상시키는데 도움을 줌\n",
        "- 일반적인 CNN의 구성 방법\n",
        "  - Conv1D레이어를 통해 특징을 추출한 다음, MaxPooling1D 레이어를 통해 중요한 특징을 추출하고 입력 데이터의 크기를 줄이는 것"
      ],
      "metadata": {
        "id": "bLD-bdHMej6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x = np.random.randn(10, 5, 3)#(batch_s, D_in, fliter_s)\n",
        "y = tf.keras.layers.MaxPooling1D(pool_size=2,\n",
        "   strides=1, padding='valid')(x)\n",
        "\n",
        "y.shape#TensorShape([10, 4, 3])\n",
        "#출력차원은 D_out만 바뀜(batch_s, D_out, fliter_s)\n",
        "#padding = 'valid' D_out =  (D_in - pool_size+1) / strides\n",
        "#padding = 'same' D_out =  D_in / strides"
      ],
      "metadata": {
        "id": "s6KpkMqXfIMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 04) Flatten\n",
        "- 입력 데이터를 1차원으로 평탄화(flatten)함\n",
        "- 2D 혹은 3D의 특징 맵(feature map)을 1D 벡터로 변환하여 이후의 레이어에서 처리하기 쉽게 만들어주는 역할을 함\n",
        "- 특징\n",
        "  - 입력 데이터의 형태를 변경하지 않고 1D 벡터로 변환함\n",
        "  - Flatten()레이어는 학습 가능한 가중치를 갖지 않음\n",
        "- 코드 : tf.keras.layers.Flatten"
      ],
      "metadata": {
        "id": "iVl2xglMgeNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv1D(32,3, input_shape=(100,1)))\n",
        "model.output_shape#(None, 98, 32)\n",
        "\n",
        "model.add(MaxPooling1D(2))\n",
        "model.output_shape#(None, 49, 32)\n",
        "\n",
        "model.add(Flatten())\n",
        "model.output_shape#(None, 1568) <- 49*32 = 1568"
      ],
      "metadata": {
        "id": "E-3RiR-Yg1A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 05) Dense\n",
        "- 딥러닝에서 가장 기본적인 뉴런 구성 요소 중 하나인 fully connected layer를 구현한 클래스\n",
        "- 입력 뉴런과 출력 뉴런이 모두 연결되어 있는 밀집(dense)한 구조를 가지고 있음\n",
        "- 계산 방법\n",
        "  - 입력 벡터와 가중치 행렬의 곱셈에 편향을 더하여 활성화 함수를 거친 출력 값을 계산\n",
        "  - 활성화 함수로는 ReLU, sigmoid, tanh 등이 사용됨\n",
        "- 사용되는 곳\n",
        "  - 다른 레이어와 결합하여 딥러닝 모델을 구성할 때 사용됨\n",
        "- activation 인자의 디폴트 값이 없어서 명시적으로 지정해주어야 함\n",
        "  - linear : 입력과 동일한 값을 출력함\n",
        "  - sigmoid : 시그모이드 함수를 사용하여 출력값을 0과 1 사이의 값으로 변환함\n",
        "    - 이진 분류 모델에서 출력층에 많이 사용됨\n",
        "  - tanh : 하이퍼볼릭 탄젠트 함수를 사용하여 출력값을 -1과 1 사이의 값으로 변환함\n",
        "    - 은닉층에 많이 사용됨\n",
        "  - relu : ReLU(Rectified Linear Unit) 함수를 사용하여 출력값을 0 이상의 값으로 변환함\n",
        "    - 은닉층에 많이 사용됨\n",
        "  - softmax : softmax 함수를 사용하여 출력값을 다중 클래스 분류에 적합한 확률 값으로 변환함\n",
        "    - 다중 클래스 분류 모델에서 출력층에 많이 사용됨\n",
        "  - selu : SELU(Scaled Exponential Linear Unit) 함수를 사용하여 출력값을 0과 1 사이로 스케일링함\n",
        "    - 자기 정규화(self-normalizing) 효과로 인해 딥러닝 모델의 성능을 향상시킬 수 있음\n",
        "  - elu : x>0인 경우 x, x<0인 경우 $\\alpha$*(exp(x)-1)로 출력됨\n",
        "    - 음수 값을 가지므로 활성화의 평균이 0에 가까워짐\n",
        "  - 기타 등등\n",
        "\n"
      ],
      "metadata": {
        "id": "dDIrXjkjg_Kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
        "model.output_shape#(None, 64)\n",
        "\n",
        "model.add(Dense(units=10, activation='softmax'))\n",
        "model.output_shape#(None, 10) -> 각 클래스에 대한 확률값을 계산함"
      ],
      "metadata": {
        "id": "L48WiHRN573j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 06) .compile()\n",
        "- 모델을 컴파일하는 함수\n",
        "- 모델을 학습시키기 전에 compile() 함수를 사용하여 모델의 손실 함수(loss), 최적화 방법(optimizer), 평가 지표(metrics)를 설정함\n",
        "- compile() 함수의 주요 인자\n",
        "  - optimizer\n",
        "  - loss\n",
        "  - metrics\n",
        "- 이외 옵션들\n",
        "  - loss_weights : 손실 함수에 대한 가중치를 설정함\n",
        "    - 다중 손실 함수 모델에서 사용됨\n",
        "  - weighted_metrics : 샘플 가중치를 고려한 평가 지표를 설정함\n",
        "  - sample_weight_mode : 샘플 가중치를 적용하는 방법을 설정함\n",
        "  - target_tensors : 모델의 출력 대상을 설정함"
      ],
      "metadata": {
        "id": "0NZOu-zJ6Qhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "_cyYXKm760aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### loss\n",
        "- 손실 함수를 설정함\n",
        "- 모델이 예측한 값과 실제 값 사이의 차이를 계산하는 함수 -> 이 차이가 작을수록 모델이 좋은 성능을 보이게 됨\n",
        "- 모델의 학습 방향을 결정하므로 목적과 데이터의 특성에 따라 적절한 손실 함수를 선택하는 것이 중요함\n",
        "- 종류\n",
        "  - mean_squared_error\n",
        "    - 회귀 문제에서 가장 일반적으로 사용되는 손실 함수\n",
        "    - 예측 값과 실제 값 사이의 평균 제곱 오차를 계산함\n",
        "  - mean_absolute_error\n",
        "    - 회귀 문제에서 사용되는 손실 함수\n",
        "    - 예측 값과 실제 값 사이의 평균 절대 오차를 계산함\n",
        "  - mean_absolute_percentage_error\n",
        "  - mean_squared_logarithmic_error\n",
        "  - squared_hinge\n",
        "  - binary_hinge\n",
        "    - 이진 분류 문제에서 사용되는 손실 함수\n",
        "    - margin-based loss function으로 svm에서 사용되는 hinge loss와 유사함\n",
        "  - categorical hinge\n",
        "    - 다중 클래스 분류 모델에서 사용되는 손실 함수\n",
        "    - margin-based loss function으로 svm에서 사용되는 hinge loss와 유사함\n",
        "  - logcosh\n",
        "  - categorical_crossentropy\n",
        "    - 타겟들은 범주형식을 따라야함 -> to_categorical을 사용해야 함\n",
        "    - 다중 클래스 분류 문제에서 사용되는 손실 함수\n",
        "    - 각 클래스의 확률 분포와 실제 값 간의 크로스 엔트로피를 계산하여 손실 값을 구함\n",
        "  - sparse_categorical_crossentropy\n",
        "    - 다중 클래스 분류 문제에서 사용되는 손실 함수\n",
        "    - 정답 레이블을 one-hot encoding으로 변환하지 않고 정수 값으로 사용하는 경우에 적합함\n",
        "  - binary_crossentropy\n",
        "    - 이진 분류 문제에서 사용되는 손실 함수\n",
        "    - 정답이 0 또는 1인 이진 분류 문제에서 사용됨\n",
        "  - kullback_leibler_divergence\n",
        "  - poisson\n",
        "  - cosine_proximity"
      ],
      "metadata": {
        "id": "kuXb_AwH7Cc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### optimizer\n",
        "- 최적화 방법을 설정함\n",
        "- 학습 데이터셋과 모델의 특성에 따라 최적의 결과를 도출할 수 있음\n",
        "- 종류\n",
        "  - SGD : 확률적 경사 하강법 옵티마이저\n",
        "    - 가장 기본적인 최적화 알고리즘\n",
        "    - 미분 가능한 함수에서 기울기를 계산하여 가중치를 업데이트함\n",
        "    - 일반적으로 학습률과 모멘텀 파라미터를 설정하여 사용함\n",
        "    - 인자\n",
        "      - lr : 학습률(0보다 크거나 같은 float 값)\n",
        "      - momentum : SGD를 적절한 방향으로 가속화하며 흔들림(진동)을 줄여주는 매개변수(0보다 크거나 같은 float값)\n",
        "      - decay : 업데이트마다 적용되는 학습률의 감소(0보다 크거나 같은 float값)\n",
        "      - nesterov : 네스트로프 모멘텀의 적용 여부를 설정(불리언)\n",
        "  - RMSprop\n",
        "    - 학습률을 제외한 모든 인자의 기본값을 사용하는 것이 권장됨\n",
        "    - 순환 신경망(RNN)의 옵티마이저로 많이 사용됨\n",
        "    - SGD의 단점 중 하나인 학습률을 설정하기 어려운 문제를 해결하기 위해 고안된 알고리즘\n",
        "    - 기울기 제곱의 이동평균 값을 구하여 학습률을 조절함\n",
        "    - 인자\n",
        "      - lr : 학습률(0보다 크거나 같은 float값)\n",
        "      - rho : 0보다 크거나 같은 float값\n",
        "      - epsilon : 0보다 크거나 같은 float형 fuzz factor\n",
        "      - decay : 업데이트마다 적용되는 학습률의 감소율(0보다 크거나 같은 float값)\n",
        "  - Adagrad\n",
        "    - 모델 파라미터별 학습률을 사용하는 옵티마이저\n",
        "      - 학습을 진행할수록 학습률을 낮추는 방식으로 가중치를 업데이트하는 알고리즘\n",
        "      - 자주 등장하는 피처들의 가중치는 작아지고 드물게 등장하는 피처들은 크게 업데이트 됨\n",
        "    - 파라미터의 값이 업데이트되는 빈도에 의해 학습률이 결정됨\n",
        "    - 파라미터가 더 자주 업데이트될수록 더 작은 학습률이 사용됨\n",
        "    - 모든 인자의 기본값을 사용하는 것이 권장됨\n",
        "    - 인자\n",
        "      - lr : 학습률(0보다 크거나 같은 float값)\n",
        "      - epsilon : 0보다 크거나 같은 float값\n",
        "      - decay : 업데이트마다 적용되는 학습률의 감소율(0보다 크거나 같은 float값)\n",
        "  - Adadelta\n",
        "    - Adagrad를 확장한 보다 견고한 옵티마이저\n",
        "      - Adagrad의 단점 중 하나인 학습률 감소에 대한 문제를 해결하기 위해 고안된 알고리즘\n",
        "    - 과거의 모든 그래디언트를 축적하는 대신, 그래디언트 업데이트의 이동창에 기반하여 학습률을 조절함\n",
        "      - 기울기 제곱과 업데이트의 이동평균 값을 계산하여 학습률을 조절함\n",
        "    - 많은 업데이트가 이뤄진 후 일지라도 학습을 계속할 수 있음\n",
        "    - 모든 인자의 기본값을 사용하는 것이 권장됨\n",
        "    - 인자\n",
        "      - lr : 초기 학습률\n",
        "        - 기본 값은 1(해당 값 권장)\n",
        "        - 0보다 크거나 같은 float값\n",
        "      - rho : 학습률 감소에 쓰이는 인자\n",
        "        - 각 시점에 유지되는 그래디언트의 비율에 해당함\n",
        "        - 0보다 크거나 같은 float값\n",
        "      - epsilon\n",
        "      - decay : 초기 학습률 감소율(0보다 크거나 같은 float값)\n",
        "  - Adam\n",
        "    - RMSprop과 모멘텀을 결합하여 만든 최적화 알고리즘\n",
        "    - 기울기 제곱과 모멘텀의 이동평균 값을 계산하여 학습률을 조절함\n",
        "    - 인자\n",
        "      - lr : 학습률(0보다 크거나 같은 float값)\n",
        "      - beta_1 : 일반적으로 1에 가깝게 설정됨\n",
        "        - 0보다 크고 1보다 작은 float값\n",
        "      - beta_2 : 일반적으로 1에 가깝게 설정됨\n",
        "        - 0보다 크고 1보다 작은 float값\n",
        "      - epsilon\n",
        "      - deacy : 업데이트마다 적용되는 학습률의 감소율(0보다 크거나 같은 float값)\n",
        "      - amsgrad : adam의 변형 AMSGrad의 적용 여부 설정(불리언)\n",
        "  - Adamax\n",
        "    - 무한 노름에 기반한 adam의 변형\n",
        "    - 인자\n",
        "      - lr : 학습률(0보다 크거나 같은 float값)\n",
        "      - beta_1 : 일반적으로 1에 가깝게 설정됨\n",
        "        - 0보다 크고 1보다 작은 float값\n",
        "      - beta_2 : 일반적으로 1에 가깝게 설정됨\n",
        "        - 0보다 크고 1보다 작은 float값\n",
        "      - epsilon\n",
        "      - deacy : 업데이트마다 적용되는 학습률의 감소율(0보다 크거나 같은 float값)\n",
        "  - Nadam\n",
        "    - 네스테로프 adam 옵티마이저\n",
        "      - adma 알고리즘에서 Nesterov momentum을 추가하여 만든 최적화 알고리즘임\n",
        "    - adam이 본질적으로는 모멘텀이 적용된 RMSprop인 것처럼 Nadam은 네스트로프 모멘텀이 적용된 RMSprop임\n",
        "    - 모든 인자의 기본값을 사용하는 것이 권장됨\n",
        "    - 인자\n",
        "      - lr : 학습률(0보다 크거나 같은 float값)\n",
        "      - beta_1 : 일반적으로 1에 가깝게 설정됨\n",
        "        - 0보다 크고 1보다 작은 float값\n",
        "      - beta_2 : 일반적으로 1에 가깝게 설정됨\n",
        "        - 0보다 크고 1보다 작은 float값\n",
        "      - epsilon\n",
        "  - Ftrl\n",
        "    - 대규모 데이터셋에서 사용하는 알고리즘\n",
        "    - L1/L2 정규화와 함께 사용함"
      ],
      "metadata": {
        "id": "Au8xrJ4X8evf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### metrics : 평자 지표를 설정함\n",
        "- 학습 중 모델의 성능을 평가할 때 사용됨\n",
        "- 종류\n",
        "  - accuracy\n",
        "    - 분류 문제에서 많이 사용되는 평가 지표\n",
        "\n",
        "  - binary_accuracy\n",
        "    - 이진 분류 문제에서 사용되는 평가 지표\n",
        "\n",
        "  - categorical_accuracy\n",
        "    - 다중 클래스 분류 문제에서 사용되는 평가 지표\n",
        "  - sparse_categorical_accuracy\n",
        "    - 정수 형태의 레이블을 사용하는 다중 클래스 분류 문제에서 사용되는 평가 지표\n",
        "  - top_k_categorical_accuracy\n",
        "    - 다중 클래스 분류 문제에서 사용되는 평가 지표\n",
        "    - 상위 K개의 클래스 중에 실제 클래스가 포함되어 있는 경우를 계산\n",
        "  - sparse_top_k_categorical_accuracy\n",
        "    - 정수 형태의 레이블을 사용하는 다중 클래스 분류 문제에서 사용되는 평가 지표\n",
        "    - 상위 K개의 클래스 중에 실제 클래스가 포함되어 있는 경우를 계산\n",
        "  - mean_squared_error\n",
        "    - 회귀 문제에서 사용되는 평가 지표\n",
        "  - mean_absolute_error\n",
        "    - 회귀 문제에서 사용되는 평가 지표\n",
        "  - mean_absolute_percentage_error\n",
        "    - 회귀 문제에서 사용되는 평가 지표\n",
        "  - mean_squared_logarithmic_error\n",
        "    - 회귀 문제에서 사용되는 평가 지표\n",
        "  - cosine_similarity\n",
        "    - 유사성 비교에서 사용되는 평가 지표\n",
        "    - 두 벡터 간의 코사인 유사도를 계산합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "pIRqRqfvBtLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 07) .fit()\n",
        "- 모델을 학습시키는 함수\n",
        "- 입력 데이터와 정답 데이터를 모델에 입력하여 모델의 가중치를 업데이트함\n",
        "- 각 에포크마다 손실과 평가지표를 계산함\n",
        "  - 검증용 데이터에 대한 손실과 평가지표도 계산함\n",
        "- fit() 함수가 반환하는 객체\n",
        "  - histroy 객체 : 학습 과정에서 발생한 정보들을 담음\n",
        "  - 학습 과정에서 발생한 손실 값과 평가지표 값의 변화를 그래프로 나타내거나 학습 과정에서 최적의 모델을 저장할 수 있음\n",
        "- 매개변수\n",
        "  - x : 모델의 입력 데이터를 나타내는 numpy 배열 또는 numpy 배열의 리스트\n",
        "  - y : 모델의 정답 데이터를 나타내는 numpy 배열 또는 numpy 배열의 리스트\n",
        "  - batch_size : 한 번에 처리되는 샘플의 수를 나타내는 정수 값\n",
        "    - 기본 값 : 32\n",
        "  - epochs : 모델이 학습할 총 횟수를 나타내는 정수 값\n",
        "    - 기본 값 : 1\n",
        "  - verbose : 학습 과정을 어떻게 출력할 것인지를 결정하는 값\n",
        "    - 0 : 출력이 없음\n",
        "    - 1 : 진행 막대가 표시됨(기본값)\n",
        "    - 2 : 에포크마다 한 줄씩 출력됨\n",
        "  - validation_data : 검증용 데이터를 나타내는 튜플\n",
        "    - 입력 형태 : (x_val, y_val)\n",
        "    - 기본값 : None\n",
        "  - callbacks : 훈련 중에 호출되는 콜백 함수의 리스트를 지정하는 인수\n",
        "    - 훈련 중간에 모델의 상태를 확인하거나 모델의 가중치를 저장하는 등의 역할을 수행할 수 있음\n",
        "  - shuffle : 샘플을 학습할 때마다 데이터를 무작위로 섞을지 여부를 결정하는 값\n",
        "    - 기본 값 : True\n",
        "- 이외의 매개 변수\n",
        "  - sample_weight: 샘플 가중치\n",
        "    - 각 샘플이 학습에서 차지하는 비중을 결정\n",
        "    - ex) 어떤 데이터셋에서 특정 클래스의 샘플을 더 중요하게 다루고 싶다면 해당 클래스의 샘플에 높은 가중치를 주어 학습에 더 큰 영향을 미치도록 할 수 있음\n",
        "    - 기본값: None\n",
        "  - initial_epoch: 학습을 시작할 에포크 번호\n",
        "    - 이 인자를 사용하면 이전에 학습한 모델을 불러와서 이어서 학습을 수행할 수 있음\n",
        "    - 기본값: 0\n",
        "  - validation_split: 검증 데이터셋의 비율을 나타내는 실수값\n",
        "    - ex) 전체 데이터셋 중 20%를 검증 데이터셋으로 사용하려면 validation_split 값을 0.2로 설정\n",
        "    - 기본값: 0.0\n",
        "  - validation_steps: 검증 데이터셋에서 몇 개의 배치를 사용할 것인지를 지정\n",
        "    - 예를 들어, 검증 데이터셋을 전체적으로 사용하는 것보다 일부 배치만 사용하는 것이 더 효율적일 경우 사용\n",
        "    - 이 값이 지정되면 validation_data에 지정된 데이터셋의 일부만 사용\n",
        "    - 기본값: None\n",
        "  - class_weight: 각 클래스에 대한 가중치를 지정\n",
        "    - 이 가중치는 클래스 불균형 문제를 해결하는 데 사용\n",
        "    - 기본값: None\n",
        "  - max_queue_size: 데이터를 전처리하거나 생성하는 데 사용되는 프로세스의 최대 큐 크기\n",
        "    - 기본값: 10\n",
        "  - workers: 데이터를 전처리하거나 생성하는 데 사용되는 프로세스의 수\n",
        "    - 기본값: 1\n",
        "  - use_multiprocessing: 멀티프로세싱을 사용하여 데이터를 전처리하거나 생성하는지 여부 결정\n",
        "    - 기본값: False\n",
        "  - steps_per_execution: GPU나 TPU를 사용하는 경우, 한 번 실행에 처리할 배치 수를 지정합니다.\n",
        "    - 기본값: 1\n",
        "\n",
        "## 반환하는 객체 histroy\n",
        "- model.fit() 메소드는 모델을 훈련시키는 동안 발생한 모든 정보를 담음 history 객체를 반환함\n",
        "- 딕셔너리 형태로 훈련 손실(loss)과 훈련 정확도(accuracy), 검증 손실(val_loss)과 검증 정확도(val_accuracy)의 값을 가지고 있음\n",
        "- history 형태(딕셔너리)\n",
        "  - {'loss':,[ㅇㅇ,ㅇㅇ,ㅇㅇ,....], 'accuracy':[ㅇㅇ,ㅇㅇ,ㅇㅇ,...], 'val_loss':[ㅇㅇ,ㅇㅇㅇ,ㅇㅇ,....],'val_accuracy':[ㅇㅇ,ㅇㅇ,ㅇㅇ,...]}\n",
        "\n",
        "- 모델의 학습 과정을 시각화하거나 훈련 및 검증 손실/정확도를 비교 분석할 수 있음"
      ],
      "metadata": {
        "id": "Tb57uiZOHME2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 모델 훈련\n",
        "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n",
        "\n",
        "# 훈련 손실 그래프\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# 훈련 정확도 그래프\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['train', 'val'], loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d1m39xX9Boh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 08-1).evaluation()\n",
        "- 학습된 모델을 사용하여 주어진 데이터셋에 대한 손실값과 지정된 평가 지표를 계산함\n",
        "- 인자\n",
        "  - x: 모델에 입력될 데이터를 나타내는 Numpy 배열 또는 배열의 리스트\n",
        "  - y: 모델의 출력과 비교할 데이터를 나타내는 Numpy 배열 또는 배열의 리스트\n",
        "  - batch_size : 한 번에 처리할 샘플의 개수\n",
        "  - verbose : 진행 상황을 출력할지 여부\n",
        "    - 0 : 출력x\n",
        "    - 1 : 진행 상황 바와 함께 출력\n",
        "    - 2 : 진행 상황 바 없이 출력함\n",
        "  - sample_weight : 샘플별 가중치를 나타내는 Numpy 배열 또는 배열의 리스트\n",
        "  - steps : 평가할 총 샘플 수\n",
        "    - None일 경우 x의 길이를 사용함\n",
        "- 반환 값의 형태 : 튜플\n",
        "  - 첫 번째 요소 : 손실값을 나타내는 실수\n",
        "  - 두 번째 요소 : 모델에 지정된 평가 지표에 대한 값을 나타내는 실수\n",
        "- ex) 테스트 데이터에 대한 손실값과 정확도를 계산할 때 사용"
      ],
      "metadata": {
        "id": "Wq4ImVPVKL22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "#결과값을 단순히 확인만 하려면\n",
        "model.evaluate(test_images, test_labels)\n",
        "313/313 [==============================] - 3s 10ms/step - loss: 0.0892 - accuracy: 0.9726\n",
        "[0.08917324244976044, 0.972599983215332]\n",
        "# 처리과정을 보여주는 verbose가 기본 1로 세팅되어 있어 위와 같은 결과를 내준다."
      ],
      "metadata": {
        "id": "ddQb-HPgL3Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 08-2).prediction()\n",
        "- 모델의 예측 결과를 반환함\n",
        "- ex) 분류 모델에서는 각 클래스에 대한 확률이 포함된 Numpy 배열이 반환됨\n",
        "- ex) 텍스트 생성 모델이라면 생성된 문장이나 문단을 반환할 수 있음\n",
        "- 인자\n",
        "  - x: 모델의 입력 데이터\n",
        "    - Numpy 배열 또는 배열의 리스트\n",
        "  - batch_size: 각 예측 단계에서 사용할 배치 크기\n",
        "    - 기본값 : 32\n",
        "  - verbose: 예측 과정을 출력할지 여부를 결정\n",
        "    - 0, 1 또는 2의 값 중 하나\n",
        "    - 기본값 : 1('auto')\n",
        "  - steps: 예측 단계 수\n",
        "    - x가 제공된 경우, steps는 x의 샘플 수를 batch_size로 나눈 값\n",
        "    - 기본값 : None\n",
        "  - callbacks: 예측 중에 호출될 콜백 함수의 리스트"
      ],
      "metadata": {
        "id": "TmqdJZtJL_Gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x, batch_size=32, verbose=1, steps=None, callbacks=None)"
      ],
      "metadata": {
        "id": "Y4u06veIM0nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 08-3) loss, accuracy 그래프\n",
        "- model.fit() : 학습을 수행하고 학습 과정에서 발생한 손실(loss)과 정확도(accuracy)를 기록한 객체를 반환함\n",
        "  - 이 객체를 history 변수에 저장하고 있음\n",
        "- history 객체\n",
        "  - 딕셔너리 형태\n",
        "  - 훈련 데이터와 검증 데이터에 대한 손실과 정확도를 기록하고 있음\n",
        "  - 키\n",
        "    - loss: 훈련 데이터에 대한 손실값\n",
        "    - val_loss: 검증 데이터에 대한 손실값\n",
        "    - accuracy: 훈련 데이터에 대한 정확도\n",
        "    - val_accuracy: 검증 데이터에 대한 정확도"
      ],
      "metadata": {
        "id": "jrz_fj0AM2de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2x1 subplot으로 나타내는 코드\n",
        "plt.clf()\n",
        "plt.figure(figsize=(8,3))\n",
        "\n",
        "# 훈련 손실 그래프\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "\n",
        "# 훈련 정확도 그래프\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['train', 'val'], loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MW6df_2_Nm-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하나의 plot에 담아보기\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 간단한 예시용 loss, accuracy, val_loss, val_accuracy 리스트 생성\n",
        "loss = [1.2, 0.8, 0.6, 0.5, 0.4, 0.3, 0.25, 0.2]\n",
        "accuracy = [0.4, 0.6, 0.7, 0.8, 0.85, 0.9, 0.92, 0.94]\n",
        "val_loss = [1.1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.45, 0.4]\n",
        "val_accuracy = [0.45, 0.55, 0.65, 0.75, 0.8, 0.86, 0.88, 0.9]\n",
        "\n",
        "# epoch 리스트 생성\n",
        "epoch_list = list(range(1, len(loss)+1))\n",
        "\n",
        "# plot 그리기\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "color = 'tab:red'\n",
        "# tab : 다음에 오는 문자열은 matplotlib에서 제공하는 컬러 맵 이름을 나타냄\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss', color=color)\n",
        "ax1.plot(epoch_list, loss, color=color, label='train')\n",
        "ax1.plot(epoch_list, val_loss, linestyle='--', color=color, label='val')\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "#.twinx() : 하나의 subplot에 두 개의 y축을 생성하는 데 사용됨 -> x축이 공유되지만 y축은 완전히 독립적임\n",
        "color = 'tab:blue'\n",
        "ax2.set_ylabel('Accuracy', color=color)\n",
        "ax2.plot(epoch_list, accuracy, color=color, label='train')\n",
        "ax2.plot(epoch_list, val_accuracy, linestyle='--', color=color, label='val')\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "# 그래프에 legend 추가\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "lines = lines1 + lines2\n",
        "labels = labels1 + labels2\n",
        "ax1.legend(lines, labels, loc='lower right')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZmnodLyWNrNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09) tf.keras.callbacks 모듈\n",
        "- 모델의 학습 과정 중에 호출되는 콜백 함수들을 포함하는 모듈\n",
        "- 콜백함수\n",
        "  - 모델 학습 도중에 지정된 시점에서 호출되어 추가적인 작업을 수행하거나 모델을 조작할 수 있음\n",
        "  - 모델 학습을 좀 더 세밀하게 제어하고 학습 결과를 모니터링하고 저장하는 등의 작업을 수행할 수 있음\n",
        "- 자주 사용되는 콜백 함수 종류\n",
        "  - ModelCheckpoint\n",
        "    - 지정된 간격으로 모델의 가중치를 저장함\n",
        "    - 저장된 가중치 중에서 최상의 결과를 보인 모델을 선택할 수 있음\n",
        "  - EarlyStopping\n",
        "    - 모델이 더 이상 개선되지 않을 때 학습을 자동으로 종료\n",
        "    - 오버피팅 방지\n",
        "  - ReduceLROnPlateau\n",
        "    - 검증 손실이 개선되지 않을 때 학습률 감소\n",
        "    - 학습이 더욱 안정적으로 진행될 수 있음\n",
        "  - TensorBoard\n",
        "    - 모델의 학습 과정을 시각화하기 위한 TensorBoard 로그를 생성\n",
        "  - CSVLogger: 모델의 학습 과정을 CSV 파일로 기록\n",
        "  - LearningRateScheduler: 학습률을 동적으로 조정\n",
        "\n"
      ],
      "metadata": {
        "id": "O9Sa0ehQOJhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09-1) ModelCheckpoint\n",
        "- 모델을 학습하는 동안 일정한 간격으로 모델의 가중치를 저장하고 최상의 성능을 보인 모델을 선택하는 기능을 제공함\n",
        "- 딥러닝 모델 학습의 안정성과 성능 향상을 위해 매우 유용함\n",
        "- 인자\n",
        "  - filepath\n",
        "    - 문자열.\n",
        "    - 모델 가중치를 저장할 파일 경로\n",
        "    - 파일 이름에는 epoch와 val_loss (또는 다른 지표)와 같은 템플릿 인수를 사용할 수 있음\n",
        "  - monitor\n",
        "    - 모니터링할 지표 지정\n",
        "    - 기본값 : val_loss\n",
        "  - verbose\n",
        "    - 얼마나 자세하게 정보를 표시할지를 제어\n",
        "    - 0, 1 또는 2로 설정할 수 있으며, 기본값은 0임\n",
        "  - save_best_only\n",
        "    - 기본값 : False\n",
        "    - 가장 좋은 성능을 보인 가중치만 저장하려면 True로 설정\n",
        "  - save_weights_only\n",
        "    - 기본값 : False\n",
        "      - 모델 구조와 가중치를 모두 저장함\n",
        "    - 가중치만 저장하려면 True로 설정\n",
        "  - mode\n",
        "    - 모니터링할 지표의 최소화 또는 최대화를 제어합니다.\n",
        "    - 'auto', 'min' 또는 'max' 중 하나로 설정\n",
        "      - 기본값 : 'auto'\n",
        "  - save_freq : 가중치를 저장할 빈도를 지정\n",
        "    - 'epoch' 또는 정수를 입력할 수 있음\n",
        "      - epoch : 각 에포크가 끝날 때마다 가중치를 저장\n",
        "      - 정수 : 해당 빈도로 가중치를 저장\n",
        "    - 기본값 : 'epoch'"
      ],
      "metadata": {
        "id": "7wXqJQ5yRDWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# ModelCheckpoint 객체를 생성합니다.\n",
        "checkpoint = ModelCheckpoint(\"weights.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n",
        "# ->모델을 학습할 때 modelcheckpoint 객체를 콜백으로 전달하여 학습 도중에 가중치를 저장함\n",
        "# -> 검증 세트의 정확도가 최고일 때만 가중치를 저장하도록 설정했음\n",
        "\n",
        "# 모델을 학습합니다. ModelCheckpoint 객체를 콜백으로 전달합니다.\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=128, callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "N92-sYZpQvGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 재학습할 때는 이전에 저장된 가중치를 불러와서 학습을 시작하면 됨"
      ],
      "metadata": {
        "id": "ajSWKUfOPXFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이전에 저장된 모델 가중치를 불러옵니다.\n",
        "model.load_weights(\"weights.h5\")\n",
        "\n",
        "# 이어서 모델을 학습합니다.\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=128, callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "kxerFRDHPk7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- save() 함수\n",
        "  - 전체 모델(아키텍처와 가중치)을 하나의 HDF5 파일에 저장\n",
        "  - 모델의 아키텍처와 가중치 모두를 지정된 파일에 저장함\n",
        "- .h5와 .hdf5\n",
        "  - Hierarchical Data Format(HDF)의 파일 확장자\n",
        "  - 두 확장자 모두 같은 파일 형식을 가지고 있음\n",
        "- HDF5\n",
        "  - 고성능 컴퓨팅 분야에서 데이터 저장 및 교환에 사용되는 파일 형식 중 하나\n",
        "  - 대규모 데이터 세트를 처리하는 데 매우 효율적임"
      ],
      "metadata": {
        "id": "VjFh023aP2fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련 후 HDF5 파일에 저장하기\n",
        "model.save('model.h5')\n",
        "\n",
        "# 저장된 모델 로드하기\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('model.h5')\n",
        "#-> 모델을 로드할 때도 load_model() 함수를 사용하여 간단하게 로드할 수 있음"
      ],
      "metadata": {
        "id": "lz_tuogkQQTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09-1-1) 모델 저장 및 로드\n",
        "- 전체 모델(모델 아키텍처와 가중치 모두)을 하나의 hdf5 파일에 저장하는 것\n",
        "  - 장점 : 모델 저장 및 재사용의 편리함과 성능 측면에서 유리\n",
        "  - 단점 : 디스크 공간이 제한되어 있거나 모델이 매우 큰 경우에는 단점이 될 수 있음\n",
        "- 모델 아키텍처와 가중치를 별도의 파일로 저장함\n",
        "  - ModelCheckpoint 콜백 함수의 save_weights_only=True 옵션을 사용하여 가중치만 저장하고, 모델 아키텍처는 별도의 JSON 또는 YAML 파일로 저장할 수 있음\n",
        "  - 이렇게 하면 디스크 공간을 절약할 수 있고 모델 아키텍처와 가중치를 따로 저장하여 로드할 수 있음\n",
        "\n",
        "- JSON(JavaScript Object Notation)\n",
        "  - 경량의 데이터 교환 형식\n",
        "  - 인간이 읽고 쓰기 쉽고 기계가 파싱하고 생성하기도 쉬움\n",
        "  - JavaScript에서 객체를 나타내는 방법에서 파생되었지만 다른 프로그래밍 언어에서도 사용할 수 있음\n",
        "    - 모델을 불러오는 코드가 간결해지며 이식성이 높아짐\n",
        "  - 모델 아키텍처를 JSON 파일로 저장하면 파이썬 뿐만 아니라 다른 프로그래밍 언어엥서도 모델 아키텍처를 쉽게 로드할 수 있음\n",
        "  - 텍스트 형식이므로 디버깅이 용이하며 버전 관리 시스템(Git 등)을 통해 변경 사항을 추적하기 용이함\n",
        "  - 장점 : 모델의 이식성과 유연성을 높이는데 도움이 됨\n"
      ],
      "metadata": {
        "id": "9RCTXWVAR6f1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장하기\n",
        "# 모델 아키텍처를 JSON 파일로 저장\n",
        "m_j = model.to_json()\n",
        "with open(\"model.json\", \"w\") as f:\n",
        "    f.write(m_j)\n",
        "\n",
        "# 모델 가중치를 HDF5 파일로 저장\n",
        "model.save_weights(\"weights.h5\")"
      ],
      "metadata": {
        "id": "Wv8AbX9-UNRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 불러오기\n",
        "from keras.models import model_from_json\n",
        "# 모델 아키텍처 로드\n",
        "with open(\"model.json\", \"r\") as f:\n",
        "    m_j = f.read()\n",
        "loaded_model = model_from_json(m_j)\n",
        "\n",
        "# 모델 가중치 로드\n",
        "loaded_model.load_weights(\"weights.h5\")"
      ],
      "metadata": {
        "id": "85d26IaRUVfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09-2) EarlyStopping\n",
        "- 검증 손실이 더 이상 개선되지 않을 때 학습을 중단시키는 역할을 함\n",
        "- 과적합을 방지하고 학습 시간을 단축할 수 있음\n",
        "- 매개변수\n",
        "  - monitor: 모니터링할 지표를 선택\n",
        "    - 일반적으로 'val_loss'나 'val_acc'를 선택\n",
        "  - min_delta: 개선되었다고 판단할 최소한의 변화량\n",
        "    - 기본값 : 0\n",
        "    - 이 값보다 작으면 개선이 없는 것으로 간주\n",
        "  - patience: 개선이 없다고 판단하기 전에 대기할 에폭 수\n",
        "    - 기본값 : 0\n",
        "    - 이 값만큼 검증 손실이 개선되지 않으면 학습을 중단\n",
        "  - mode: 모니터링할 지표를 최소화할지 최대화할지 결정\n",
        "    - 'min'으로 설정하면 지표를 최소화하고, 'max'로 설정하면 지표를 최대화함.\n",
        "    - 기본값은 'auto'로 지표가 'acc', 'accuracy', 'fmeasure', 'precision', 'recall' 중 하나이면 'max'로 설정하고, 그렇지 않으면 'min'으로 설정\n",
        "  - baseline: 모니터링할 지표의 기준값을 설정\n",
        "    - 이 값보다 개선되지 않으면 학습을 중단함\n",
        "  - restore_best_weights: 최상의 가중치를 복원할지 여부를 결정함\n",
        "    - 기본값 : False\n",
        "    - 이 값을 True로 설정하면 학습 중 최상의 검증 손실을 가진 가중치를 복원함.\n",
        "- 모델 학습 시 fit() 함수의 callbacks 매개변수를 이용하여 사용함\n",
        "- 장점\n",
        "  - 과적합을 방지하고 학습 시간을 단축하는 데 효과적인 콜백 함수 중 하나임\n",
        "- 단점\n",
        "  - 지나치게 일찍 중단되어 최적의 모델을 찾지 못할 가능성이 있음\n",
        "  - 해결방법\n",
        "    - patience 값을 적절하게 조정하여 사용하는 것이 중요함\n",
        "    - restore_best_weights 값을 True로 설정하여 최상의 가중치를 복원하는 것이 좋음\n",
        "\n",
        "- 매개변수 restore_best_weight\n",
        "  - 기본값 False인 이유\n",
        "    - 최적화하는 데 추가 계산 비용이 들기 때문임\n",
        "  - 검증 손실이 일정 시간 동안 개선되지 않으면 학습을 중단시키기 때문에모델 가중치가 항상 최적화되어 있는 것은 아님\n",
        "  - True 설정하면 학습을 마친 후 별도의 검증 단계에서 최적의 모델을 찾는 과정이 추가로 필요함\n",
        "    - 추가적인 계산 비용이 들게 됨\n",
        "  - ModelCheckpoint에서 따로 모델이나 가중치를 저장하지 않는다면 반드시 restore_bset_weights를 True로 설정해야 함\n",
        "    - model.prediction()이나 model.evaluation()할 때 최적의 모델 가중치를 사용할 수 있음\n",
        "    - ex) KFold나 StratifiedKFold로 교차 검증하는 for 루프 안에서 가중치를 따로 저장하고 불러오지 않아도 가장 좋은 지표의 가중치로 prediction()과 evaluation()을 할 수 있음"
      ],
      "metadata": {
        "id": "1FH2aDwaVYif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3) #-> 3 에폭 동안 검증 손실이 개선되지 않으면 학습을 중단시킴\n",
        "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=32, callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "gtRWp4_0YDZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09-3) 학습률 조정 함수\n",
        "- 종류\n",
        "  - LearningRateScheduler 함수\n",
        "    - 사용자가 지정한 함수에 따라 epoch마다 학습률을 업데이트함\n",
        "    - 사용자가 직접 학습률 감소 전략을 지정해줘야 함\n",
        "    - 학습률 감소 기준도 직접 정해줘야 함\n",
        "  - ReduceLROnPlateau\n",
        "    - 학습률을 자동으로 감소시킴\n",
        "    - 모니터링 중인 지표에 대한 지정된 조건에 따라 학습률을 동적으로 감소시킴\n",
        "    - ex) validation loss가 5 epoch 동안 감소하지 않을 경우 학습률을 감소시키는 방식으로 학습률을 조정함"
      ],
      "metadata": {
        "id": "NUofnvjoaiWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09-3-1) LearningRateScheduler\n",
        "- 학습률을 동적으로 조정할 수 있도록 해줌\n",
        "- 사용자가 지정한 함수에 따라 학습률이 epoch마다 업데이트됨"
      ],
      "metadata": {
        "id": "mYIpbMsAbzpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# 사용자가 지정한 함수\n",
        "def lr_scheduler(epoch):\n",
        "    # 10 epoch 마다 학습률 감소\n",
        "    if epoch % 10 == 0:\n",
        "        return 0.1 * (0.5 ** (epoch//10))\n",
        "    else:\n",
        "        return learning_rate\n",
        "\n",
        "lr_scheduler_callback = LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "model.fit(X_train, y_train, epochs=100, callbacks=[lr_scheduler_callback])\n"
      ],
      "metadata": {
        "id": "tUBmxdWSbynZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09-3-2) ReduceLROnPlateau\n",
        "- 검증 손실이 개선되지 않을 때 학습률을 동적으로 조정하여 학습을 최적화하는 콜백 함수\n",
        "- 검증 손실이 개선되지 않으면 학습률을 줄여 더 작은 학습률로 다시 학습을 시도함\n",
        "- 장점\n",
        "  - 모델 학습이 더욱 안정적으로 이루어지고 수렴에 도움을 줌\n",
        "  - 학습률이 너무 크면 발산하고 학습률이 너무 작으면 수렴 속도가 느려지는 문제를 피할 수 있음\n",
        "- 매개변수\n",
        "  - monitor: 모니터링할 지표\n",
        "    - 기본값 : 'val_loss'\n",
        "  - factor: 학습률을 얼마나 감소시킬 것인지를 결정하는 인수\n",
        "    - 기본값 : 0.1\n",
        "    - ex) factor가 0.5이면, 검증 손실이 개선되지 않을 때 학습률이 절반으로 줄어듬\n",
        "  - patience: 검증 손실이 개선되지 않았을 때, 얼마나 많은 epoch를 기다릴 것인지를 결정하는 인수\n",
        "    - 기본값 : 10\n",
        "    - ex) patience가 5이면, 검증 손실이 5 epoch 동안 개선되지 않으면 학습률을 감소시킴\n",
        "  - mode: 모니터링 지표의 개선 여부를 어떻게 판단할 것인지를 결정하는 인수임\n",
        "    - 기본값 : 'auto'\n",
        "    - 'min', 'max', 또는 'auto' 중 하나의 값을 사용할 수 있음\n",
        "    - 'min' : 모니터링 지표가 감소할 때 개선되었다고 판단\n",
        "    - 'max' : 모니터링 지표가 증가할 때 개선되었다고 판단 - 'auto'인 경우 monitor 매개변수에 따라 적절한 모드가 자동으로 선택\n",
        "  - min_delta: 개선된 것으로 판단되기 위한 최소한의 변화량을 결정하는 인수\n",
        "    - 기본값 : 1e-4\n",
        "    - ex) min_delta가 0.5이면, 모니터링 지표가 0.5보다 적게 개선되면 개선된 것으로 판단하지 않음\n",
        "  - cooldown: 학습률을 감소시킨 이후 다시 학습을 재개하기까지 기다리는 epoch 수\n",
        "    - 기본값 : 0\n",
        "  - min_lr: 학습률의 하한을 결정하는 인수\n",
        "    - 기본값 : 1e-7\n",
        "    - 학습률이 min_lr보다 작아지면 학습이 중단됨\n"
      ],
      "metadata": {
        "id": "GC_1SlpJcprs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "#-> 5 epoch 동안 개선되지 않으면 학습률이 0.2로 줄어듬. 최소 학습률은 1e-6으로 설정되어 있음\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, callbacks=[reduce_lr])"
      ],
      "metadata": {
        "id": "_JzU4kBrebIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09-4) TensorBoard\n",
        "- Tensorflow에서 제공하는 시각화 도구 중 하나\n",
        "- 딥러닝 모델의 학습 과정과 결과를 모니터링하고 시각화하는데 사용됨\n",
        "- 제공하는 기능\n",
        "  - 학습 과정 모니터링\n",
        "    - 케라스 모델의 학습 과정을 모니터링\n",
        "    - ex) 손실(loss) 및 정확도(accuracy)와 같은 지표를 그래프로 시각화하여 학습 과정의 변화 확인\n",
        "  - 모델 구조 시각화\n",
        "    - 모델의 구조를 쉽게 이해\n",
        "  - 임베딩 시각화\n",
        "    - 임베딩(embedding) : 단어나 문장과 같은 텍스트 데이터를 수치화하는 기술\n",
        "    - 임베딩 공간을 시각화하여 단어 간 유사성을 시각적으로 확인\n",
        "  - 그래프 시각화: 모델의 그래프를 시각화하여 모델의 구조를 이해하고 디버깅할 수 있음\n",
        "- 웹 브라우저에서 http://localhost:6006에 접속하여 TensorBoard를 확인 가능"
      ],
      "metadata": {
        "id": "aFFNDje0e1zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "tensorboard_callback = TensorBoard(log_dir=\"./logs\")\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, callbacks=[tensorboard_callback])\n",
        "#tensorboard 실행\n",
        "tensorboard --logdir=./logs"
      ],
      "metadata": {
        "id": "lUownnENe6wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09-5) CSVLogger\n",
        "- 모델 훈련 시, 각 epoch의 손실 값과 측정값을 CSV 파일로 로깅함\n",
        "- 모델의 훈련 과정을 추적하고 분석할 수 있음\n",
        "- 인수\n",
        "  - filename: 로그를 기록할 파일 이름\n",
        "    - 문자열 형식으로 지정\n",
        "    - 기본값 : csv_logger.csv\n",
        "  - separator: CSV 파일의 필드 구분 기호\n",
        "    - 기본값 : 쉼표(',')\n",
        "  - append: 로그 파일을 새로 작성하는 대신 기존 파일에 추가할지 여부를 결정\n",
        "    - 기본값 : False"
      ],
      "metadata": {
        "id": "u3M-AsIYVTjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "\n",
        "csv_logger = CSVLogger('training.log') # 로그 파일명을 지정합니다.\n",
        "model.fit(x_train, y_train, epochs=10, callbacks=[csv_logger])\n",
        "#훈련이 끝나면 training.log는 훈련 결과와 관련된 정보를 가지게 됨"
      ],
      "metadata": {
        "id": "z2Nf5YgIW-bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 09-6) 모델 평가 분석\n",
        "- model.predict()로 얻은 모델의 예측값인 pred에서 argmax 함수를 사용하여 가장 높은 확률값을 갖는 클래스의 인덱스를 추출하는 과정\n",
        "  1. np.array(pred) : 리스트 pred를 NumPy 배열로 변환함\n",
        "  2. np.argmax() : NumPy 배열에서 가장 큰 값의 인덱스를 반환함\n",
        "    - axis=1을 설정하여 각 샘플(이미지)에 대해 가장 높은 확률값을 갖는 클래스의 인덱스를 추출함\n",
        "  3. 이렇게 추출한 인덱스를 y_pred 변수에 저장함\n",
        "\n",
        "- 분류 보고서\n",
        "  - classification_report() 함수\n",
        "    - 정밀도, 재현율, f1-score, 지원 데이터 수 등을 보여주는 분류 보고서를 출력\n",
        "  - 첫 번째 인자로 정답값 y_test, 두 번째 인자로 예측값 y_pred를 입력\n",
        "\n",
        "- Confustion 행렬\n",
        "  - confusion_matrix() 함수\n",
        "    - Confustion 행렬을 구함\n",
        "  - 첫 번째 인자로 정답값 y_test, 두 번째 인자로 예측값 y_pred를 입력\n",
        "  - sns.heatmap() 함수\n",
        "    - Confustion 행렬을 시각화\n",
        "    - annot=True : 각 셀에 숫자를 출력하도록 하는 옵션입니다.\n",
        "    - center=0 : 색상 막대의 중심 값을 0으로 지정하는 옵션\n",
        "    - cmap='coolwarm' : 색상 맵을 'coolwarm'으로 지정하는 옵션\n",
        "    - fmt='g' : 숫자 포맷을 일반 숫자로 지정하는 옵션\n",
        "    - cbar=True : 색상 막대를 보여주는 옵션입니다.\n",
        "  - plt.show() 함수\n",
        "    - 시각화된 Confustion 행렬을 출력"
      ],
      "metadata": {
        "id": "1vJk1G6aXRys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred = np.argmax(np.array(pred), axis=1)\n",
        "\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "plt.title(\"Confusion Matix\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, center=0, cmap='coolwarm', fmt='g', cbar=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PCoTQ-6zYkqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 추가 layers\n",
        "## 1) BatchNormalization\n",
        "- 딥러닝 모델에서 학습을 안정화하는데 사용되는 레이어\n",
        "- 일반적으로 합성곱 또는 fully connected layer(완전 연결층)와 함께 사용되며 입력 텐서의 각 배치에 대해 정규화를 수행함\n",
        "- 이점\n",
        "  - 그래디언트 소실 문제를 완화함\n",
        "  - 모델의 가중치 초기화에 덜 민감함\n",
        "  - 상대적으로 큰 학습률 사용으로 학습 속도가 빨라짐\n",
        "  - 미니 배치 단위로 학습이 수행되므로 미니 배치 단위로 데이터의 분포가 다를 경우 학습을 안정화할 수 있음\n",
        "  - 정규화를 통해 과적합 문제를 방지함\n",
        "- 수행 과정\n",
        "  - 입력 데이터에 대해 각 채널마다 평균과 표준 편차를 계산함\n",
        "  - 평균과 표준 편차를 사용하여 정규화를 수행하고 스케일링과 이동을 수행하여 결과를 출력함\n",
        "  - 모든 배치에 대해 수행됨\n",
        "- 매개 변수\n",
        "  - axis : 정규화할 축을 선택함\n",
        "    - 기본값 : -1\n",
        "  - momentum : 이동 평균의 가중치\n",
        "    - 기본값 : 0.99\n",
        "  - epsilon : 분모에 추가되는 작은 상수\n",
        "    - 기본값: 0.001\n",
        "  - center : 평균을 조정할지 여부 선택\n",
        "    - 기본값 : True\n",
        "  - scale : 분산을 조정할지 여부를 선택함\n",
        "    - 기본값 : True\n",
        "  - beta_initializer : beta 가중치의 초기값을 설정함\n",
        "    - 기본값 : 'zeros'\n",
        "  - gamma_initializer : gamma 가중치의 초기값을 설정함\n",
        "    - 기본값 : 'ones'\n",
        "  - moving_mean_initializer : 이동 평균의 초기값을 설정함\n",
        "    - 기본값 : 'zeros'\n",
        "  - moving_variance_initializer : 이동 분산의 초기값을 설정함\n",
        "    - 기본값 : 'ones'\n",
        "  - beta_regularizer : beta 가중치에 대한 정규화 함수를 설정함\n",
        "    - 기본값 : None\n",
        "  - gamma_regularizer : gamma 가중치에 대한 정규화 함수를 설정함\n",
        "    - 기본값 : None\n",
        "  - beta_constraint : beta 가중치에 대한 제약 조건을 설정함\n",
        "    - 기본값 : None\n",
        "  - gamma_constraint : gamma 가중치에 대한 제약 조건을 설정함\n",
        "    - 기본값 : None\n",
        "- 단점\n",
        "  - 미니 배치의 크기가 작을 경우 평균과 분산이 노이즈에 민감하게 반응할 수 있음\n",
        "    - 이러한 경우 모델 성능이 감소할 수 있음\n",
        "  - 이때문에 큰 batch size를 사용하는 것이 권장됨\n",
        "- 주의사항\n",
        "  - 모델 성능을 향상시키는 데 유용하지만 미니 배치의 크기와 모델의 다른 레이어와의 상호작용 등 몇 가지 제한 사항이 있음\n",
        "  - 제한 사항을 고려하여 적절한 경우, batchnormalization을 사용해야 함\n",
        "\n",
        "- 관련 내용\n",
        "  - 입력 데이터를 정규화하고 이에 대해 활성화 함수를 적용하기 전에 스케일링 및 이동되기 전에 활성화 함수의 입력으로 사용됨\n",
        "  - batchnormalization은 activation 레이어 이전에 사용됨\n",
        "  \n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "x_03ird7ZBdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 첫 번째 합성곱 레이어\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(28, 28, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# 두 번째 합성곱 레이어\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "# 완전히 연결된 레이어\n",
        "model.add(Dense(512))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))"
      ],
      "metadata": {
        "id": "ULL9VAy5O8p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Dropout\n",
        "- 신경망의 일반화 성능을 개선하기 위해 고안된 regularization 기법 중 하나임\n",
        "- 신경망이 학습할 때, dropout은 무작위로 일부 뉴런을 선택하여 그들의 출력을 0으로 만듦\n",
        "- 신경망의 일부 파라미터를 무작위로 삭제하여 과적합을 방지하고 모델의 일반화 능력을 향상시키는 데 도움이 됨\n"
      ],
      "metadata": {
        "id": "nZplBqoYXcFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 입력 데이터\n",
        "input_data = tf.keras.Input(shape=(100,))\n",
        "\n",
        "# Dropout 레이어 정의\n",
        "dropout = tf.keras.layers.Dropout(0.2) #-> 무작위로 선택된 뉴런 중 20%를 삭제한다는 의미\n",
        "\n",
        "# 레이어 적용\n",
        "output_data = dropout(input_data)\n",
        "\n",
        "print(output_data.shape) # 출력: (None, 100)"
      ],
      "metadata": {
        "id": "36-XZpyFbJtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) GlobalAveragePooling2D\n",
        "- 2D 입력 텐서의 공간 차원을 평균화하여 하나의 벡터로 변환함\n",
        "- 특성 맵의 공간 위치에 대한 정보를 유지하지 않고 모든 공간 위치를 동등하게 처리함\n",
        "- 주로 합성곱 신경망(CNN)의 출력 또는 다른 2D 특성 맵을 다룰 때 사용됨\n",
        "  - 일반적으로 CNN은 마지막에 GlobalAveragePooling2D레이어를 추가하여 출력을 벡터로 변환하고 이 벡터를 소프트맥스 레이어와 연결하여 클래스 분류를 수행함\n",
        "- 장점\n",
        "  - 모델의 파라미터 수를 크게 줄일 수 있음\n",
        "  - 과적합을 방지하고 모델의 일반화 성능을 향상시키는 데 도움이 됨\n",
        "  - 입력의 공간 위치를 무시하고 전체적인 패턴을 파악하는 데 유용함\n"
      ],
      "metadata": {
        "id": "jU-3YskmbPeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 데이터의 차원: (batch_size, height, width, channels)\n",
        "x = tf.random.normal((32, 224, 224, 3))\n",
        "\n",
        "# GlobalAveragePooling2D 계층 생성\n",
        "y = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "print(y.shape) # 출력 데이터의 차원: (batch_size, channels)(32, 3)"
      ],
      "metadata": {
        "id": "dlpzx4X8cwoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) GlobalMaxPool1D\n",
        "- 1차원의 입력 데이터에서 가장 큰 값을 선택하여 출력하는 레이어\n",
        "- 주로 시계열 데이터나 자연어 처리 분야에서 사용됨\n",
        "  - 시계열 데이터에서 특징 추출 시계열 데이터는 시간에 따라 변화하는 값들로 이루어져 있음\n",
        "  - 특정 시점의 값이 중요한 의미를 가지는 경우가 많음\n",
        "  - GlobalMaxPool1D는 모든 시점에서 가장 큰 값을 선택하여 해당 데이터에서 가장 중요한 특징을 추출할 수 있음\n",
        "- 장점\n",
        "  - 과적합 방지\n",
        "    - 입력 데이터에서 가장 중요한 값만 선택하기 때문에 레이어의 파라미터 수가 줄어듬\n",
        "    - 입력 데이터가 매우 큰 경우 유용함\n",
        "  - 연산 속도 개선\n",
        "    - 입력 데이터의 크기와 상관없이 항상 동일한 크기의 출력을 생성함\n",
        "    - 연산 속도를 개선하는데 도움이 됨\n",
        "    - MaxPooling 레이어보다 연산 속도가 빠름\n",
        "- 레이어의 출력 차원\n",
        "  - 입력 데이터의 마지막 차원에 따라 결정됨\n",
        "  - EX) 입력 데이터의 크기가 (batch_size, timesteps,features)인 경우, GlobalMaxPool1D 레이어의 출력 차원은 (batch_size,features)가 됨\n",
        "    - batch_size : 데이터의 배치 크기\n",
        "    - features : 입력 데이터에서 선택된 가장 큰 값들의 개수\n",
        "  - 레이어를 적용할 때 축소할 차원은 timesteps 차원\n",
        "  "
      ],
      "metadata": {
        "id": "-66hs7Oecyk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "x = np.random.randn(1000,100,4)\n",
        "max_pool_1d = tf.keras.layers.GlobalMaxPool1D()\n",
        "max_pool_1d(x).shape#TensorShape([1000, 4])\n",
        "\n",
        "batch_size = 32\n",
        "timesteps = 100\n",
        "features = 64\n",
        "\n",
        "# 입력 데이터\n",
        "input_data = tf.keras.Input(shape=(timesteps, features))\n",
        "\n",
        "# GlobalMaxPool1D 레이어 정의\n",
        "max_pool = tf.keras.layers.GlobalMaxPool1D()\n",
        "\n",
        "# 레이어 적용\n",
        "output_data = max_pool(input_data)\n",
        "\n",
        "print(output_data.shape) # 출력: (None, 64)"
      ],
      "metadata": {
        "id": "LDfH906cgVxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) GlobalMaxPool2D\n",
        "- 2D 입력 텐서의 공간 차원을 풀링하여 하나의 벡터로 변환함\n",
        "- 특성 맵의 공간 위치에 대한 정보를 유지하지 않고 모든 공간 위치에서 가장 큰 값을 선택함\n",
        "- 일반적으로 합성곱 신경망(CNN)의 출력 또는 다른 2D 특성 맵을 다룰 때 사용됨\n",
        "- CNN의 출력 특성 맵에서 가장 중요한 특징들을 선택하여 벡터로 변환하므로 입력에 대한 중요한 패턴을 파악하는 데 도움이 됨\n",
        "- 특징\n",
        "  - GlobalAveragePooling2D 레이어와 달리 입력의 공간 위치를 무시하고 가장 큰 값만 선택하여 변환함\n",
        "    - 입력의 강력한 특징을 더 잘 파악할 수 있음\n",
        "  - 모델의 파라미터 수를 크게 줄일 수 있음\n",
        "    - 과적합을 방지\n",
        "    - 모델의 일반화 성능을 향상시키는 데 도움이 됨\n",
        "  "
      ],
      "metadata": {
        "id": "eDO4e0Dkgame"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, GlobalMaxPool2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# 입력 데이터의 shape은 (batch_size, height, width, channels)\n",
        "inputs = Input(shape=(28, 28, 1))\n",
        "x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
        "x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "x = GlobalMaxPool2D()(x)  # GlobalMaxPool2D 적용\n",
        "#-> 28x28 크기의 입력에서 가장 큰 값만을 골라서 1D 벡터로 만듦\n",
        "# ->만들어진 1D 벡터는 출력 데이터에 연결되어 이미지 분류를 수행하게 됨\n",
        "# Conv2D 레이어 다음에 적용되어 입력 이미지에서 가장 중요한 특징만 추출하고 이를 출력 레이어로 전달함\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "0KAu4IVBhMG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 데이터의 차원: (batch_size, height, width, channels)\n",
        "x = tf.random.normal((32, 224, 224, 3))\n",
        "\n",
        "# GlobalAveragePooling2D 계층 생성\n",
        "y = tf.keras.layers.GlobalMaxPool2D()(x)\n",
        "print(y.shape) # 출력 데이터의 차원: (batch_size, channels)(32, 3)"
      ],
      "metadata": {
        "id": "FSi0y2ZPhz5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) GlobalMaxPool2D과 GlobalAveragePool2D 비교\n",
        "- 공통점\n",
        "  - 모두 2D 입력 텐서의 공간 차원을 풀링하여 하나의 벡터로 변환하는 레이어\n",
        "- 차이점\n",
        "\n",
        "||GlobalMaxPool2D|GlobalAveragePool2D|\n",
        "|--|-------------|-------------------|\n",
        "|선택점|입력의 공간 위치에서 가장 큰 값을 선택하여 변환함|입력의 공간 위치에서 평균 값을 선택하여 변환|\n",
        "|도움되는 곳|입력의 중요한 패턴을 파악하는 데 도움이 됨->입력의 강력한 특징을 파악하는 데 유용함|입력의 전체적인 패턴을 파악하는 데 도움이 됨|\n",
        "|벡터 변환|입력에 대한 중요한 특징들을 선택하여 벡터로 변환함|입력의 모든 공간 위치를 동등하게 처리하여 벡터로 변환함|\n",
        "|공간 위치 사용 여부|입력의 공간 위치를 무시하고 가장 큰 값만 선택하여 변환함|입력의 공간 위치를 모두 고려하여 평균 값을 선택하여 변환함|\n",
        "\n",
        "- 이 때문에 서로 다른 목적에 사용되며 주어진 문제에 적합한 레어이를 선택해야 함\n",
        "  - EX)\n",
        "    - GlobalMaxPool2D -> 이미지 분류 문제에 사용\n",
        "    - GlobalAveragePool2D -> 이미지 분할 문제에 사용"
      ],
      "metadata": {
        "id": "mpLDzvOpn697"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 제너레이터 사용\n",
        "## 1) .fit_generator()\n",
        "- 데이터 제너레이터(generator)로부터 데이터 배치(batch)를 생성하여 모델을 학습함\n",
        "- 일반적인 fit() 함수와 비슷한 방식으로 작동하지만 데이터가 매우 크거나 제한된 메모리를 사용하는 경우에 유용함\n",
        "- 매개변수\n",
        "  - generator : 학습 데이터를 생성하는 제너레이터 객체\n",
        "  - steps_per_epoch : 한 epoch에서 생성할 배치(batch) 개수\n",
        "  - epochs: 전체 학습할 epoch 수\n",
        "  - max_queue_size: 데이터 생성 시 큐(Queue)의 최대 크기를 지정\n",
        "  - shuffle: 각 에폭(epoch)마다 데이터를 임의로 섞을 것인지 여부를 지정합\n",
        "  - verbose: 학습 중 출력을 제어\n",
        "    - 0 : 출력하지 않음\n",
        "    - 1 : 진행바와 함께 출력\n",
        "    - 2 : 진행바 없이 출력\n",
        "  - callbacks: 콜백 함수를 지정\n",
        "    - 학습 과정 중에 특정 이벤트가 발생할 때 실행되는 함수 - ex) 모델 체크포인트를 저장하거나 학습률을 조정하는 등의 작업을 수행할 수 있음\n",
        "  - validation_data: 검증 데이터를 생성하는 제너레이터 객체 또는 넘파이 배열.\n",
        "  - validation_steps: 검증 데이터에서 생성할 배치(batch) 개수\n",
        "  - workers: 데이터 생성을 위해 사용할 프로세스 개수\n",
        "  - use_multiprocessing: True로 설정하면, 생성기 함수를 병렬 처리하기 위해 멀티 프로세싱(multiprocessing)을 사용할 수 있음\n",
        "    - 이 경우에는 workers 인자를 사용하여 몇 개의 프로세스를 사용할 것인지를 설정할 수 있음\n",
        "    - 일부 경우에는 데이터를 복사해야 하기 때문에, 병렬 처리 시간이 데이터 복사 시간에 의해 지연될 수 있음\n",
        "    - 모든 경우에서 멀티 프로세싱을 사용하는 것이 빠른 것은 아니므로, 해당 컴퓨터에서의 효율적인 처리 방법을 찾아야 함\n",
        "\n"
      ],
      "metadata": {
        "id": "0DdvMmTtpykV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_generator(train_generator,\n",
        "                    steps_per_epoch=100,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    callbacks=[checkpoint],\n",
        "                    validation_data=val_generator,\n",
        "                    validation_steps=50)\n",
        "# train_generator : 학습 데이터 배치를 생성하는 데이터 제너레이터 함수\n",
        "# val_generator : 검증 데이터 배치를 생성하는 데이터 제너레이터 함수\n",
        "# epochs : 전체 학습할 epoch 수 vs steps_per_epoch : 학습 epoch 당 생성할 배치 개수"
      ],
      "metadata": {
        "id": "9UDBUsX-pw4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) .predict_generator()\n",
        "- 제너레이터에서 생성된 데이터를 기반으로 모델의 예측값을 계산\n",
        "- 학습 데이터가 너무 커서 메모리에 한번에 로딩할 수 없는 경우에 사용됨\n",
        "  - 제너레이터를 통해 필요한 만큼 데이터를 생성하고 모델에 입력할 수 있음\n",
        "- 인자\n",
        "  - generator : 예측을 수행할 데이터를 생성하는 제너레이터 객체\n",
        "  - steps : 제너레이터로부터 몇 개의 배치를 만들어 낼 것인지 정수로 지정함\n",
        "  - max_queue_size : 데이터 생성 시 큐의 최대 크기를 지정함\n",
        "  - workers : 데이터 생성을 위해 사용할 프로세스 개수 지정\n",
        "  - use_multiprocessing : 멀티 프로세싱을 사용할 것인지 여부를 지정함\n",
        "- 지정한 generaotr에서 steps개의 배치를 생성하여 각 배치에 대한 예측값을 계산함\n",
        "  - 제너레이터가 반환하는 데이터의 개수가 배치 크기로 나누어 떨어지지 않는 경우, 마지막 배치는 자동으로 버려짐\n",
        "- 예측값으로 이루어진 넘파이 배열을 반환함"
      ],
      "metadata": {
        "id": "1YMHdmVnKmxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) utils.Sequence\n",
        "- Sequence 클래스는 모델 학습에 사용될 데이터셋을 생성하기 위한 유틸리티 클래스\n",
        "- 데이터셋을 미리 전처리하고 배치 단위로 데이터를 제공하며 데이터를 무작위로 섞어서 모델 학습을 도움\n",
        "- 사용하기 위해선 tf.keras.utils.Sequence 클래스를 상속받아서 새로운 클래스를 만들고 \\_\\_len\\_\\_ 메소드와 \\_\\_getitem\\_\\_ 메소드를 구현해야함\n",
        "  - \\_\\_len\\_\\_ 메소드 : 데이터셋의 전체 길이를 반환\n",
        "  - \\_\\_getitem\\_\\_ : 지정된 인덱스의 데이터 배치를 반환\n",
        "- 이점\n",
        "  - 모델 학습을 할 때 매번 전체 데이터셋을 메모리에 올릴 필요가 없음\n",
        "  - 대용량 데이터셋에서도 효율적으로 학습할 수 있음\n",
        "  - 데이터셋을 여러 스레드에서 병렬로 처리할 수 있음\n",
        "    - 데이터 전처리가 병목이 되는 경우에도 효율적으로 학습할 수 있음"
      ],
      "metadata": {
        "id": "CBkzv-x0LXmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.utils import Sequence\n",
        "\n",
        "class MyDataGenerator(Sequence):\n",
        "    def __init__(self, x_set, y_set, batch_size):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        return batch_x, batch_y\n",
        "x_train = np.random.rand(1000, 32, 32, 3)\n",
        "y_train = np.random.randint(0, 2, (1000, 1))\n",
        "\n",
        "batch_size = 32\n",
        "my_generator = MyDataGenerator(x_train, y_train, batch_size)\n",
        "\n",
        "for x_batch, y_batch in my_generator:\n",
        "    # 데이터 배치를 이용해서 모델 학습 수행\n",
        "    pass\n",
        "# MyDataGenerator 클래스를 사용해서 x_train과 y_train 데이터셋을 생성하고, batch_size 개수만큼의 데이터 배치를 가져와서 모델 학습을 수행\n",
        "# getitem 메소드 : idx 인덱스를 기준으로 batch_size 개수만큼의 데이터 배치를 생성해서 반환"
      ],
      "metadata": {
        "id": "POGNseSyLV17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 번외\n",
        "- ImageDataGenerator\n",
        "  - 실시간 데이터 증강을 사용해서 텐서 이미지 데이터 배치를 생성함\n",
        "  - 데이터에 대해 (배치 단위로) 루프가 순환됨\n",
        "  - 인수\n",
        "    - rescale: 크기 재조절 인수\n",
        "      - 기본 값 : None\n",
        "      - None 혹은 0인 경우 크기 재조절이 적용되지 않고 그 외의 경우 데이터를 주어진 값으로 곱함\n",
        "    - validation_split: 검증의 용도로 남겨둘 남겨둘 이미지의 비율\n",
        "    - rotation_range : 무작위 회전의 각도 범위\n",
        "    - 기타 등등\n",
        "  - 매개 변수\n",
        "    - apply_transform : 주어진 매개변수에 따라 이미지에 변형을 가함\n",
        "    - fit : 샘플 데이터에 데이터 생성기를 학습\n",
        "    - flow : 데이터와 라벨 배열을 받아 증강된 데이터의 배치를 생성\n",
        "    - flow_from_dataframe : dataframe과 디렉토리의 위치를 전달받아 증강/정규화된 데이터의 배치를 생성\n",
        "    - flow_from_directory : 디렉토리에의 경로를 전달받아 증강된 데이터의 배치를 생성"
      ],
      "metadata": {
        "id": "eCbIQkyvNa-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'data/train',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        'data/validation',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n",
        "\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=2000,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=800)"
      ],
      "metadata": {
        "id": "bGE8bKtvPKiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "출처\n",
        "- https://keras.io/ko/\n",
        "- https://www.tensorflow.org/api_docs\n",
        "- https://wikidocs.net/book/9214"
      ],
      "metadata": {
        "id": "Sojwtj7xPQQZ"
      }
    }
  ]
}