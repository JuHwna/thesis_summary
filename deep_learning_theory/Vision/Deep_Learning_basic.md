# 딥러닝 기초
## 1. 딥러닝과 신경망
### 1) 퍼셉트론과 신경망
- 퍼셉트론 : 신경망의 기본 구성요소
- 퍼셉트론의 구성요소
  - 입력(inputs)
  - 가중치(Connection Weight)
  - 바이어스(bias)
  - 합 연산(Sum)
  - 활성 함수(Activation Function)
- 하나의 퍼셉트론에서 이루어지는 연산 : 입력의 가중합 -> 활성화 함수 적용(두단계)
- 하나의 퍼셉트론으로 가능한 연산과 의미
  - Linearly Separable Problem만 해결할 수 있음
    - AND연산, OR연산, NOT연산 가능
    - 최종적으로 하나의 퍼셉트론은 입력(X)의 공간에서 그려보면 하나의 직선(하이퍼 플레인)을 의미
    - 또한, 하나의 퍼셉트론은 AND, OR, NOT 연산을 통해서 Linearly Separable 문제를 해결할 수 있다는 의미
    - AND, OR, NOT 3가지 연산이 가능하다는 것 : 현재 컴퓨터에서 존재하는 3가지 기본연산을 할 수 있다는 의미
    - 퍼셉트론 여러 개를 연결하면 이론적으로 기존 컴퓨터에서 소화하는 모든 연산(함수)를 재현할 수 있다는 것을 뜻함
  - 신경망(OR 모델) : 퍼셉트론을 층 단위로 배치하고 나열하여 만들어진 아키텍처들

- 신경망으로 해결 가능한 묹들
  - 퍼셉트론 덩어리에 활성화 함수, 네트워크 구조 변경을 통해서 문제를 잘 해결할 수 있음
  - Classification(분류)
    - 데이터를 구성하는 클래스 개수와 동일하게 Output 레이어의 퍼셉트론의 개수를 정함
  - Regression(회귀)
    - 앞서 classification과는 다르게 몇 개의 노드로 구성되는지 알 수 없음
    - 넓은 숫자 범위를 가지는 활성화 함수로 변경하여 적용

### 2) 학습 알고리즘
- 딥러닝에서 말하는 학습 : 퍼셉트론 사이 모든 가중치를 주어진 데이터 맞게 셋팅하는 것
- 지도학습 학습 진행 순서
  - [순서1] : 모든 커넥션에 임의에 가중치를 부여함
    - 퍼셉트론 연산이 입력층부터 순차적으로 이루어짐
    - 이전 퍼셉트론 출력의 가중합 -> 활성화 함수 적용이 네트워크를 따라서 계속 이루어지고 마지막 퍼셉트로에서 활성화 함수를 적용한 것이 예측치가 될 거임
  - [순서2] : 가중치를 이용하여 라벨(Y)과 예측값(Prediction)의 에러(Error)를 계산함
    - 뉴럴 넷을 통해 계산된 예측치와 라벨값의 에러를 구함
    - 에러는 개발자가 선택한 특정 함수를 이용하여 계산됨
      - 에러 함수(=목적함수=로스펑션)
        - 보통 로스 평션이라고 부름 : 함수의 출력값(Loss)이 클수록 라벨과의 차이가 크다는 것을 의미함
        - 학습 알고리즘은 반복을 통해 이 로스 값이 최소가 되도록 가중치를 조금씩 변경함
      - 딥러닝에서 중요한 부분 중에 하나는 이 에러를 구하는 함수를 정의하는 것
      - 어떤 함수를 정의하느냐에 따라 최종적으로 신경망에 업데이트되는 가중치의 값이 달라짐
      - 딥러닝의 학습 메커니즘이 에러 함수 값을 최대화 혹은 최소화하도록 구성되어 있음
        - 분류 문제의 경우, Cross Entropy Error를 이용. 회귀 문제의 경우, Mean Squared Error 이용
  - [순서3] : 에러를 가중치로 미분하여 기울기를 구함
    - 에러(로스펑션)의 구성
      - 가중치들의 함수(X,Y값은 고정된 상수, 가중치들만이 고정되지 않은 변수)
    - 학습의 목적
      - 주어진 데이터(X,Y)에 대해서 가중치들을 적절히 셋팅해서 에러가 최소가 되도록 하는 것
      - 미분해서 0이 되는 가중치를 찾는 것이 불가능 -> 이유 : 수백만개의 변수들이 엉켜있어 단순한 뉴럴넷이 아닌 이상 불가능
        - 대신 최적해(Global optimum)를 찾는 것이 아니라 반복을 통해서 그 중에서 제일 로스값을 작게 만드는 가중치(Local optimum)를 찾는 것은 가능함
    - 현재위치 기울기의 반대방향으로 가중치를 이동하면 에러 감소
  - [순서4] : 기울기에 작은 상수를 곱하고 이 값을 기존 가중치에서 빼주어 가중치를 업데이트함
    - 가중치 방향을 알았으면 이동할 크기를 정해야 함
    - 가중치의 이동할 양 : 개발자가 결정함
      - 크게 이동하면 많이 어긋남
      - 보통은 작은 값으로 선택함
      - 선택한 작은 값에 기울기의 반대방향을 곱하고 이 크기만큼 가중치를 이동함
  - [순서4] ->[순서2]로 돌아감
  -      
### 3) 활성화 함수
- 퍼셉트론의 출력값 : Activation Function(모든 퍼셉트론 입력의 가중함)
- 활성화 함수의 종류와 선택
  - 신경망을 개발할 때 사람이 선택 가능함
  - 활성화 함수의 ㅅ너택은 신경망의 예측 성능에 영향을 미침
  - 최소한 활성화 함수 출력 범위 내에 라벨값(Y)들이 포함되어 있어야 정상적인 학습이 가능
- 활성화 함수는 왜 필요할까?
  - 활성화 함수는 신경망에 비선형성을 더해줌
    - 주어진 데이터를 설명하는 복잡한 형상의 선이나 경계면을 찾는데 도움을 줌

### 4) 로스펑션
- 로스 펑션(=에러 함수)
  - 로스(Loss) : 라벨값과 예측값의 차이
  - 로스 펑션 : 라벨값과 예측값의 차이를 계산하기 위한 함수
  - 학습 진행 : 로스를 최소화하는 방향으로 가중치를 업데이트하며 진행
    - 로스 펑션을 제대로 정하는 것이 매우 중요함
- Mean Squared Error
  - $$MSE=\frac{1}{n}\sum(Label-Pred)^2$$
- Cross Entropy
  - $$CE = -\sum(Label)*log(pred)$$
  - 정답 클래스의 확률을 낮게 계산할수록 로스 값이 크게 계산되는 형식

### 5) 옵티마이저(Optimizer)
- 뉴럴넷의 가중치를 업데이트하는 알고리즘
- 가중치를 업데이트하는 방법
  - 경사하강법에서 생기는 여러가지 단점을 극복하기 위해 다양한 알고리즘이 제안됨

- GD(Gradient Descent)의 문제점
  - 모든 데이터를 다 계산하고 발생한 모든 에를 합하여 가중치를 업데이트함
    - 실제 학습에 사용하기에는 큰 문제가 발생
    - 뉴럴넷이 성능을 발휘하기 위해 가중치를 업데이트하기 위해서는 전체 데이터셋을 이용하여 수십번의 가중치를 업데이트해야함
    - GD의 경우 너무 느리고 메모리 소모가 많음
      - EX) 1000개의 데이터셋이 존재하고 뉴럴넷의 가중치가 200개 존재할 때
        - 1 epoch에 1번의 update가 이뤄지고, 한 번의 변화를 이루기 위해 약 100*200번의 gradient 계산이 필요함
    - 1 epoch : 트레이닝 데이터를 모두 활용하여 학습을 진행하는 것
- SGD(Stochastic Gradient Descent)
  - 전체가 아니라 배치(batch)단위로 가중치 업데이트하자는 아이디어
    - 배치 : 전체 데이터를 동일한 숫자로 나눈 것
    - ex) 1000개의 데이터셋을 200개 1배치로 묶으면 1 epoch당 총 5번의 업데이트가 일어남
  
