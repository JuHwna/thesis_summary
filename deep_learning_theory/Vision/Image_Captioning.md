# Image Captioning(이미지 캡셔닝)
- 이미지를 입력하면 해당 이미지를 잘 설명하는 문장을 생성해내는 Task
- 컴퓨터 비전과 NLP가 합쳐진 영역으로 NLP에 대한 지식도 필요한 Task

![image](https://github.com/JuHwna/thesis_summary/assets/49123169/9b2d42ad-13ef-4eaa-80ca-cff3a126f454)

# (1) 이미지 캡셔닝 아이디어
![image](https://github.com/JuHwna/thesis_summary/assets/49123169/5fb513f3-155f-43bd-a1dc-e6563cb7afb5)

- text generation : 이미지가 주어졌을 때 이미지의 각 물체와 상황을 판단하고 설명하는 문장이 만들어지는 것
- image captioning
  - 이미지 내에 있는 객체에 대한 판단뿐만 아니라 객체들 간의 관계를 파악하고 자연어의 형태로 알맞게 표현하는 문제도 겸하고 있음
  - 컴퓨터 비전과 자연어처리의 종합적인 이해를 필요로 함
  - 이미지를 문장으로 바꾼 후 NLP 분야와 결합하여 다양하게 활용할 수 있고 비교적 용량이 큰 데이터인 이미지를 텍스트로 바꾸기 때문에 저장공간이 획기적으로 줄어들 수 있음

#### 이미지 캡셔닝 활용 사례
1. 이미지에 캡션을 추가해 검색의 효율성이 증가함
   - 기존에는 텍스트만으로 검색이 가능했지만 이미지를 텍스트화하여 가능하게 바꾸어 주었기 때문에 검색 효율이 더 좋아짐

2. 시각 장애인이나 저시력자들에게 도움을 줄 수 있음
   - 이미지를 텍스트화한 후 TTS(Text To Speak) 기술을 이용하여 음성으로 읽어주어 앞의 상황이나 그림에 대해 설명해줄 수 있음

3. 미술 치료 분야에서 치료사의 주관을 배제하고 이미지 캡셔닝을 이용하여 객관적으로 미술에 대한 설명을 함으로서 치료의 일관성을 부여할 수 있음


### 이미지 분할 네트워크의 기본 구조 : CNN & RNN
![image](https://github.com/JuHwna/thesis_summary/assets/49123169/408dab24-710f-4bc5-a5b5-8e383abc83fc)

- 이미지 캡셔닝에 기본 네트워크는 이미지 특징을 추출하기 위한 CNN과 문장으로 나타내기 위한 RNN을 주로 사용함
   1. 입력 이미지를 CNN을 통과시켜 특징들을 추출함
   2. 추출한 특징들을 RNN에 대입하여 문장을 만들 수 있도록 이미지 데이터셋과 이미지에 대한 캡션들을 토대로 학습시킴

![image](https://github.com/JuHwna/thesis_summary/assets/49123169/93f33434-7e44-45ab-8ce5-dab65e0551c3)

- 사진과 같이 입력 이미지를 CNN을 이용해 특징을 추출하고 attention과 LSTM을 이용한 RNN 모델을 통과하여 각 특징에 맞는 단어들을 얻어내고 이미지를 설명하는 문장을 완성

### 이미지 캡셔닝 단계
- 예시 사진
![image](https://github.com/JuHwna/thesis_summary/assets/49123169/80fc1cde-f0bd-4139-813d-c5638728d8ef)

#### 1. CNN을 이용한 특징 추출
![image](https://github.com/JuHwna/thesis_summary/assets/49123169/ab2933c8-64ee-4065-97eb-fd46c0b65772)

- RGB 3채널의 인풋 데이터를 ResNet-101을 이용하여 특징을 추출함

#### 2. Attention Network를 이용하여 단어 생성
![image](https://github.com/JuHwna/thesis_summary/assets/49123169/d4eb75e8-1571-4743-84c1-d9505d225b51)

- Attention Network를 통과하여 중요한 부분에 가중치를 두어 단어를 추출해 냄
- 가중치가 높은 부분을 이용해 사진을 한 문장으로 설명할 수 있는 물체, 행위 등을 추출해냄

#### 3. RNN을 이용하여 문장 완성
![image](https://github.com/JuHwna/thesis_summary/assets/49123169/3c55314f-9f3a-4557-8c12-d96b153f4ba5)

- 가중치를 바탕으로 추출해낸 이미지의 주요 단어들을 RNN을 통해 조합하여 문장을 완성함
- 주요 단어들로 문장을 완성했기 때문에 배경과 같은 중요하지 않은 부분은 누락될 수 있지만 한 문장으로 간결하게 이미지를 설명해낼 수 있음

#### 전체 구조
![image](https://github.com/JuHwna/thesis_summary/assets/49123169/fde82fb4-b9c8-46b8-9061-10973ba3fd68)

### 이미지 캡셔닝 성공 예시
![image](https://github.com/JuHwna/thesis_summary/assets/49123169/7e7e87d1-8837-4818-b8de-34b607d4c601)

- 물체와 상황을 정확히 인지한 경우 한 문장으로 이미지를 잘 설명하고 있는 것을 알 수 있음
- 어텐션을 이용하여 중요한 부분에 집중하여 사진의 특징을 정확하게 텍스트로 나타내어 주었음

### 이미지 캡셔닝 실패 예시
![image](https://github.com/JuHwna/thesis_summary/assets/49123169/14517972-270c-44b6-9ec4-de7cee1fdb8b)

- 실패는 물체나 행위를 잘못 인지한 상황으로 어느 한 가지라도 잘못 추출하면 아예 다른 설명이 되어버릴 수 있기 때문에 상당한 정확성이 요구됨

## 1) 이미지 캡셔닝 데이터셋

# (2) 이미지 캡셔닝 모델 이해를 위한 사전지식
## 1) 워드 임베딩
#### 워드 임베딩(Word Embedding)
- 워드 임베딩 : 단어를 벡터로 표현하는 방법으로 단어를 밀집 표현으로 변환하는 것
  - 단어를 밀집 벡터(dense vecotr)의 형태로 표현하는 방법
- 임베딩 벡터 : 밀집 벡터를 워드 임베딩 과정을 통해 나온 결과
- 워드 임베딩 방법론 : LSA, Word2Vec, FastText, Glove 등
  - 케라스에서 제공하는 도구인 Embedding() : 앞서 언급한 방법들을 사용하지는 않지만 단어를 랜덤한 값을 가지는 밀집 벡터로 변환한 뒤에 인공 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습하는 방법을 사용함

### 1. 희소 표현(Sparse Representation)
- 희소 표현 : 단어를 표현할 때 모든 단어 개수로 벡터 차원을 설정하고 표현하고자 하는 인덱스 값만 1로 설정하고 나머지는 0으로 표현하는 원-핫 벡터로 표현하는 방법
  - ex) 고양이 = [ 0 0 0 0 1 0 0 0 0 ......]
- 희소 벡터의 문제점
  - 단어의 개수가 늘어나면 벡터의 차원이 한없이 커진다는 점
    - 단어 집합이 클수록 고차원의 벡터가 됨
    - 공간적 낭비를 불러일으킴
      - 희소 표현의 일종인 DTM과 같은 경우에도 특정 문서에 여러 단어가 다수 등장하였으나 다른 많은 문서에서는 해당 특정 문서에 등장했던 단어들이 전부 등장하지 않는다면 역시나 행렬의 많은 값이 0이 되면서 공간적 낭비를 일으킴
  - 원-핫 벡터는 단어의 의미를 담지 못함

### 2. 밀집 표현(Dense Representation)
- 밀집 표현 : 벡터의 차원을 단어 집합의 크기로 상정하지 않음
  - 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤
  - 이 과정에서 더 이상 0과 1만 가진 값이 아니라 실수값을 가지게 됨
- ex) 고양이 = [0.2 1.8 1.1 -2.1 ...........]
- 밀집 벡터 : 벡터의 차원이 조밀해졌다고 하여 붙여진 이름

## 2) Word2Vec
#### Word2Vec
- 단어 간 유사도를 반영할 수 있도록 단어의 의미를 벡터화할 수 있는 방법 중 대표적인 방법
- 두 가지 방식
  - CBOW(Continuous Bag of Words) : 주변에 있는 단어들을 가지고 중간에 있는 단어들을 예측하는 방법
  - Skip-Gram : 중간에 있는 단어로 주변 단어들을 예-측하는 방법
- 두 가지 방식 모두 메커니즘 자체는 거의 동일함


### 1. CBOW(Continuous Bag of Words)
- CBOW의 기본 아아디어 : 주변 단어를 통해 주어진 단어를 예측하는 것
  - 총 단어가 C개라고 할 때 앞 뒤로 C/2개의 단어를 통해 주어진 단어를 예측하는 방법
- ex) "The fat cat sat on the mat" {"The", "fat", "cat", "on", "the", "mat"}으로부터 sat을 예측하는 것 => CBOW가 하는 일
  - 중심 단어(center word) : 예측해야하는 단어 sat
  - 주변 단어(context word) : 예측에 사용되는 단어들
  - 윈도우 : 중심 단어를 기준으로 앞 뒤로 몇 개의 단어를 볼지 선택하는 ㄴ범위
    - ex) 위 예문에서 중심 단어가 cat이고 window가 2라면 "The", "fat", "on", "the" 단어를 참고
![image](https://github.com/JuHwna/thesis_summary/assets/49123169/9f110264-5793-4c42-9f7e-2bbc9e323ecc)

- 위 그림처럼 학습시킬 문장의 모든 단어들을 원-핫 인코딩 시켜줌
- 그 다음 window = m 으로 설정하고 하나의 Center 단어에 대해 주변 단어의 벡터를 Input으로 넣어줌
  - 수식 : $(x^{c-m},x^{c-m+1},...,x^{c-1},x^{c+1},...,x^{c+m-1},x{c+m}) \in IR^{|V|}$
- CBOW 방식의 파라미터 : Input layr -> Hidden layer로 가는 weights와 Hidden layer -> Output layer로 가는 weights
  - $W \in IR^{V \times N}, W` \in IR^{N \times V}$
- 워드 임베딩된 벡터 : CBOW 신경망을 학습시켜 나온 최적의 가중치
  - CBOW 모델은 맥락 단어로부터 타깃 단어를 예측하는 모델
  - CBOW 모델의 입력 : 맥락 단어의 원-핫 벡터
  - 출력 : 타깃 단어의 원-핫 벡터

![image](https://github.com/JuHwna/thesis_summary/assets/49123169/5135f151-1e05-4f0c-922e-acff4471f270)

- 위는 CBOW 신경망 모델의 개괄적인 모습
  - Window가 2일 때의 예이며 입력인 맥락 단어는 fat, cat, on, the이며 출력인 타깃 단어는 sat임


- 입출력 데이터가 모두 원-핫 벡터이며 지면 관계상 fat과 the는 생략되어 있음
  - 투사층(projection layer)의 크기 : M
    - 워드 임베딩 결과의 차원임
  - 입력층(input layer)에서의 가중치 행렬 W의 크기 : V x M
    - V : 단어의 개수
  - 출력층 W` 행렬의 크기 : M x V
  - W와 W`는 서로 다른 행렬임
- CBOW 신경망 모델은 입력 베터(맥락 단어)를 통해 출력 벡터(타깃 단어)를 맞추기 위해 계속해서 학습하며 이 가중치 행렬 W와 W`을 갱신함

![image](https://github.com/JuHwna/thesis_summary/assets/49123169/d370804d-fe97-4ca8-9988-2388802f1267)

- 입력층의 입력 벡터와 가중치 행렬 W가 곱해지는 과정
  - 위 그림에서 맥락 단어의 원-핫 벡터를 X라 표기했음
  - 예를 들어 x_cat(cat에 대한 원-핫 벡터)은 3번째 인덱스만 1이고 나머지는 다 0
  - 이와 가중치 행렬 W를 곱해주면 W의 3행에 해당되는 벡터가 도출됨
  - 그저 가중치 행렬 W에서 '입력 벡터에서 1이 포함된 인덱스'에 해당하는 행을 추출하는 작업일 뿐임
  - 원-핫 벡터 x에서 1의 값을 가지고 있는 인덱스를 i라 할 때, 가중치 행렬 W의 i번째 행을 가져오는 작업임

- 가중치 행렬 W가 결국 우리가 구하고자 하는 워드 임베딩 결과
  - 좋은 워드 임베딩 값을 구하기 위해서는 가중치 행렬 W를 잘 학습해야 함
![image](https://github.com/JuHwna/thesis_summary/assets/49123169/6a71d293-64e6-4251-9847-23cc1bf89b62)

- 입력인 맥락 단어가 총 4개이므로 각 입력 원-핫 벡터와 가중치 행렬 W를 곱한 벡터 v가 4개 도출될 것임
  - 이 벡터들의 평균 벡터를 구해야 함
    - 구해진 모든 v벡터(v_fat, v_eat, v_on, v_the)를 더한 뒤 4로 나누어주면 됨
      - 값 4 : 2 x (window size)
