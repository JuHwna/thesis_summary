# CHAPTER 01. 서론
## 1.3 가설 공간
- 귀납 : '특수'에서 '일반'으로 일반화(generalization)하는 과정
  - 구체적인 사실에서 일반성을 가진 규칙을 찾는 것
  - 귀납 학습(inductive learning) : 샘플을 통해 학습하는 것
- 연역 : '일반'에서 '특수'로 특화(specialization)하는 과정
  - 기초 원리로부터 구체적인 정황을 추론하는 것
- 귀납 학습
  - 좁은 의미 : 훈련 데이터에서 개념을 배울 것을 요구함 -> 개념 학습 or 개념 형성
  - 넓은 의미 : 샘플을 통해 배우는 것
  - 개념 학습 기술 : 현재 연구되거나 응용되는 것이 비교적 많지 않음
    - why? : 일반화 성능이 좋으면서 의미 또한 명확한(해석력이 좋은) 개념을 학습하는 것은 쉽지 않기 때문
    - 이 때문에 현실에서 자주 사용되는 기술은 대부분 블랙박스 모델임
    - 그런데 개념 학습에 대한 이해가 있다면 머신러닝의 기초 사상을 이해하는 데 많은 도움이 됨
    - 개념 학습 중에서 가장 기초 : 부울 개념 학습
      - '예'나 '아니오' 같은 표현을 0/1 불리언값을 가진 목표 개념을 학습하는 것
- 버전 공간(version space)
  - 많은 가설이 훈련 데이터 세트와 일치할 수 있고 훈련 데이터 세트와 일치하는 '가설들의 집합'이 존재함

## 1.4 귀납적 편향
- 학습을 통해 얻은 모델은 가설 공간 중에 하나의 가설에 대응함
  - 어떤 모델(혹은 가설)을 사용해야 할까?
- 구체적인 학습 알고리즘은 반드시 하나의 모델을 생성해 내야함
  - 학습 알고리즘 본연의 '편향'이 중요하게 작용함
- 귀납적 편향(inductive bias) or 편향(bias) : 머신러닝 알고리즘이 학습 과정에서 특정한 유형의 가설에 대한 편향
  - 모든 유효한 머신러닝 알고리즘은 귀납적 편향을 가지고 있음
  - 그렇지 않다면 가설 공간 훈련 데이터상의 '효과가 같아 보이는' 비슷한 가설들 사이에서 혼란에 빠질 수 있으며 확실한 학습 결과를 생성해 낼 수도 없음
  - 학습 알고리즘이 방대한 가설 공간에서 가설들을 선택할 때 가지는 휴리스틱 방법 혹은 가치관 정도로 해석할 수 있음
  - 학습 알고리즘이 내놓은 '어떤 모델이 더 좋은가'에 관한 가설에 대응함
  - 현실의 문제 중에서 이 가설의 성립 여부는 알고리즘이 좋은 성능을 얻을 수 있는지를 결정하게 됨
- 오컴의 면도날(자연과학 연구에서 가장 기본이 되는 원칙)
  - '만약 다수의 가설이 관측된 것과 일치한다면, 가장 간단한 것을 선택해야 한다'라는 원칙
  - 유일한 원칙은 아님
