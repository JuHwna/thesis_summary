# CHAPTER 01. 서론
## 1.3 가설 공간
- 귀납 : '특수'에서 '일반'으로 일반화(generalization)하는 과정
  - 구체적인 사실에서 일반성을 가진 규칙을 찾는 것
  - 귀납 학습(inductive learning) : 샘플을 통해 학습하는 것
- 연역 : '일반'에서 '특수'로 특화(specialization)하는 과정
  - 기초 원리로부터 구체적인 정황을 추론하는 것
- 귀납 학습
  - 좁은 의미 : 훈련 데이터에서 개념을 배울 것을 요구함 -> 개념 학습 or 개념 형성
  - 넓은 의미 : 샘플을 통해 배우는 것
  - 개념 학습 기술 : 현재 연구되거나 응용되는 것이 비교적 많지 않음
    - why? : 일반화 성능이 좋으면서 의미 또한 명확한(해석력이 좋은) 개념을 학습하는 것은 쉽지 않기 때문
    - 이 때문에 현실에서 자주 사용되는 기술은 대부분 블랙박스 모델임
    - 그런데 개념 학습에 대한 이해가 있다면 머신러닝의 기초 사상을 이해하는 데 많은 도움이 됨
    - 개념 학습 중에서 가장 기초 : 부울 개념 학습
      - '예'나 '아니오' 같은 표현을 0/1 불리언값을 가진 목표 개념을 학습하는 것
- 버전 공간(version space)
  - 많은 가설이 훈련 데이터 세트와 일치할 수 있고 훈련 데이터 세트와 일치하는 '가설들의 집합'이 존재함

## 1.4 귀납적 편향
- 학습을 통해 얻은 모델은 가설 공간 중에 하나의 가설에 대응함
  - 어떤 모델(혹은 가설)을 사용해야 할까?
- 구체적인 학습 알고리즘은 반드시 하나의 모델을 생성해 내야함
  - 학습 알고리즘 본연의 '편향'이 중요하게 작용함
- 귀납적 편향(inductive bias) or 편향(bias) : 머신러닝 알고리즘이 학습 과정에서 특정한 유형의 가설에 대한 편향
  - 모든 유효한 머신러닝 알고리즘은 귀납적 편향을 가지고 있음
  - 그렇지 않다면 가설 공간 훈련 데이터상의 '효과가 같아 보이는' 비슷한 가설들 사이에서 혼란에 빠질 수 있으며 확실한 학습 결과를 생성해 낼 수도 없음
  - 학습 알고리즘이 방대한 가설 공간에서 가설들을 선택할 때 가지는 휴리스틱 방법 혹은 가치관 정도로 해석할 수 있음
  - 학습 알고리즘이 내놓은 '어떤 모델이 더 좋은가'에 관한 가설에 대응함
  - 현실의 문제 중에서 이 가설의 성립 여부는 알고리즘이 좋은 성능을 얻을 수 있는지를 결정하게 됨
- 오컴의 면도날(자연과학 연구에서 가장 기본이 되는 원칙)
  - '만약 다수의 가설이 관측된 것과 일치한다면, 가장 간단한 것을 선택해야 한다'라는 원칙
  - 유일한 원칙은 아님

# CHAPTER 02. 모델 평가 및 선택
## 2.1 경험 오차 및 과적합
- 오차율(error rate) : 전체 샘플 수와 잘못 분류한 샘플 수의 비율
  - 오차율 : E = ａ/m
  - 정밀도 : 1 -ａ/m ( 1 - 오차율)
- 오차 : 학습기의 실제 예측 값과 샘플의 실제 값 사이의 차이
  - 훈련 오차(training error) or 경험 오차(empirical error) : 학습기가 훈련 세트상에서 만들어낸 오차
  - 일반화 오차(generalization error) : 학습기가 새로운 샘플 위에서 만들어낸 오차
- 과적합 : 학습기가 훈련 데이터에서 학습을 '과도하게 잘하면', 훈련 데이터 중의 일정한 특성을 모든 데이터에서 내재된 일반 성질이라 오해하게 만듦
  - 일반화 성능이 떨어지는 현상
- 과소적합 : 학습기가 훈련 데이터의 일반 성질을 제대로 배우지 못했다는 뜻
- 과적합을 일으키는 원인
  - 학습능력이 너무 뛰어나 훈련 데이터들이 가진 일반적이지 않은 특성까지 학습하는 경우
  - 해결 방법 : 매우 까다로움
- 과소적합을 일으키는 원인
  - 학습능력이 좋지 못해서인 경우가 많음
  - 해결 방법 
    - ex) 의사결정 트리의 경우는 가지치기, 신경망 학습의 경우에는 에포크 수를 늘리면 됨
    - 에포크(epoch) : 인공 신경망에서 전체 데이터에 대한 순전파와 역전파가 끝난 상태를 말함

- 모델 선택 문제 
  - 어떠한 학습 알고리즘을 사용하고 어떠한 파라미터를 선택해야 하는 문제
  - 이상적인 해답 : 일반화 오차를 기준으로 평가한 뒤, 일반화 오차가 가장 작은 모델을 선택하는 것
    - 일반화 오차를 직접적으로 얻을 수 없음
    - 훈련 오차에서 부딪히는 과적합 문제는 피할 수도 완벽하게 극복할 수도 없음
    
## 2.2 평가 방법
- 테스트라는 과정을 통해 학습기의 일반화 오차에 대해 평가를 진행하고 모델을 선택함
  - 테스트 세트(testing set)를 활용하여 학습기가 만나보지 못했던 새로운 샘플에서 어떻게 작동할지 예측할 수 있고 테스트 세트에서 나온 테스트 오차(testing error)를 실제 일반화 오차의 근삿값으로 생각함
- 테스트 샘플이 실제 샘플과 동일한 분포를 나타내고 있다고 가정함
- 주의해야할 점 : 테스트 세트와 훈련 세트의 중복을 최대한 피해야 함

- 우리에게 m개의 샘플을 가진 데이터 세트 D = {(x<sub>1</sub>, y<sub>1</sub>), (x<sub>2</sub>, y<sub>2</sub>),...,(x<sub>m</sub>,y<sub>m</sub>)}
  - 데이터 세트 D를 적절히 처리하여 훈련 세트 S와 테스트 세트 T로 나누는ㄴ 것
  
### 2.2.1 홀드아웃
- 홀드아웃(hold-out) 방법(검증 세트 기법) : 데이터 세트 D를 겹치지 않는 임의의 두 집합으로 나눔
  - S : 훈련 세트 집합
  - T : 테스트 세트
  - D = S ∪ T, S ∩ T = x
  - 훈련 세트 S를 통해 훈련된 모델은 테스트 세트 T를 활용해 오차를 측정하고 일반화 오차에 대한 추정치를 제공함
- 주의해야 할 것 : 훈련/ 테스트 세트를 나눌 때 되도록이면 데이터 분포가 같게 나눠야 한다는 것
  - 층화 추출법 : 양성/음성 샘플의 비중을 비슷하게 분류하는 것
  
### 2.2.2 교차 검증
- 교차 검증 
  - 데이터 세트 D를 k개의 서로소 집합으로 나누는 것으로 시작함
    - D = D<sub>1</sub> ∪ D<sub>2</sub> ∪ ... ∪ D<sub>k</sub>, D<sub>i</sub> ∩ D<sub>j</sub> = Φ (i ≠ j)
    - 매 부분집합 D<sub>i</sub>는 되도록 데이터 분포를 반영하도록 나눔
    - D로부터 층화 추출법을 통해 나누는 거
  - K-1개의 부분집합들을 훈련 세트로 사용하고 나머지 하나의 부분집합을 테스트 세트로 사용함
- K-겹 교차 검증(K-fold cross validation) : 교차 검증법을 통한 평가 결과의 안정성과 정확도는 k의 값에 따라 달라짐
  
- LOOCV(Leave-One-Out Cross Validation) : m개의 샘플이 있는 데이터 세트 D를 K=m으로 설정하고 교차 검증을 실행하는 것
  - 샘플 분류 방법에 대한 영향을 받지 않음
    - why? : m개의 샘플을 분류하는 방법은 m개의 부분집합을 만드는 것 밖에 없기 때문
    - LOOCV에 사용한 훈련 세트는 원본 데이터 세트와 비교할 때 1개의 샘플밖에 차이가 나질 않기 때문에 대부분 상황에서 LOOCV 방법을 활용한 모델은 모든 데이터 세트 D를 활용하여 훈련한 모델과 매우 비슷한 성능을 보임
      - LOOCV를 활용한 방법은 편향이 작음
  - 단점 : 데이터 세트의 크기가 매우 클 때 모델을 m번 적합해야 하므로 계산량이 많아짐
  
### 2.2.3 부트스트래핑
- 데이터 세트 D의 모든 데이터를 활용하여 훈련시킨 모델을 평가할 때 사용하는 해결책
- 부트스트래핑(bootstrapping) : 부트스트랩샘플링에 기반을 둔 샘플 추출 기법
  - m개의 샘플이 있는 데이터 세트 D를 가정한다면 우리는 샘플링을 통해 데이터 세트 D`를 만듦
  - 매번 D에서 샘플 하나를 꺼내 D`에 복사하여 넣음
  - 그리고 다시 원래의 데이터 세트 D로 돌려보냄
    - 한 번 뽑혔던 샘플도 다시 뽑힐 가능성이 있음
  - 이러한 과정을 m번 반복한 후, 우리는 m개의 샘플이 들어 있는 데이터 세트 D`를 얻음
    - 이것이 부트스트래핑의 결과
  - D의 샘플 중 일부는 D`에서 반복 출현함. 어떤 샘플은 아예 뽑히지 않을 수도 있음
  - m번의 채집 과정 중 샘플이 한 번도 뽑히지 않을 확률 : (1-1/m)m
    - 극한값을 계산 시

$$\lim_{m \to \infty} \(1-\frac{1}{m})^m = \frac{1}{e} ≒ 0.368 $$

- Out-of-Bag 예측
  - 부트스트래핑을 사용하면 데이터 세트 D 중의 36.8%의 샘플은 D`에 들어가지 못함
    - D`:훈련 세트
    - D/D`: 테스트 세트
    - m개의 샘플을 모두 활용하여 모델 훈련에 사용할 수 있고 활용하지 못한 1/3에 해당되는 샘플들은 테스트 샘플로 활용할 수 있음
- 부트스트래핑의 장점
  - 데이터 세트가 비교적 적거나 훈련/테스트 세트로 분류하기 힘들 때 사용하기 좋음
  - 초기 데이터 세트에서 다양한 훈련 세트를 여러 개 만들 수 있어서 앙상블 기법에 적용하기 좋음
- 부트스트래핑의 단점
  - 부트스트래핑을 통해 생성된 데이터 세트들은 초기 데이터의 분포와 다를 수 있어 편향을 크게 만들 수 있음
  - 이 때문에 초기 데이터 보유량이 충분할 때 ㄱ머증 세트 기법과 교차 검증 기법을 더 자주 활용함
  
### 2.2.4 파라미터 튜닝과 최종 모델
- 머신러닝의 파라미터 종류
  - 첫 번째, 알고리즘의 파라미터(하이퍼 파라미터) : 일반적으로 10개 이내
  - 두 번째, 모델의 파라미터 : 개수가 매우 많을 수도 있음
  - 튜닝 방식 : 전자와 후자 모두 비슷함
  - 선택 방법 : 여러 개의 모델을 생성한 후 모종의 평가 기준을 통해 선택
  - 다른 점
    - 전자 : 일반적으로 사람이 파라미터 개수를 선택하고 모델을 생성
    - 후자 : 학습을 통해 다수의 후보 모델을 만들어냄(ex) 딥러닝의 서로 다른 횟수의 학습 조기 종료와 같은 파라미터
- 파라미터 튜닝(parameter tuning)
  - 모델 평가 및 선택 시 학습 알고리즘의 선택뿐만 아니라 알고리즘 파라미터에 대한 설정도 고려해야 함
  - 주의해야 할 점
    - 학습 알고리즘의 많은 파라미터는 실수 범위의 값을 가진다는 것
    - '모든 파라미터'를 훈련을 통해 모델에 적합시키기는 어려움
    - 현실에서 사용되는 방법 : 파라미터의 범위와 변화 간격을 설정해 주는 것
      - 이러한 파라미터는 최적의 파라미터가 아닐 수 있음
      - 단지 계산량과 성능 예측을 위한 목표 사이에서 절충한 결과
- 학습한 모델의 실제 성능을 측정하기 위한 데이터는 테스트 데이터라는 점
- 검증 세트 : 모델 선택과 파라미터 조율을 위해, 테스트 데이터를 활용하여 성능을 측정하기 전에 검사할 수도 있는데 모델 평가 및 선택 과정에서 쓰는 평가 테스트 데이터 집합

- 테스트 세트 : 모델의 실제 성능을 예측
- 검증 세트 : 모델이나 파라미터 선택에 검증 세트로 활용

## 2.3 모델 성능 측정
- 성능 측정
  - 학습기의 일반화 성능에 대해 평가할 때 유효하고 실험 가능한 테스트 방법뿐만 아니라 모델의 일반화 성능을 평가할 기준이 있어야 함
  - 프로젝트 목적을 반영해야 함
- 예측을 위한 과제
  - 데이터 샘플 D = {(x<sub>1</sub>, y<sub>1</sub>), (x<sub>2</sub>, y<sub>2</sub>, ..., (x<sub>m</sub>, y<sub>m</sub>)}
  - y<sub>i</sub>는 x<sub>i</sub>의 정답 데이터
  - 학습기 f의 성능을 측정하려면 학습기의 예측 결과인 f(x)와 정답 데이터인 y를 비교해야 함
- 회귀분석에서 가장 자주 사용하는 성능 측정 방법 : 평균제곱오차(mean squared error)

$$E(f; D) = \frac{1}{m}\displaystyle\sum_{i=1}^{m} (f(x_{i})-y_{i})^2 $$

- 데이터 분포 D와 확률밀도 함수 p(·)로 표현 시

$$E(f; D) = \int\limits_x^D (f(x_{i})-y_{i})^2 p(x)dx $$

### 2.3.1 오차율과 정확도
- 분류 분석에서 가장 자주 사용하는 두 가지 성능 측도 : 오차율, 정확도
  - 오차율 : 모든 샘플 수에서 잘못 분류한 샘플 수가 차지하는 비율
  - 정확도 : 전체 샘플 수에서 정확히 분류한 샘플 수가 차지하는 비율
- 샘플 데이터 D에서 분류 오차율

$$E(f; D) = \frac{1}{m}\displaystyle\sum_{i=1}^{m} Ⅱ(f(x_{i})≠y_{i}) $$

- 정확도

$$acc(f; D) = \frac{1}{m}\displaystyle\sum_{i=1}^{m} Ⅱ(f(x_{i})=y_{i})= 1-E(f; D) $$

- 더 일반적으로 데이터 분포 D에 대해서 확률밀도 함수 p(·)로 나타내면 오차율과 정확도는 각각 다음과 같은 식으로 표현됨
- 오차율

$$E(f; D) = \int\limits_x^D Ⅱ(f(x)≠y)p(x)dx $$

- 정확도

$$acc(f; D) = \int\limits_x^D Ⅱ(f(x)=y)p(x)dx = 1-E(f; D) $$

### 2.3.2 재현율, 정밀도 그리고 F1 스코어
- 오차율과 정확도는 자주 사용되지만 모든 문제에 활용되진 못함
- 이진 분류 문제에서 실제 클래스와 학습기가 예측 분류한 클래스의 조합
  - 실제 양성(true positive), 거짓 양성(false positive), 실제 음성(true negative), 거짓 음성(false negative)
  - TP, FP, TN, FN
  - TP+FP+TN+FN=총 샘플 수
  - 분류 결과 혼동행렬
  
  |실제 값|예측 값|
  |-------|------|
