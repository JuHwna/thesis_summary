# CHAPTER 01. 서론
## 1.3 가설 공간
- 귀납 : '특수'에서 '일반'으로 일반화(generalization)하는 과정
  - 구체적인 사실에서 일반성을 가진 규칙을 찾는 것
  - 귀납 학습(inductive learning) : 샘플을 통해 학습하는 것
- 연역 : '일반'에서 '특수'로 특화(specialization)하는 과정
  - 기초 원리로부터 구체적인 정황을 추론하는 것
- 귀납 학습
  - 좁은 의미 : 훈련 데이터에서 개념을 배울 것을 요구함 -> 개념 학습 or 개념 형성
  - 넓은 의미 : 샘플을 통해 배우는 것
  - 개념 학습 기술 : 현재 연구되거나 응용되는 것이 비교적 많지 않음
    - why? : 일반화 성능이 좋으면서 의미 또한 명확한(해석력이 좋은) 개념을 학습하는 것은 쉽지 않기 때문
    - 이 때문에 현실에서 자주 사용되는 기술은 대부분 블랙박스 모델임
    - 그런데 개념 학습에 대한 이해가 있다면 머신러닝의 기초 사상을 이해하는 데 많은 도움이 됨
    - 개념 학습 중에서 가장 기초 : 부울 개념 학습
      - '예'나 '아니오' 같은 표현을 0/1 불리언값을 가진 목표 개념을 학습하는 것
- 버전 공간(version space)
  - 많은 가설이 훈련 데이터 세트와 일치할 수 있고 훈련 데이터 세트와 일치하는 '가설들의 집합'이 존재함

## 1.4 귀납적 편향
- 학습을 통해 얻은 모델은 가설 공간 중에 하나의 가설에 대응함
  - 어떤 모델(혹은 가설)을 사용해야 할까?
- 구체적인 학습 알고리즘은 반드시 하나의 모델을 생성해 내야함
  - 학습 알고리즘 본연의 '편향'이 중요하게 작용함
- 귀납적 편향(inductive bias) or 편향(bias) : 머신러닝 알고리즘이 학습 과정에서 특정한 유형의 가설에 대한 편향
  - 모든 유효한 머신러닝 알고리즘은 귀납적 편향을 가지고 있음
  - 그렇지 않다면 가설 공간 훈련 데이터상의 '효과가 같아 보이는' 비슷한 가설들 사이에서 혼란에 빠질 수 있으며 확실한 학습 결과를 생성해 낼 수도 없음
  - 학습 알고리즘이 방대한 가설 공간에서 가설들을 선택할 때 가지는 휴리스틱 방법 혹은 가치관 정도로 해석할 수 있음
  - 학습 알고리즘이 내놓은 '어떤 모델이 더 좋은가'에 관한 가설에 대응함
  - 현실의 문제 중에서 이 가설의 성립 여부는 알고리즘이 좋은 성능을 얻을 수 있는지를 결정하게 됨
- 오컴의 면도날(자연과학 연구에서 가장 기본이 되는 원칙)
  - '만약 다수의 가설이 관측된 것과 일치한다면, 가장 간단한 것을 선택해야 한다'라는 원칙
  - 유일한 원칙은 아님

# CHAPTER 02. 모델 평가 및 선택
## 2.1 경험 오차 및 과적합
- 오차율(error rate) : 전체 샘플 수와 잘못 분류한 샘플 수의 비율
  - 오차율 : E = ａ/m
  - 정밀도 : 1 -ａ/m ( 1 - 오차율)
- 오차 : 학습기의 실제 예측 값과 샘플의 실제 값 사이의 차이
  - 훈련 오차(training error) or 경험 오차(empirical error) : 학습기가 훈련 세트상에서 만들어낸 오차
  - 일반화 오차(generalization error) : 학습기가 새로운 샘플 위에서 만들어낸 오차
- 과적합 : 학습기가 훈련 데이터에서 학습을 '과도하게 잘하면', 훈련 데이터 중의 일정한 특성을 모든 데이터에서 내재된 일반 성질이라 오해하게 만듦
  - 일반화 성능이 떨어지는 현상
- 과소적합 : 학습기가 훈련 데이터의 일반 성질을 제대로 배우지 못했다는 뜻
- 과적합을 일으키는 원인
  - 학습능력이 너무 뛰어나 훈련 데이터들이 가진 일반적이지 않은 특성까지 학습하는 경우
  - 해결 방법 : 매우 까다로움
- 과소적합을 일으키는 원인
  - 학습능력이 좋지 못해서인 경우가 많음
  - 해결 방법 
    - ex) 의사결정 트리의 경우는 가지치기, 신경망 학습의 경우에는 에포크 수를 늘리면 됨
    - 에포크(epoch) : 인공 신경망에서 전체 데이터에 대한 순전파와 역전파가 끝난 상태를 말함

- 모델 선택 문제 
  - 어떠한 학습 알고리즘을 사용하고 어떠한 파라미터를 선택해야 하는 문제
  - 이상적인 해답 : 일반화 오차를 기준으로 평가한 뒤, 일반화 오차가 가장 작은 모델을 선택하는 것
    - 일반화 오차를 직접적으로 얻을 수 없음
    - 훈련 오차에서 부딪히는 과적합 문제는 피할 수도 완벽하게 극복할 수도 없음
    
## 2.2 평가 방법
- 테스트라는 과정을 통해 학습기의 일반화 오차에 대해 평가를 진행하고 모델을 선택함
  - 테스트 세트(testing set)를 활용하여 학습기가 만나보지 못했던 새로운 샘플에서 어떻게 작동할지 예측할 수 있고 테스트 세트에서 나온 테스트 오차(testing error)를 실제 일반화 오차의 근삿값으로 생각함
- 테스트 샘플이 실제 샘플과 동일한 분포를 나타내고 있다고 가정함
- 주의해야할 점 : 테스트 세트와 훈련 세트의 중복을 최대한 피해야 함

- 우리에게 m개의 샘플을 가진 데이터 세트 D = {(x<sub>1</sub>, y<sub>1</sub>), (x<sub>2</sub>, y<sub>2</sub>),...,(x<sub>m</sub>,y<sub>m</sub>)}
  - 데이터 세트 D를 적절히 처리하여 훈련 세트 S와 테스트 세트 T로 나누는ㄴ 것
  
### 2.2.1 홀드아웃
- 홀드아웃(hold-out) 방법(검증 세트 기법) : 데이터 세트 D를 겹치지 않는 임의의 두 집합으로 나눔
  - S : 훈련 세트 집합
  - T : 테스트 세트
  - D = S ∪ T, S ∩ T = x
  - 훈련 세트 S를 통해 훈련된 모델은 테스트 세트 T를 활용해 오차를 측정하고 일반화 오차에 대한 추정치를 제공함
- 주의해야 할 것 : 훈련/ 테스트 세트를 나눌 때 되도록이면 데이터 분포가 같게 나눠야 한다는 것
  - 층화 추출법 : 양성/음성 샘플의 비중을 비슷하게 분류하는 것
  
### 2.2.2 교차 검증
- 교차 검증 
  - 데이터 세트 D를 k개의 서로소 집합으로 나누는 것으로 시작함
    - D = D<sub>1</sub> ∪ D<sub>2</sub> ∪ ... ∪ D<sub>k</sub>, D<sub>i</sub> ∩ D<sub>j</sub> = Φ (i ≠ j)
    - 매 부분집합 D<sub>i</sub>는 되도록 데이터 분포를 반영하도록 나눔
    - D로부터 층화 추출법을 통해 나누는 거
  - K-1개의 부분집합들을 훈련 세트로 사용하고 나머지 하나의 부분집합을 테스트 세트로 사용함
- K-겹 교차 검증(K-fold cross validation) : 교차 검증법을 통한 평가 결과의 안정성과 정확도는 k의 값에 따라 달라짐
  
- LOOCV(Leave-One-Out Cross Validation) : m개의 샘플이 있는 데이터 세트 D를 K=m으로 설정하고 교차 검증을 실행하는 것
  - 샘플 분류 방법에 대한 영향을 받지 않음
    - why? : m개의 샘플을 분류하는 방법은 m개의 부분집합을 만드는 것 밖에 없기 때문
    - LOOCV에 사용한 훈련 세트는 원본 데이터 세트와 비교할 때 1개의 샘플밖에 차이가 나질 않기 때문에 대부분 상황에서 LOOCV 방법을 활용한 모델은 모든 데이터 세트 D를 활용하여 훈련한 모델과 매우 비슷한 성능을 보임
      - LOOCV를 활용한 방법은 편향이 작음
  - 단점 : 데이터 세트의 크기가 매우 클 때 모델을 m번 적합해야 하므로 계산량이 많아짐
  
### 2.2.3 부트스트래핑
- 데이터 세트 D의 모든 데이터를 활용하여 훈련시킨 모델을 평가할 때 사용하는 해결책
- 부트스트래핑(bootstrapping) : 부트스트랩샘플링에 기반을 둔 샘플 추출 기법
  - m개의 샘플이 있는 데이터 세트 D를 가정한다면 우리는 샘플링을 통해 데이터 세트 D`를 만듦
  - 매번 D에서 샘플 하나를 꺼내 D`에 복사하여 넣음
  - 그리고 다시 원래의 데이터 세트 D로 돌려보냄
    - 한 번 뽑혔던 샘플도 다시 뽑힐 가능성이 있음
  - 이러한 과정을 m번 반복한 후, 우리는 m개의 샘플이 들어 있는 데이터 세트 D`를 얻음
    - 이것이 부트스트래핑의 결과
  - D의 샘플 중 일부는 D`에서 반복 출현함. 어떤 샘플은 아예 뽑히지 않을 수도 있음
  - m번의 채집 과정 중 샘플이 한 번도 뽑히지 않을 확률 : (1-1/m)m
    - 극한값을 계산 시

$$\lim_{m \to \infty} \(1-\frac{1}{m})^m = \frac{1}{e} ≒ 0.368 $$

- Out-of-Bag 예측
  - 부트스트래핑을 사용하면 데이터 세트 D 중의 36.8%의 샘플은 D`에 들어가지 못함
    - D`:훈련 세트
    - D/D`: 테스트 세트
    - m개의 샘플을 모두 활용하여 모델 훈련에 사용할 수 있고 활용하지 못한 1/3에 해당되는 샘플들은 테스트 샘플로 활용할 수 있음
- 부트스트래핑의 장점
  - 데이터 세트가 비교적 적거나 훈련/테스트 세트로 분류하기 힘들 때 사용하기 좋음
  - 초기 데이터 세트에서 다양한 훈련 세트를 여러 개 만들 수 있어서 앙상블 기법에 적용하기 좋음
- 부트스트래핑의 단점
  - 부트스트래핑을 통해 생성된 데이터 세트들은 초기 데이터의 분포와 다를 수 있어 편향을 크게 만들 수 있음
  - 이 때문에 초기 데이터 보유량이 충분할 때 ㄱ머증 세트 기법과 교차 검증 기법을 더 자주 활용함
  
### 2.2.4 파라미터 튜닝과 최종 모델
- 머신러닝의 파라미터 종류
  - 첫 번째, 알고리즘의 파라미터(하이퍼 파라미터) : 일반적으로 10개 이내
  - 두 번째, 모델의 파라미터 : 개수가 매우 많을 수도 있음
  - 튜닝 방식 : 전자와 후자 모두 비슷함
  - 선택 방법 : 여러 개의 모델을 생성한 후 모종의 평가 기준을 통해 선택
  - 다른 점
    - 전자 : 일반적으로 사람이 파라미터 개수를 선택하고 모델을 생성
    - 후자 : 학습을 통해 다수의 후보 모델을 만들어냄(ex) 딥러닝의 서로 다른 횟수의 학습 조기 종료와 같은 파라미터
- 파라미터 튜닝(parameter tuning)
  - 모델 평가 및 선택 시 학습 알고리즘의 선택뿐만 아니라 알고리즘 파라미터에 대한 설정도 고려해야 함
  - 주의해야 할 점
    - 학습 알고리즘의 많은 파라미터는 실수 범위의 값을 가진다는 것
    - '모든 파라미터'를 훈련을 통해 모델에 적합시키기는 어려움
    - 현실에서 사용되는 방법 : 파라미터의 범위와 변화 간격을 설정해 주는 것
      - 이러한 파라미터는 최적의 파라미터가 아닐 수 있음
      - 단지 계산량과 성능 예측을 위한 목표 사이에서 절충한 결과
- 학습한 모델의 실제 성능을 측정하기 위한 데이터는 테스트 데이터라는 점
- 검증 세트 : 모델 선택과 파라미터 조율을 위해, 테스트 데이터를 활용하여 성능을 측정하기 전에 검사할 수도 있는데 모델 평가 및 선택 과정에서 쓰는 평가 테스트 데이터 집합

- 테스트 세트 : 모델의 실제 성능을 예측
- 검증 세트 : 모델이나 파라미터 선택에 검증 세트로 활용

## 2.3 모델 성능 측정
- 성능 측정
  - 학습기의 일반화 성능에 대해 평가할 때 유효하고 실험 가능한 테스트 방법뿐만 아니라 모델의 일반화 성능을 평가할 기준이 있어야 함
  - 프로젝트 목적을 반영해야 함
- 예측을 위한 과제
  - 데이터 샘플 D = {(x<sub>1</sub>, y<sub>1</sub>), (x<sub>2</sub>, y<sub>2</sub>, ..., (x<sub>m</sub>, y<sub>m</sub>)}
  - y<sub>i</sub>는 x<sub>i</sub>의 정답 데이터
  - 학습기 f의 성능을 측정하려면 학습기의 예측 결과인 f(x)와 정답 데이터인 y를 비교해야 함
- 회귀분석에서 가장 자주 사용하는 성능 측정 방법 : 평균제곱오차(mean squared error)

$$E(f; D) = \frac{1}{m}\displaystyle\sum_{i=1}^{m} (f(x_{i})-y_{i})^2 $$

- 데이터 분포 D와 확률밀도 함수 p(·)로 표현 시

$$E(f; D) = \int\limits_x^D (f(x_{i})-y_{i})^2 p(x)dx $$

### 2.3.1 오차율과 정확도
- 분류 분석에서 가장 자주 사용하는 두 가지 성능 측도 : 오차율, 정확도
  - 오차율 : 모든 샘플 수에서 잘못 분류한 샘플 수가 차지하는 비율
  - 정확도 : 전체 샘플 수에서 정확히 분류한 샘플 수가 차지하는 비율
- 샘플 데이터 D에서 분류 오차율

$$E(f; D) = \frac{1}{m}\displaystyle\sum_{i=1}^{m} Ⅱ(f(x_{i})≠y_{i}) $$

- 정확도

$$acc(f; D) = \frac{1}{m}\displaystyle\sum_{i=1}^{m} Ⅱ(f(x_{i})=y_{i})= 1-E(f; D) $$

- 더 일반적으로 데이터 분포 D에 대해서 확률밀도 함수 p(·)로 나타내면 오차율과 정확도는 각각 다음과 같은 식으로 표현됨
- 오차율

$$E(f; D) = \int\limits_x^D Ⅱ(f(x)≠y)p(x)dx $$

- 정확도

$$acc(f; D) = \int\limits_x^D Ⅱ(f(x)=y)p(x)dx = 1-E(f; D) $$

### 2.3.2 재현율, 정밀도 그리고 F1 스코어
- 오차율과 정확도는 자주 사용되지만 모든 문제에 활용되진 못함
- 이진 분류 문제에서 실제 클래스와 학습기가 예측 분류한 클래스의 조합
  - 실제 양성(true positive), 거짓 양성(false positive), 실제 음성(true negative), 거짓 음성(false negative)
  - TP, FP, TN, FN
  - TP+FP+TN+FN=총 샘플 수
  - 분류 결과 혼동행렬
  
  |실제 값|예측 값||
  |-------|------|---|
  ||양성|음성|
  |양성|TP(실제 양성)|FN(거짓 음성)|
  |음성|FP(거짓 양성)|TN(실제 음성)|
  
  - 정밀도 $$P = \frac{TP}{TP+FP}$$
  - 재현율 $$R = \frac{TP}{TP+FN}$$

  - 정밀도와 재현율 사이에는 트레이드 오프가 존재함
- P-R, PR곡선, P-R 그래프(정밀도-재현율 곡선)
  - 학습기의 예측 결과에 따라 샘플을 내림차순으로 정렬
  - 가장 앞 순서에 있는 샘플들은 학습기가 양성 샘플(positive sample)로 분류할 가능성이 가장 큰 샘플
  - 가장 마지막에 위치한 샘플은 반대로 학습기가 양성 샘플로 분류할 가능성이 가장 작은 샘플
  - 위의 순서대로 모든 샘플이 양성값이라 가정하고 예측을 진행함
  - 매번 이에 해당하는 정밀도, 재현율을 계산할 수 있고 계산된 정밀도를 Y축에, 재현율을 X축에 둔 그래프를 그림
  - 두 지표를 비교할 때 만약 어떠한 학습기의 P-R곡선이 다른 학습기의 P-R곡선의 영역 내에 완전히 포함된다면 우리는 후자가 전자보다 학습 성능이 뛰어나다고 판단할 수 있음
- 손익분기점(break-even point, BEP)
  - '정밀도 = 재현율'일 때의 값을 나타냄
- F1 스코어
  - 재현율과 정밀도의 조화 평균
  - $$\frac{1}{F1} = \frac{1}{2}·(\frac{1}{P}+\frac{1}{R})$$
  - $$F1 = \frac{2×P×R}{P+R}=\frac{2×TP}{총 샘플 수+TP-TN}$$

- F1 스코어 일반 형식 : F<sub>β</sub>
  - 정밀도/재현율에 대한 서로 다른 선호도
  - $$\frac{1}{F_{β}} = \frac{1}{1+β^2}·(\frac{1}{P}+\frac{β^2}{R})$$
  - $$F_{β} = \frac{(1+β^2)×P×R}{(β^2×P)+R}$$
    - β>0 : 정밀도에 대한 재율의 상대적 중요도를 측정함
    - β=1 : 일반적인 F1 스코어가 됨
    - β>1 : 재현율의 영향력이 더 큼
    - β<1 : 정밀도의 영향력이 더 큼
- 한 번에 정밀도와 재현율을 평가할 수 있는 하나의 종합적인 혼동행렬을 얻고 싶어함
  - 한 가지 방법 : 각 혼동행렬에 대해 정밀도와 재현율을 계산하고 (P<sub>1</sub>, R<sub>1</sub>),(P<sub>2</sub>, R<sub>2</sub>), ..., (P<sub>n</sub>, R<sub>n</sub>)
  - 매크로 정밀도(macro-P), 매크로 재현율(macro-R) : 위의 평균값을 계산한 것
  - $$macro-P = \frac{1}{n}\displaystyle\sum_{i=1}^{n}P_{i}$$
  - $$macro-R = \frac{1}{n}\displaystyle\sum_{i=1}^{n}R_{i}$$
  - 매크로 F1(macro-F1) : 위의 방법으로 만든 F1 스코어
  - $$macro-F1 = \frac{2×macro-P×macro-R}{macro-P+macro-R}$$

- 마이크로 정밀도(micro-P), 마이크로 재현율(micro-R), 마이크로 F1(micro-F1) 스코어
  - 다른 방법으로는 각 혼동행렬이 대응하는 원소에 대한 평균을 내면 TP, FP, TN, FN의 평균값을 얻을 수 있고 이들을 각각 \overline{TP}, \overline{FP}, \overline{TN}, \overline{FN} 으로 기록하여 나타내고 평균값을 계산함
  - $$micro-P = \frac{\overline{TP}}{\overline{TP}+\overline{FP}}$$
  - $$micro-R = \frac{\overline{TP}}{\overline{TP}+\overline{FN}}$$
  - $$micro-F1 = \frac{2×micro-P×micro-R}{micro-P+micro-R}$$

### 2.3.3 ROC와 AUC
- 많은 학습기가 테스트 세트를 위해 실숫값 혹은 확률 예측값을 계산해 냄
  - 해당 예측값과 하나의 분류 임계치를 비교함
  - 만약 임계치보다 크면 양성값(positive value), 작으면 음성값(negative value)으로 분류함
  - 차단점(cut point) : 양성 값으로 분류될 '가능성'이 가장 큰 샘플이 가장 앞으로 '가능성'이 가장 작은 샘플은 가장 뒤에 놓음
    - 해당 분류 과정은 해당 순서를 기준으로 샘플을 양분하는 과정과 유사함
- ROC(수신기 조작 특성)(Receiver Operating Characteristic)
  - 학습기의 일반화 성능을 연구하는 좋은 도구
  - 학습기의 예측 결과를 기반으로 샘플에 대해 순서를 매기고 해당 순서에 따라 샘플이 양성값이 될 확률을 계산함
  - TPR과 FPR값을 계산하여 x축과 y축에 그려 넣으면 ROC 곡선이 완성됨
  - P-R 곡선 : 재현율과 정밀도를 각각 x와 y축에 놓음
  - ROC 곡선 : 세로축(y축)은 참 양성률(True Positive Rate, TPR), 가로축(x축)은 거짓 양성률(False Positive Rate, FPR)
    - $$TPR = \frac{TP}{TP+FN}$$
    - $$FPR = \frac{FP}{TN+FP}$$
- 학습기를 비교할 때 P-R 그래프와 마찬가지로 만약 어떤 한 학습기의 ROC 곡선이 다른 하나의 ROC 곡선에 완전히 '포함'되는 경우, 후자가 전자보다 우수한 성능을 가진 학습기라고 판단할 수 있음
- 두 학습기 ROC 곡선에 교차가 발생한다면 한눈에 우열을 가리기 힘든 경우가 많음
  - 이런 상황에서 비교적 합리적인 판단 방법은 ROC 곡선 아래의 면적을 비교하는 것 -> AUC

- AUC(Area Under ROC Curve)
  - ROC 곡선 아래 각 부분의 면적을 모두 더해 구할 수 있음
  - 만약 ROC 곡선이 각 좌표 {(x<sub>1</sub>, y<sub>1</sub>), (x<sub>2</sub>, y<sub>2</sub>), ..., (x<sub>m</sub>, y<sub>m</sub>)}의 점들을 이어 (x<sub>1</sub> = 0, x<sub>m</sub> = 1)로 만들어졌다고 가정한다면 AUC는 다음과 같이 계산될 수 있음
    - $$AUC = \frac{1}{2}\displaystyle\sum_{i=1}^{m-1}(x_{i+1}-x_{i})·(y_{i}+y{i+1})$$
  - AUC가 고려하는 것 : 샘플 예측의 배열 순서 품질임
    - 순서 오차와 긴밀한 관계가 있음
- $m^+$ 개의 양성값과 $m^-$ 개의 음성값이 있고 $D^+$와 $D^-$로 양성값, 음성값의 집합을 나타내면, 순서의 손실(loss)은 다음과 같이 정의됨
  - $$l_{rank} = \frac{1}{m^+ +m^-} \displaystyle\sum_{x^+ ∈ D^+} \displaystyle\sum_{x^- ∈ D^-}\Bigg(Ⅱ(f(x^+)<f(x^-))+\frac{1}{2}Ⅱ(f(x^+)=f(x^-))\Bigg) $$

- $l_{rank}$ : ROC 곡선 위의 면적으로서 만약 하나의 양성값이 ROC 곡선 위에 대응하는 점 (x,y)라고 한다면 x는 그 앞에 얼마만큼의 음성값이 있는지를 나타낸다고 볼 수 있음
  - 즉, 거짓 양성률을 나타냄
  - AUC = 1 - $l_{rank}$


### 2.3.4 비용민감 오차율과 비용 곡선
- 비균등비용(unequal cost) : 서로 다른 종류의 오차가 일으키는 서로 다른 종류의 비용에 대한 균형을 맞추는 것

## 2.4 비교 검증
- 성능 비교에 중요한 몇 가지 요소
  1. 우리가 비교하고자 하는 것은 학습기의 일반화 성능
     - 소개한 평가 방법들로 테스트 데이터 세트상에서 성능을 얻을 수 있지만 일반화 성능과 늘 일치하진 않음
  2. 테스트 세트 상에서 성능은 테스트 세트 그 자체와 큰 상관관계가 있음
     - 다른 크기의 테스트 세트를 사용한다면 테스트 결과는 달라질 것
     - 같은 크기의 테스트 데이터 세트를 사용한다고 하더라도 어떠한 데이터를 포함하고 있느냐에 따라 결괏값이 달라질 수도 있음
  3. 모든 학습기는 자체적으로 일종의 무작위성을 포함하고 있음
     - 파라미터를 똑같이 설정하여 테스트 세트에서 실험하더라도 다른 결과를 얻을 수 있음

- 통계가설 검정(hypothesis test) 
  - 학습기 성능을 비교하는 데 중요한 근거를 제공함
  - 가설 검정 결과에 기초하여 학습기 A가 B보다 테스트 세트상에서 성능이 좋다면 A의 일반화 성능이 통계적으로도 B보다 좋은 것인지 그리고 해당 판단이 어느 정도로 정확한지 판단할 수 있게 도와줌
  - 
