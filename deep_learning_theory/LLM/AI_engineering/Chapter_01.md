# 1장. 파운데이션 모델을 활용한 AI 애플리케이션 입문
- 2020년 이후의 AI를 한 단어로 표현 시 => '규모'
  - 챗GPT, 구글의 제미나이, 미드저니 같은 애플레이션의 AI 모델들은 너무 거대해져서 이미 상당한 양의 전기를 사용
  - 이들을 학습시키기 위한 인터넷의 공개 데이터도 부족해질 위험에 처해 있음

- AI 모델의 규모가 커지면서 두 가지 중요한 현상
  - (1) AI 모델이 더욱 강력해지고 더 많은 작업을 수행할 수 있게 되어 더 많은 애플리케이션을 개발할 수 있게 되었음
    - 많은 사람과 팀이 생산성을 높이고 경제적 가치를 창출하며 삶의 질을 향상시키기 위해 AI를 활용함
  - (2) 대규모 언어 모델을 학습하려면 데이터, 컴퓨팅 자원, 전문 인력이 필요하고 이는 소수의 조직(거대 기업)만이 감당할 수 있음
    - 소수의 거대 조직이 개발한 모델을 다른 사람들이 서비스로 이용할 수 있게 되었음
    - AI를 활용해 애플리케이션을 만들고 싶은 사람이라면 처음부터 모델 개발에 투자하지 않아도 개발된 모델들을 바로 사용할 수 있음
  - AI 애플리케이션에 대한 수요는 증가, AI 애플리케이션을 만드는 진입 장벽은 낮아짐
  - **AI 엔지니어링** : 쉽게 사용할 수 있는 모델들을 기반으로 애플리케이션을 만드는 과정 
    - ML모델을 기반으로 애플리케이션을 만드는 것은 새로운 일이 아님
- AI 엔지니어링의 폭발적인 성장의 원동력인 파운데이션 모델의 개요
  - 다양한 성공적인 AI 활용 사례를 논의하고 AI가 잘하는 것과 아직 부족한 것을 설명함

## 1.1 AI 엔지니어링의 부상
- 파운데이션 모델 : 대규모 언어 모델에서 나옴 => 단순한 언어모델에서 비롯됨
- 최초의 언어 모델 : 1950년대에 등장
- 언어 모델이 AI 엔지니어링으로 진화하는 데 핵심이 된 중요한 발전 위주

### 1.1.1 언어 모델에서 대규모 언어 모델로
- 자기 지도 학습 덕분에 오늘날과 같은 규모로 성장할 수 있었음
- 언어 모델과 자기 지도 학습이 무엇을 의미하는지?

#### 언어 모델
- 언어 모델 : 하나 이상의 언어에 대한 통계 정보를 인코딩함
  - 주어진 컨텍스트에서 어던 단어가 나타날 것 같은지를 알려줌
    - EX) '내가 가장 좋아하는 색상은 __'라는 컨텍스트가 주어졌을 때, 언어 모델은 파란색을 더 자주 예측할 것
- 언어의 통계적 특성은 이미 오래 전에 발견되었음
  - 1905년 작품인 춤추는 사람들에서 려록 홈즈는 영어의 간단한 통계적 정보를 활용해 수수께끼 같은 막대 모양의 그림들을 해독했음
  - 클로드 새넌 : 영어를 모델링하는 방법의 연구 => 1951년 Prediction and Entropy of Printed English 발표
    - 엔트로피를 비롯한 여러 개념이 오늘날에도 언어 모델링에 사용되고 있음
- 언어 모델의 기본 단위 : 토큰
  - 토큰은 모델에 따라 문자, 단어 또는 단어의 일부(-tion과 같은)가 될 수 있음
    - 영어 이외의 언어는 단일 유니코드 문자가 여러 개의 토큰으로 표시될 수 있음
    - GPT-4의 토큰 나누는 예시
<img width="686" height="57" alt="image" src="https://github.com/user-attachments/assets/6a2199e1-d11c-440f-b725-67a964169f44" />

- 토큰화 : 원문을 모델이 정한 길이로 나누는 과정
  - GPT-4의 경우, 토큰 하나의 평균 길이는 단어의 약 3/4 정도
  - 100 토큰은 약 75개의 단어에 해당함
- 어휘 : 모델이 다룰 수 있는 모든 토큰의 집합
  - 알파벳의 몇 글자를 사용해 많은 단어를 만들 수 있는 것처럼, 소수의 토큰만 사용해 많은 고유한 단어를 만들 수 있음
- 언어 모델의 두 가지 유형(토큰을 예측할 때 사용할 수 있는 정보에 따라 구분됨)
- 


##### 언어 모델은 왜 단어나 문장이 아닌 토큰을 사용할까?
(1) 문자에 비해 토큰은 단어를 의미 있는 구성 요소로 나눌 수 있음
- '요리하기'는 '요리'와 '하기'로 나눌 수 있음 => 두 구ㅅ어 요소 모두 원래 단어의 일부 의미를 담고 있음
(2) 고유한 토큰의 수가 고유한 단어의 수보다 적기 때문에 모델의 어휘 크기가 줄어들어 모델이 더 효율적이게 됨
(3) 토큰은 모델이 알려지지 않은 단어를 처리할 때도 도움을 줌
- EX) 'chatgpting'이라는 단어는 'chatgpt'와 '-ing'로 나눌 수 있어 모델이 그 구조를 이해하는 데 도움이 됨
- 토큰은 단어보다 단위가 적으면서도 개별 문자보다 더 많은 의미를 유지함
