# LLM 기초
## RNN에서 트랜스포머 아키텍처로
- 딥러닝이나 머신러닝 분야에서 텍스트의 정의 : 단어가 연결된 문장 형태의 데이터
  - 시퀀스 : 작은 단위(단어)의 데이터가 연결되고 그 길이가 다양한 데이터의 형태
  - 시퀀스 데이터를 처리하기 위해 크게 순환신경망(RNN)이나 트랜스포머의 두 가지 아키텍처로 대표되는 다양한 모델을 사용해 왔음
### RNN
- 트랜스포머가 개발되기 전에 활용하여 텍스트를 생성
- 입력하는 텍스트를 순차적으로 처리해서 다음 단어를 예측함
- 특징 : 모델이 하나의 잠재 상태에 지금까지의 입력 텍스트의 맥락을 압축한다는 점
  - 위 방식의 장점
    - 여러 단어로 구성된 맥락을 하나의 잠재 상태에 압축하기 때문에 메모리를 적게 사용함
    - 다음 단어를 예측할 때 지금까지 계산을 통해 만들어진 잠재 상태와 입력 단어만 있으면 되기 때문에 다음 단어를 빠르게 생성할 수 있음
  - 단점
    - 순차적으로 입력되는 단어를 하나의 잠재 상태에 압축하다 보니 먼저 입력한 단어의 의미가 점차 희석됨
      - 학습 속도가 느림
    - 입력이 길어지는 경우 의미를 충분히 담지 못하고 성능이 떨어짐
    - 성능을 높이기 위해 층을 깊이 쌓으면 그레이디언트 소실이나 그레이디언트 증폭이 발생하며 학습이 불안정함

### Transformer(트랜스포머)
- 순차적인 처리 방식을 버리고 맥락을 모두 참조하는 어텐션 연산을 사용
- 셀트 어텐션(self-attention) : 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산하여 각 단어의 표현을 조정하는 역할
- 장점
  - 확장성 : 더 깊은 모델을 만들어도 학습이 잘 됨, 동일한 블록을 반복해 사용하기 때문에 확장이 용이함
  - 효율성 : 학습할 때 병렬 연산이 가능하기 때문에 학습 시간이 단축됨
  - 더 긴 입력 처리 : 입력이 길어져도 성능이 거의 떨어지지 않음
- 트랜스포머 아키텍처가 있었기 때문에 대규모 언어 모델이 가능했다고 할 수 있음

![image](https://github.com/user-attachments/assets/78bc6a62-e192-4efe-b679-06c98724eadb)

- 트랜스포머 아키텍처
  - 인코더 : 그림의 왼쪽 상자는 언어를 이해하는 역할
    - 층 정규화, 멀티 헤드 어텐션, 피드 포워드 층을 거치며 영어 문장을 이해하고 그 결과를 그림 중간의 선에 나타나듯이 디코더로 전달함
  - 디코더 : 언어를 생성하는 역할
    - 인코더에서와 유사하게 층 정규화, 멀티 헤드 어텐션 연산을 수행하면서 크로스 어텐션 연산을 통해 인코더가 전달한 데이터를 출력과 함께 종합해서 피드 포워드 층을 거쳐 한국어 번역 결과를 생성함
- 공통
  - 입력을 임베딩 층을 통해 숫자 집합인 임베딩으로 변환 위치 인코딩 층에서 문장의 위치 정보를 더함
  - 인코더에서 
  - 


## BERT, GPT, T5 등 트랜스포머를 활용한 아키텍처
- 트랜스포머 아키텍처 : 인코더와 디코더로 이뤄졌음
- 트랜스포머 아키텍처를 활용한 모델 3가지

|모델 그룹|대표 모델|장점|단점|
|---------|-------|---|----|
|인코더|구글의 Bert|(1)양방향 이해를 통해 자연어 이해에서 일반적으로 디코더 모델 대비 높은 성능을 보임 <br> (2)입력에 대해 병렬 연산이 가능하므로 빠른 학습과 추론이 가능 <br> (3)다양한 작업에 대한 다운스트림 성능이 뛰어남|(1) 자연어 생성 작업에 부적합한 형태 <br> (2) 컨텍스트의 길이가 제한적임|
|디코더|Open AI의 GPT|(1) 생성 작업에서 뛰어난 성능을 보임 <br> (2) 비교적 긴 컨텍스트 길이에 대해서도 성능이 좋음|(1) 양방향이 아닌 단방향 방식이므로 자연어 이해 작업에서 비교적 성능이 낮음 <br> (2) 모든 작업을 생성 작업으로 변환할 수 있으나 비효율적일 수 있음|
|인코더-디코더|메타의 BART, 구글의 T5|(1) 생성과 이해 작업 모두에서 뛰어난 성능을 보임 <br> (2) 이해 작업에서 양방향 방식을 사용할 수 있고 인코더의 결과를 디코더에서 활용할 수 있어 문맥을 반영한 생성 능력이 뛰어남|(1)인코더와 디코더를 모두 활용하기 때문에 더 복잡함 <br> (2) 학습에 더 많은 데이터와 컴퓨팅 자원이 필요함|
