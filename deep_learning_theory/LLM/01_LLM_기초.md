# LLM 기초
## RNN에서 트랜스포머 아키텍처로
- 딥러닝이나 머신러닝 분야에서 텍스트의 정의 : 단어가 연결된 문장 형태의 데이터
  - 시퀀스 : 작은 단위(단어)의 데이터가 연결되고 그 길이가 다양한 데이터의 형태
  - 시퀀스 데이터를 처리하기 위해 크게 순환신경망(RNN)이나 트랜스포머의 두 가지 아키텍처로 대표되는 다양한 모델을 사용해 왔음
### RNN
- 트랜스포머가 개발되기 전에 활용하여 텍스트를 생성
- 입력하는 텍스트를 순차적으로 처리해서 다음 단어를 예측함
- 특징 : 모델이 하나의 잠재 상태에 지금까지의 입력 텍스트의 맥락을 압축한다는 점
  - 위 방식의 장점
    - 여러 단어로 구성된 맥락을 하나의 잠재 상태에 압축하기 때문에 메모리를 적게 사용함
    - 다음 단어를 예측할 때 지금까지 계산을 통해 만들어진 잠재 상태와 입력 단어만 있으면 되기 때문에 다음 단어를 빠르게 생성할 수 있음
  - 단점
    - 순차적으로 입력되는 단어를 하나의 잠재 상태에 압축하다 보니 먼저 입력한 단어의 의미가 점차 희석됨
    - 입력이 길어지는 경우 의미를 충분히 담지 못하고 성능이 떨어짐

### Transformer(트랜스포머)
- 순차적인 처리 방식을 버리고 맥락을 모두 참조하는 어텐션 연산을 사용



## BERT, GPT, T5 등 트랜스포머를 활용한 아키텍처
- 트랜스포머 아키텍처 : 인코더와 디코더로 이뤄졌음
- 트랜스포머 아키텍처를 활용한 모델 3가지

|모델 그룹|대표 모델|장점|단점|
|---------|-------|---|----|
|인코더|구글의 Bert|(1)양방향 이해를 통해 자연어 이해에서 일반적으로 디코더 모델 대비 높은 성능을 보임 <br> (2)입력에 대해 병렬 연산이 가능하므로 빠른 학습과 추론이 가능 <br> (3)다양한 작업에 대한 다운스트림 성능이 뛰어남|(1) 자연어 생성 작업에 부적합한 형태 <br> (2) 컨텍스트의 길이가 제한적임|
|디코더|Open AI의 GPT|(1) 생성 작업에서 뛰어난 성능을 보임 <br> (2) 비교적 긴 컨텍스트 길이에 대해서도 성능이 좋음|(1) 양방향이 아닌 단방향 방식이므로 자연어 이해 작업에서 비교적 성능이 낮음 <br> (2) 모든 작업을 생성 작업으로 변환할 수 있으나 비효율적일 수 있음|
|인코더-디코더|메타의 BART, 구글의 T5|(1) 생성과 이해 작업 모두에서 뛰어난 성능을 보임 <br> (2) 이해 작업에서 양방향 방식을 사용할 수 있고 인코더의 결과를 디코더에서 활용할 수 있어 문맥을 반영한 생성 능력이 뛰어남|(1)인코더와 디코더를 모두 활용하기 때문에 더 복잡함 <br> (2) 학습에 더 많은 데이터와 컴퓨팅 자원이 필요함|
