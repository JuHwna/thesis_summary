# LLM 기초
## RNN에서 트랜스포머 아키텍처로
- 딥러닝이나 머신러닝 분야에서 텍스트의 정의 : 단어가 연결된 문장 형태의 데이터
  - 시퀀스 : 작은 단위(단어)의 데이터가 연결되고 그 길이가 다양한 데이터의 형태
  - 시퀀스 데이터를 처리하기 위해 크게 순환신경망(RNN)이나 트랜스포머의 두 가지 아키텍처로 대표되는 다양한 모델을 사용해 왔음
### RNN
- 트랜스포머가 개발되기 전에 활용하여 텍스트를 생성
- 입력하는 텍스트를 순차적으로 처리해서 다음 단어를 예측함
- 특징 : 모델이 하나의 잠재 상태에 지금까지의 입력 텍스트의 맥락을 압축한다는 점
  - 위 방식의 장점
    - 여러 단어로 구성된 맥락을 하나의 잠재 상태에 압축하기 때문에 메모리를 적게 사용함
    - 다음 단어를 예측할 때 지금까지 계산을 통해 만들어진 잠재 상태와 입력 단어만 있으면 되기 때문에 다음 단어를 빠르게 생성할 수 있음
  - 단점
    - 순차적으로 입력되는 단어를 하나의 잠재 상태에 압축하다 보니 먼저 입력한 단어의 의미가 점차 희석됨
      - 학습 속도가 느림
    - 입력이 길어지는 경우 의미를 충분히 담지 못하고 성능이 떨어짐
    - 성능을 높이기 위해 층을 깊이 쌓으면 그레이디언트 소실이나 그레이디언트 증폭이 발생하며 학습이 불안정함

### Transformer(트랜스포머)
- 순차적인 처리 방식을 버리고 맥락을 모두 참조하는 어텐션 연산을 사용
- 셀트 어텐션(self-attention) : 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산하여 각 단어의 표현을 조정하는 역할
- 장점
  - 확장성 : 더 깊은 모델을 만들어도 학습이 잘 됨, 동일한 블록을 반복해 사용하기 때문에 확장이 용이함
  - 효율성 : 학습할 때 병렬 연산이 가능하기 때문에 학습 시간이 단축됨
  - 더 긴 입력 처리 : 입력이 길어져도 성능이 거의 떨어지지 않음
- 트랜스포머 아키텍처가 있었기 때문에 대규모 언어 모델이 가능했다고 할 수 있음

![image](https://github.com/user-attachments/assets/78bc6a62-e192-4efe-b679-06c98724eadb)

- 트랜스포머 아키텍처
  - 인코더 : 그림의 왼쪽 상자는 언어를 이해하는 역할
    - 층 정규화, 멀티 헤드 어텐션, 피드 포워드 층을 거치며 영어 문장을 이해하고 그 결과를 그림 중간의 선에 나타나듯이 디코더로 전달함
  - 디코더 : 언어를 생성하는 역할
    - 인코더에서와 유사하게 층 정규화, 멀티 헤드 어텐션 연산을 수행하면서 크로스 어텐션 연산을 통해 인코더가 전달한 데이터를 출력과 함께 종합해서 피드 포워드 층을 거쳐 한국어 번역 결과를 생성함
- 공통
  - 입력을 임베딩 층을 통해 숫자 집합인 임베딩으로 변환 위치 인코딩 층에서 문장의 위치 정보를 더함

- 텍스트를 임베딩으로 변환하기
  - 텍스트를 숫자 형식의 데이터로 변경, 텍스트를 모델에 입력할 수 있는 숫자형 데이터인 임베딩으로 변환하기 위해선 세 가지 과정을 거쳐야 함
    - (1) 토큰화 : 텍스트를 적절한 단위로 잘라 숫자형 아이디를 부여하는 토큰화 수행
    - (2) 토큰 임베딩 : 토큰 아이디를 토큰 임베딩 층을 여러 숫자의 집합인 토큰 임베딩으로 변환
    - (3) 위치 인코딩 층을 통해 토큰의 위치 정보를 담고 있는 위치 임베딩 추가=> 최종적으로 모델에 입력할 임베딩을 만듦

  - 토큰화
    - 토큰화 : 텍스틀 적절한 단위로 나누고 숫자 아이디를 부여하는 것
      - 한글의 경우, 작게는 자모(자음과 모음) 단위부터 크게는 단어 단위로 나눌 수 있음(음절은 중간 단위)
    - 토큰화할 때 어떤 토큰이 어떤 숫자 아아디로 연결됐는지 기록해 둔 **사전**을 만들어야 함
      - 큰 단위를 기준으로 토큰화할수록
        - 장점 : 텍스트의 의미가 잘 유지됨
        - 단점 : 사전의 크기가 커짐, 이전에 본 적이 없는 새로운 단어는 사전에 없기 때문에 처리하지 못하는 OOV(사전에 없는 단어) 문제 자주 발생
      - 작은 단위로 토큰화
        - 장점 : 사전의 크기 작아짐, OOV 문제를 줄일 수 있음
        - 단점 : 텍스트의 의미가 유지되지 않음
    - 서브워드 토큰화
      - 작은 단위와 큰 단위 모두 각각의 장단점이 뚜렷함
      - 최근에 데이터에 등장하는 빈도에 따라 토큰화 단위를 결정
      - 서브워드 토큰화 방식
        - 자주 나오는 단어는 단어 단위 그대로 유지, 가끔 나오는 단어는 더 작은 단위로 나눠 텍스트의 의미를 최대한 유지하면서 사전의 크기는 작고 효율적으로 유지할 수 있음
      - 한글의 경우, 보통 음절과 단어 사이에서 토큰화됨

    - 토큰화 코드
      ~~~
      #띄어쓰기 단위로 분리
      input_text = "나는 최근 파리 여행을 다녀왔다"
      input_text_list=input_text.split()


      #토큰 => 아이디 딕셔너리와 아이디 => 토큰 딕셔너리 만들기
      str2idx={word:idx for idx, word in enumerate(input_text_list)}
      idx2str={idx:word for idx, word in enumerate(input_text_list)}

      #토큰을 토큰 아이디로 변환
      input_ids=[str2idx[word] for word in input_text_list]

      ~~~
      
  - 토큰 임베딩으로 변환하기
    - 임베딩 : 데이터를 의미를 담아 숫자 집합으로 변환하는 것
    - 토큰을 임베딩으로 변환하기 때문에 토큰 임베딩이라고 부름
      - 딥러닝 모델이 텍스트 데이터를 처리하기 위해서는 입력으로 들어오는 토큰과 토큰 사이의 관계를 계산할 수 있어야 함
      - 토큰과 토큰 사이의 관계를 계산하기 위해서는 토큰의 의미를 숫자로 나타낼 수 잇어야 함
      - 앞서 토큰화에서 부여한 토큰 아이디는 하나의 숫자일 뿐, 토큰의 의미를 담을 수 없어 의미를 담기 위해서 최소 2개 이상의 숫자 집합인 **벡터**여야 함
    - 파이토치가 제공하는 nn.Embedding 클래스를 사용하면 토큰 아이디를 토큰 임베딩으로 변환할 수 있음
      - 텐서플로우는 tf.keras.layers.Embedding

    ~~~
    embedding_dim=16
    embed_layer = nn.Embedding(len(str2idx),embedding_dim)

    input_embeddings = embed_layer(torch.tensor(input_ids)) #(5,16)
    input_embeddings = input_embeddings.unsqueeze(0) #(1,5,16)
    ~~~

    - 위의 코드에서 임베딩 층은 토큰의 의미를 담아 벡터로 변환하지 않음
    - 지금의 임베딩 층은 그저 입력 토큰 아이디(input_ids)를 16차원의 임의의 숫자 집합으로 바꿔줄 뿐임
    - 임베딩 층이 단어의 의미를 담기 위해서는 딥러닝 모델이 학습 데이터로 훈련되어야 함.
    
    
## BERT, GPT, T5 등 트랜스포머를 활용한 아키텍처
- 트랜스포머 아키텍처 : 인코더와 디코더로 이뤄졌음
- 트랜스포머 아키텍처를 활용한 모델 3가지

|모델 그룹|대표 모델|장점|단점|
|---------|-------|---|----|
|인코더|구글의 Bert|(1)양방향 이해를 통해 자연어 이해에서 일반적으로 디코더 모델 대비 높은 성능을 보임 <br> (2)입력에 대해 병렬 연산이 가능하므로 빠른 학습과 추론이 가능 <br> (3)다양한 작업에 대한 다운스트림 성능이 뛰어남|(1) 자연어 생성 작업에 부적합한 형태 <br> (2) 컨텍스트의 길이가 제한적임|
|디코더|Open AI의 GPT|(1) 생성 작업에서 뛰어난 성능을 보임 <br> (2) 비교적 긴 컨텍스트 길이에 대해서도 성능이 좋음|(1) 양방향이 아닌 단방향 방식이므로 자연어 이해 작업에서 비교적 성능이 낮음 <br> (2) 모든 작업을 생성 작업으로 변환할 수 있으나 비효율적일 수 있음|
|인코더-디코더|메타의 BART, 구글의 T5|(1) 생성과 이해 작업 모두에서 뛰어난 성능을 보임 <br> (2) 이해 작업에서 양방향 방식을 사용할 수 있고 인코더의 결과를 디코더에서 활용할 수 있어 문맥을 반영한 생성 능력이 뛰어남|(1)인코더와 디코더를 모두 활용하기 때문에 더 복잡함 <br> (2) 학습에 더 많은 데이터와 컴퓨팅 자원이 필요함|
