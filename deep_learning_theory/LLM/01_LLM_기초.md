# LLM 기초
## RNN에서 트랜스포머 아키텍처로
- 딥러닝이나 머신러닝 분야에서 텍스트의 정의 : 단어가 연결된 문장 형태의 데이터
  - 시퀀스 : 작은 단위(단어)의 데이터가 연결되고 그 길이가 다양한 데이터의 형태
  - 시퀀스 데이터를 처리하기 위해 크게 순환신경망(RNN)이나 트랜스포머의 두 가지 아키텍처로 대표되는 다양한 모델을 사용해 왔음
### RNN
- 트랜스포머가 개발되기 전에 활용하여 텍스트를 생성
- 입력하는 텍스트를 순차적으로 처리해서 다음 단어를 예측함
- 특징 : 모델이 하나의 잠재 상태에 지금까지의 입력 텍스트의 맥락을 압축한다는 점
  - 위 방식의 장점
    - 여러 단어로 구성된 맥락을 하나의 잠재 상태에 압축하기 때문에 메모리를 적게 사용함
    - 다음 단어를 예측할 때 지금까지 계산을 통해 만들어진 잠재 상태와 입력 단어만 있으면 되기 때문에 다음 단어를 빠르게 생성할 수 있음
  - 단점
    - 순차적으로 입력되는 단어를 하나의 잠재 상태에 압축하다 보니 먼저 입력한 단어의 의미가 점차 희석됨
    - 입력이 길어지는 경우 의미를 충분히 담지 못하고 성능이 떨어짐

### Transformer(트랜스포머)
- 순차적인 처리 방식을 버리고 맥락을 모두 참조하는 어텐션 연산을 사용
- 
