# Chapter 01. LLM API의 기초
- 오픈 ai의 에이전트 SDK처럼 직접 AI 에이전트 개발을 지원한는 경우도 있지만 대부분 경우 기존 API 위에 레이어를 한층 쌓아올린 형태가 되기 때문임

## 1.1 LLM API를 왜 사용해야 하는가?
- LLM API : 대규모 언어 모델에 프롬프트와 파라미터를 주고 추론하게 하는 API
  - 대표적 : 오픈 AI의 API, 클로드의 API, 제미나이 API 등

- 번거롭게 API를 사용해서 LLM으로 추론을 하여 답변을 받도록 하는 근본적인 이유 : AI를 서비스에 적용하기 위해서
  - 만드는 서비스의 사용자가 직접 타이핑을 해서 챗GPT에 프롬프트를 입력하도록 할 수는 없기 때문임
- LLM API를 사용해야만 서비스에 적용할 수 있음
  - LLM이 가지고 있는 능려과 기능의 최대치를 끌어내는 것이 목표라면 LLM API를 사용해야 함
 

## 1.2 LLM API의 기본적인 사용법
- 오픈 AI와 클로드 둘 다 사용법이 크게 다르지 않음

### 1.2.1 모델별 특징을 잘 이해하고 사용하자
- LLM API 사용 시에 주의할 점 중 하나는 모델의 특징을 잘 이해하고 사용해야 한다는 점
- 모델의 특징을 알아야 하는 이유
  - 비용
  - 컨텍스트 윈도우 크기, 응답 속도, 특정 태스크에서의 정확도, 프롬프트 엔지니어링 방식 등이 모델마다 다름
  - 위의 특징을 이해해야 최상의 사용자 경험을 제공할 수 있음
 
### 1.2.2 오픈 AI API의 사용법
- 오픈 AI의 API를 사용하기 위해 REST API를 사용하는 방법도 있음
- 파이썬용 SDK를 사용하는 것이 가장 편함(pip install openai로 쉽게 설치 가능)
- 오픈 AI의 API에는 텍스트 생성 관련된 API가 꽤 다양한 버전으로 있음
  - Chat Compeletion(챗 컴플리션)API : 오픈 AI의 모델에 단순한 질의를 할 때 좋은 API
    - 장점 : 기능이 단순하기 때문에 구현이 쉬움
    - 단점 : 이전 대화를 기억하지 못하는 단점이 있음
  - Assistant API(어시스턴트 API) : 이전의 대화를 기억하게 하는 기능인 대화 상태 관리 기능을 가지고 있는 API
    - 2026년 상반기에 폐지될 예쩡
  - Response API(리스폰스 API) : 어시스턴트 API의 기능을 이어 받음
    - 챗 컴플리션과 어시스턴트 API의 기능 통합
    - 현재 사용 가능하며 AI 에이전트 구축의 표준 API로 자리 잡고 있음


#### 리스폰스 API
- 가장 최근에 발표된 API
  - 기존의 챗 컴플리션 API와 사용하는 방법이 거의 비슷하고 기능이 더 많음
- 특징
  - LLM이 도구를 사용할 수 있다는 점
  - 기본 내장된 도구는 웹 검색, 파일 검색이 있음.
 
### 1.2.3 앤트로픽 API 사용해보기
- 오픈 AI : 대부분의 작업에 적절한 성능을 내어주고 이미지 생성이나 벡터 서치 등 텍스트 생성 이외에도 좋은 편의 기능이 많이 있음
  - But, 한국어의 유창함은 약간 떨어짐
  - 최신 모델이 아닌 때는 꽤 수다스러워서 간단한 질문에도 대부분 많은 응답을 줌
- 클로드 : 매우 자연스러운 대화를 생성함
  - 한국어도 매우 잘해서 응답 결과에 유창한 한국어가 필요하다면 꼭 써야 함
  - 안전하고 윤리적인 AI시스템을 개발하려고 매우 노력을 했기에 안전성이 높은 답변을 하는 편임

- 클로드 사용 시 : anthropic 패키지 설치해야 함(pip install anthropic)
  - 앤트로픽 api 사용법 : 오픈 AI의 챗 컴플리션 API와 거의 같음

## 1.3 스트리밍 처리
- LLM API의 요청에 대한 응답 형태
  - 요청에 대한 응답을 한 번에 반환하는 방법 : 이미 이전 예제들
  - 결괏값을 계속해서 흘려보내는 방식 => 스트리밍 방식
    - LLM의 응답은 오래 걸리거나 길게 나오는 경우가 많으므로 상호작용이 필요한 서비스에서 사용하면 좋음

### 1.3.1 오픈 AI의 스트리밍 처리
- 오픈 AI는 대부분의 모델이 스트리밍 옵션을 지원함
  - 기존 코드에 stream=True만 추가하면 요청에 대한 결과가 즉시 토큰 단위로 반환됨

### 1.3.2 앤트로픽의 스트리밍 처리
- 앤트로픽 API는 stream 메서드를 별도로 제공하고 with를 사용한 컨텍스트 매니저 기능도 제공함

## 1.4 비동기 처리 및 오류 핸들링
- LLM API는 기존의 API 개발과 확연하게 다른 점
  - 최종 결과를 받기 전까지 시간이 많이 걸린다는 점
  - 단순히 느리다면 기다리면 됨
    - But, 요청이 연속적으로 발생한다고 가정할 때, 이전 요청이 끝나지 않았따면 뒤에 온 요청은 마냥 기다려야 한다는 점
- LLM API를 혼자서 사용하는 것이 아니라 서비스에 사용할 생각이라면 반드시 **비동기 처리**를 해야 함

- LLM API를 제공하는 오픈 AI나 앤트로픽 API가 장애 상태에 빠지거나 유독 응답이 느려지는 경우도 있음
  - 이런 상황을 방지하기 위해 재시도 로직을 넣어주어야 함

### 1.4.1 async await를 사용하여 비동기 처리하기
- 파이썬 : async await 키워드를 사용하여 비동기 함수를 만들 수 있음
  - 사용법 : def 앞에 async를 넣고 다른 비동기 함수에서 결괏값을 받는 부분에 await 키워드를 붙이는 거
    - 함수 선언 시 : async def [함수명](파라미터):
    - 이 함수를 사용하는 곳 : await [함수명]
      - await는 함수 내에서만 사용할 수 있음
     
### 1.4.2 간헐적으로 실패하는 함수 만들기
- LLM API은 실패율이 생각보다 높음
  - 많은 토큰을 사용하는 경우, 짧은 시간에 반복해서 호출하는 경우 => 에러 발생함
- LLM API 사용 시 재시도가 필수임
  - 실패하면 다시 시도하는 로직을 직접 짜도 됨
  - tenacity라는 라이브러리 사용하면 에러 발생 시의 재시도를 매우 쉽게 구현할 수 있음

### 1.4.3 tenacity를 적용하여 재처리하기
